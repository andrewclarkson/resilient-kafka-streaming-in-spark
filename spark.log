log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/10/09 08:10:06 INFO CheckpointReader: Checkpoint files found: file:/tmp/checkpoint-1444396203000,file:/tmp/checkpoint-1444396203000.bk,file:/tmp/checkpoint-1444396202000,file:/tmp/checkpoint-1444396202000.bk,file:/tmp/checkpoint-1444396201000,file:/tmp/checkpoint-1444396201000.bk,file:/tmp/checkpoint-1444396200000,file:/tmp/checkpoint-1444396200000.bk,file:/tmp/checkpoint-1444396199000,file:/tmp/checkpoint-1444396199000.bk
15/10/09 08:10:06 INFO CheckpointReader: Attempting to load checkpoint from file file:/tmp/checkpoint-1444396203000
15/10/09 08:10:06 INFO Checkpoint: Checkpoint for time 1444396203000 ms validated
15/10/09 08:10:06 INFO CheckpointReader: Checkpoint successfully loaded from file file:/tmp/checkpoint-1444396203000
15/10/09 08:10:06 INFO CheckpointReader: Checkpoint was generated at time 1444396203000 ms
15/10/09 08:10:06 INFO SparkContext: Running Spark version 1.5.1
15/10/09 08:10:06 INFO SecurityManager: Changing view acls to: andrewclarkson
15/10/09 08:10:06 INFO SecurityManager: Changing modify acls to: andrewclarkson
15/10/09 08:10:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(andrewclarkson); users with modify permissions: Set(andrewclarkson)
15/10/09 08:10:07 INFO Slf4jLogger: Slf4jLogger started
15/10/09 08:10:07 INFO Remoting: Starting remoting
15/10/09 08:10:07 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.101.21.127:57164]
15/10/09 08:10:07 INFO Utils: Successfully started service 'sparkDriver' on port 57164.
15/10/09 08:10:07 INFO SparkEnv: Registering MapOutputTracker
15/10/09 08:10:07 INFO SparkEnv: Registering BlockManagerMaster
15/10/09 08:10:07 INFO DiskBlockManager: Created local directory at /private/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/blockmgr-0b841de2-95c6-4e57-8de4-ce59e4379628
15/10/09 08:10:07 INFO MemoryStore: MemoryStore started with capacity 530.0 MB
15/10/09 08:10:07 INFO HttpFileServer: HTTP File server directory is /private/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/spark-2703f33f-6f61-4da2-b63e-f391886287a0/httpd-7d79fd32-9935-4622-84fa-e330fd110adc
15/10/09 08:10:07 INFO HttpServer: Starting HTTP Server
15/10/09 08:10:07 INFO Utils: Successfully started service 'HTTP file server' on port 57165.
15/10/09 08:10:07 INFO SparkEnv: Registering OutputCommitCoordinator
15/10/09 08:10:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/10/09 08:10:07 INFO SparkUI: Started SparkUI at http://10.101.21.127:4040
15/10/09 08:10:07 INFO SparkContext: Added JAR file:/Users/andrewclarkson/code/resilient-kafka-streaming-in-spark/target/scala-2.10/events-assembly-1.0.jar at http://10.101.21.127:57165/jars/events-assembly-1.0.jar with timestamp 1444396207788
15/10/09 08:10:07 INFO Executor: Starting executor ID driver on host localhost
15/10/09 08:10:07 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 57166.
15/10/09 08:10:07 INFO NettyBlockTransferService: Server created on 57166
15/10/09 08:10:07 INFO BlockManagerMaster: Trying to register BlockManager
15/10/09 08:10:07 INFO BlockManagerMasterEndpoint: Registering block manager localhost:57166 with 530.0 MB RAM, BlockManagerId(driver, localhost, 57166)
15/10/09 08:10:07 INFO BlockManagerMaster: Registered BlockManager
15/10/09 08:10:08 INFO ForEachDStream: Set context for org.apache.spark.streaming.dstream.ForEachDStream@67c277a0
15/10/09 08:10:08 INFO MappedDStream: Set context for org.apache.spark.streaming.dstream.MappedDStream@2d5f7182
15/10/09 08:10:08 INFO DirectKafkaInputDStream: Set context for org.apache.spark.streaming.kafka.DirectKafkaInputDStream@59546cfe
15/10/09 08:10:08 INFO DStreamGraph: Restoring checkpoint data
15/10/09 08:10:08 INFO ForEachDStream: Restoring checkpoint data
15/10/09 08:10:08 INFO MappedDStream: Restoring checkpoint data
15/10/09 08:10:08 INFO DirectKafkaInputDStream: Restoring checkpoint data
15/10/09 08:10:08 INFO VerifiableProperties: Verifying properties
15/10/09 08:10:08 INFO VerifiableProperties: Property auto.offset.reset is overridden to smallest
15/10/09 08:10:08 INFO VerifiableProperties: Property group.id is overridden to 
15/10/09 08:10:08 INFO VerifiableProperties: Property zookeeper.connect is overridden to 
15/10/09 08:10:08 INFO DirectKafkaInputDStream$DirectKafkaInputDStreamCheckpointData: Restoring KafkaRDD for time 1444396203000 ms [(events,0,532,532)]
15/10/09 08:10:08 INFO DirectKafkaInputDStream: Restored checkpoint data
15/10/09 08:10:08 INFO MappedDStream: Restored checkpoint data
15/10/09 08:10:08 INFO ForEachDStream: Restored checkpoint data
15/10/09 08:10:08 INFO DStreamGraph: Restored checkpoint data
15/10/09 08:10:08 INFO WriteAheadLogManager : Recovered 1 write ahead log files from file:/tmp/receivedBlockMetadata
15/10/09 08:10:08 INFO ReceivedBlockTracker: Recovering from write ahead logs in file:/tmp
15/10/09 08:10:08 INFO WriteAheadLogManager : Reading from the logs: file:/tmp/receivedBlockMetadata/log-1444396194025-1444396254025
15/10/09 08:10:08 INFO JobGenerator: Batches during down time (6 batches): 1444396203000 ms, 1444396204000 ms, 1444396205000 ms, 1444396206000 ms, 1444396207000 ms, 1444396208000 ms
15/10/09 08:10:08 INFO JobGenerator: Batches pending processing (0 batches): 
15/10/09 08:10:08 INFO JobGenerator: Batches to reschedule (6 batches): 1444396203000 ms, 1444396204000 ms, 1444396205000 ms, 1444396206000 ms, 1444396207000 ms, 1444396208000 ms
15/10/09 08:10:08 INFO JobScheduler: Added jobs for time 1444396203000 ms
15/10/09 08:10:08 INFO JobScheduler: Starting job streaming job 1444396203000 ms.0 from job set of time 1444396203000 ms
15/10/09 08:10:08 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:08 INFO JobScheduler: Added jobs for time 1444396204000 ms
15/10/09 08:10:08 INFO DAGScheduler: Got job 0 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:08 INFO DAGScheduler: Final stage: ResultStage 0(foreachRDD at Events.scala:26)
15/10/09 08:10:08 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:08 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:08 INFO JobScheduler: Added jobs for time 1444396205000 ms
15/10/09 08:10:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:08 INFO JobScheduler: Added jobs for time 1444396206000 ms
15/10/09 08:10:08 INFO JobScheduler: Added jobs for time 1444396207000 ms
15/10/09 08:10:08 INFO JobScheduler: Added jobs for time 1444396208000 ms
15/10/09 08:10:08 INFO RecurringTimer: Started timer for JobGenerator at time 1444396209000
15/10/09 08:10:08 INFO JobGenerator: Restarted JobGenerator at 1444396209000 ms
15/10/09 08:10:08 INFO JobScheduler: Started JobScheduler
15/10/09 08:10:08 INFO StreamingContext: StreamingContext started
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=0, maxMem=555755765
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=2920, maxMem=555755765
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at Events.scala:23)
15/10/09 08:10:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/10/09 08:10:08 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 2085 bytes)
15/10/09 08:10:08 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/10/09 08:10:08 INFO Executor: Fetching http://10.101.21.127:57165/jars/events-assembly-1.0.jar with timestamp 1444396207788
15/10/09 08:10:08 INFO Utils: Fetching http://10.101.21.127:57165/jars/events-assembly-1.0.jar to /private/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/spark-2703f33f-6f61-4da2-b63e-f391886287a0/userFiles-7eb89aa0-c4fa-44b0-9be8-0a6e80c45ccc/fetchFileTemp6796524971284263193.tmp
15/10/09 08:10:08 INFO Executor: Adding file:/private/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/spark-2703f33f-6f61-4da2-b63e-f391886287a0/userFiles-7eb89aa0-c4fa-44b0-9be8-0a6e80c45ccc/events-assembly-1.0.jar to class loader
15/10/09 08:10:08 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 915 bytes result sent to driver
15/10/09 08:10:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 237 ms on localhost (1/1)
15/10/09 08:10:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/10/09 08:10:08 INFO DAGScheduler: ResultStage 0 (foreachRDD at Events.scala:26) finished in 0.245 s
15/10/09 08:10:08 INFO DAGScheduler: Job 0 finished: foreachRDD at Events.scala:26, took 0.322193 s
15/10/09 08:10:08 INFO JobScheduler: Finished job streaming job 1444396203000 ms.0 from job set of time 1444396203000 ms
15/10/09 08:10:08 INFO JobScheduler: Total delay: 5.777 s for time 1444396203000 ms (execution: 0.336 s)
15/10/09 08:10:08 INFO JobScheduler: Starting job streaming job 1444396204000 ms.0 from job set of time 1444396204000 ms
15/10/09 08:10:08 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:08 INFO DAGScheduler: Got job 1 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:08 INFO DAGScheduler: Final stage: ResultStage 1(foreachRDD at Events.scala:26)
15/10/09 08:10:08 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:08 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:08 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[3] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:08 INFO JobGenerator: Checkpointing graph for time 1444396203000 ms
15/10/09 08:10:08 INFO DStreamGraph: Updating checkpoint data for time 1444396203000 ms
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=4606, maxMem=555755765
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(1684) called with curMem=7526, maxMem=555755765
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1684.0 B, free 530.0 MB)
15/10/09 08:10:08 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:57166 (size: 1684.0 B, free: 530.0 MB)
15/10/09 08:10:08 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[3] at map at Events.scala:23)
15/10/09 08:10:08 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
15/10/09 08:10:08 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, ANY, 2085 bytes)
15/10/09 08:10:08 INFO DStreamGraph: Updated checkpoint data for time 1444396203000 ms
15/10/09 08:10:08 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
15/10/09 08:10:08 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:08 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 915 bytes result sent to driver
15/10/09 08:10:08 INFO CheckpointWriter: Saving checkpoint for time 1444396203000 ms to file 'file:/tmp/checkpoint-1444396203000'
15/10/09 08:10:08 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 5 ms on localhost (1/1)
15/10/09 08:10:08 INFO DAGScheduler: ResultStage 1 (foreachRDD at Events.scala:26) finished in 0.006 s
15/10/09 08:10:08 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/10/09 08:10:08 INFO DAGScheduler: Job 1 finished: foreachRDD at Events.scala:26, took 0.012919 s
15/10/09 08:10:08 INFO JobScheduler: Finished job streaming job 1444396204000 ms.0 from job set of time 1444396204000 ms
15/10/09 08:10:08 INFO JobScheduler: Total delay: 4.796 s for time 1444396204000 ms (execution: 0.016 s)
15/10/09 08:10:08 INFO JobScheduler: Starting job streaming job 1444396205000 ms.0 from job set of time 1444396205000 ms
15/10/09 08:10:08 INFO MapPartitionsRDD: Removing RDD 1 from persistence list
15/10/09 08:10:08 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:08 INFO DAGScheduler: Got job 2 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:08 INFO DAGScheduler: Final stage: ResultStage 2(foreachRDD at Events.scala:26)
15/10/09 08:10:08 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:08 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:08 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=9210, maxMem=555755765
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(1685) called with curMem=12130, maxMem=555755765
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1685.0 B, free 530.0 MB)
15/10/09 08:10:08 INFO KafkaRDD: Removing RDD 0 from persistence list
15/10/09 08:10:08 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:57166 (size: 1685.0 B, free: 530.0 MB)
15/10/09 08:10:08 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:08 INFO JobGenerator: Checkpointing graph for time 1444396204000 ms
15/10/09 08:10:08 INFO DStreamGraph: Updating checkpoint data for time 1444396204000 ms
15/10/09 08:10:08 INFO BlockManager: Removing RDD 1
15/10/09 08:10:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at map at Events.scala:23)
15/10/09 08:10:08 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/10/09 08:10:08 INFO DStreamGraph: Updated checkpoint data for time 1444396204000 ms
15/10/09 08:10:08 INFO BlockManager: Removing RDD 0
15/10/09 08:10:08 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, ANY, 2085 bytes)
15/10/09 08:10:08 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
15/10/09 08:10:08 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:08 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 915 bytes result sent to driver
15/10/09 08:10:08 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 5 ms on localhost (1/1)
15/10/09 08:10:08 INFO DAGScheduler: ResultStage 2 (foreachRDD at Events.scala:26) finished in 0.006 s
15/10/09 08:10:08 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/10/09 08:10:08 INFO DAGScheduler: Job 2 finished: foreachRDD at Events.scala:26, took 0.013606 s
15/10/09 08:10:08 INFO JobScheduler: Finished job streaming job 1444396205000 ms.0 from job set of time 1444396205000 ms
15/10/09 08:10:08 INFO JobScheduler: Total delay: 3.813 s for time 1444396205000 ms (execution: 0.017 s)
15/10/09 08:10:08 INFO MapPartitionsRDD: Removing RDD 3 from persistence list
15/10/09 08:10:08 INFO JobScheduler: Starting job streaming job 1444396206000 ms.0 from job set of time 1444396206000 ms
15/10/09 08:10:08 INFO BlockManager: Removing RDD 3
15/10/09 08:10:08 INFO KafkaRDD: Removing RDD 2 from persistence list
15/10/09 08:10:08 INFO BlockManager: Removing RDD 2
15/10/09 08:10:08 INFO JobGenerator: Checkpointing graph for time 1444396205000 ms
15/10/09 08:10:08 INFO DStreamGraph: Updating checkpoint data for time 1444396205000 ms
15/10/09 08:10:08 INFO DStreamGraph: Updated checkpoint data for time 1444396205000 ms
15/10/09 08:10:08 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:08 INFO DAGScheduler: Got job 3 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:08 INFO DAGScheduler: Final stage: ResultStage 3(foreachRDD at Events.scala:26)
15/10/09 08:10:08 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:08 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:08 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[7] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=13815, maxMem=555755765
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:08 INFO CheckpointWriter: Checkpoint for time 1444396203000 ms saved to file 'file:/tmp/checkpoint-1444396203000', took 4107 bytes and 27 ms
15/10/09 08:10:08 INFO CheckpointWriter: Saving checkpoint for time 1444396204000 ms to file 'file:/tmp/checkpoint-1444396204000'
15/10/09 08:10:08 INFO DStreamGraph: Clearing checkpoint data for time 1444396203000 ms
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=16735, maxMem=555755765
15/10/09 08:10:08 INFO DStreamGraph: Cleared checkpoint data for time 1444396203000 ms
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:08 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:08 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[7] at map at Events.scala:23)
15/10/09 08:10:08 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
15/10/09 08:10:08 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:08 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, ANY, 2085 bytes)
15/10/09 08:10:08 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
15/10/09 08:10:08 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:08 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 915 bytes result sent to driver
15/10/09 08:10:08 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 5 ms on localhost (1/1)
15/10/09 08:10:08 INFO DAGScheduler: ResultStage 3 (foreachRDD at Events.scala:26) finished in 0.005 s
15/10/09 08:10:08 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/10/09 08:10:08 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396202000: 
15/10/09 08:10:08 INFO DAGScheduler: Job 3 finished: foreachRDD at Events.scala:26, took 0.012247 s
15/10/09 08:10:08 INFO JobScheduler: Finished job streaming job 1444396206000 ms.0 from job set of time 1444396206000 ms
15/10/09 08:10:08 INFO JobScheduler: Total delay: 2.830 s for time 1444396206000 ms (execution: 0.016 s)
15/10/09 08:10:08 INFO JobScheduler: Starting job streaming job 1444396207000 ms.0 from job set of time 1444396207000 ms
15/10/09 08:10:08 INFO InputInfoTracker: remove old batch metadata: 
15/10/09 08:10:08 INFO MapPartitionsRDD: Removing RDD 5 from persistence list
15/10/09 08:10:08 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396202000
15/10/09 08:10:08 INFO BlockManager: Removing RDD 5
15/10/09 08:10:08 INFO KafkaRDD: Removing RDD 4 from persistence list
15/10/09 08:10:08 INFO BlockManager: Removing RDD 4
15/10/09 08:10:08 INFO JobGenerator: Checkpointing graph for time 1444396206000 ms
15/10/09 08:10:08 INFO DStreamGraph: Updating checkpoint data for time 1444396206000 ms
15/10/09 08:10:08 INFO DStreamGraph: Updated checkpoint data for time 1444396206000 ms
15/10/09 08:10:08 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:08 INFO DAGScheduler: Got job 4 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:08 INFO DAGScheduler: Final stage: ResultStage 4(foreachRDD at Events.scala:26)
15/10/09 08:10:08 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:08 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:08 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[9] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:08 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396199000.bk
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=18421, maxMem=555755765
15/10/09 08:10:08 INFO CheckpointWriter: Checkpoint for time 1444396204000 ms saved to file 'file:/tmp/checkpoint-1444396204000', took 4087 bytes and 17 ms
15/10/09 08:10:08 INFO DStreamGraph: Clearing checkpoint data for time 1444396204000 ms
15/10/09 08:10:08 INFO CheckpointWriter: Saving checkpoint for time 1444396205000 ms to file 'file:/tmp/checkpoint-1444396205000'
15/10/09 08:10:08 INFO DStreamGraph: Cleared checkpoint data for time 1444396204000 ms
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:08 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:08 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396203000: 
15/10/09 08:10:08 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396203000
15/10/09 08:10:08 INFO InputInfoTracker: remove old batch metadata: 
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=21341, maxMem=555755765
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:08 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:08 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[9] at map at Events.scala:23)
15/10/09 08:10:08 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
15/10/09 08:10:08 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, ANY, 2085 bytes)
15/10/09 08:10:08 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
15/10/09 08:10:08 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:08 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 915 bytes result sent to driver
15/10/09 08:10:08 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 4 ms on localhost (1/1)
15/10/09 08:10:08 INFO DAGScheduler: ResultStage 4 (foreachRDD at Events.scala:26) finished in 0.004 s
15/10/09 08:10:08 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/10/09 08:10:08 INFO DAGScheduler: Job 4 finished: foreachRDD at Events.scala:26, took 0.011294 s
15/10/09 08:10:08 INFO JobScheduler: Finished job streaming job 1444396207000 ms.0 from job set of time 1444396207000 ms
15/10/09 08:10:08 INFO JobScheduler: Total delay: 1.846 s for time 1444396207000 ms (execution: 0.016 s)
15/10/09 08:10:08 INFO MapPartitionsRDD: Removing RDD 7 from persistence list
15/10/09 08:10:08 INFO JobScheduler: Starting job streaming job 1444396208000 ms.0 from job set of time 1444396208000 ms
15/10/09 08:10:08 INFO BlockManager: Removing RDD 7
15/10/09 08:10:08 INFO KafkaRDD: Removing RDD 6 from persistence list
15/10/09 08:10:08 INFO JobGenerator: Checkpointing graph for time 1444396207000 ms
15/10/09 08:10:08 INFO BlockManager: Removing RDD 6
15/10/09 08:10:08 INFO DStreamGraph: Updating checkpoint data for time 1444396207000 ms
15/10/09 08:10:08 INFO DStreamGraph: Updated checkpoint data for time 1444396207000 ms
15/10/09 08:10:08 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:08 INFO DAGScheduler: Got job 5 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:08 INFO DAGScheduler: Final stage: ResultStage 5(foreachRDD at Events.scala:26)
15/10/09 08:10:08 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:08 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:08 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[11] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:08 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396199000
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=23027, maxMem=555755765
15/10/09 08:10:08 INFO CheckpointWriter: Checkpoint for time 1444396205000 ms saved to file 'file:/tmp/checkpoint-1444396205000', took 4081 bytes and 14 ms
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:08 INFO DStreamGraph: Clearing checkpoint data for time 1444396205000 ms
15/10/09 08:10:08 INFO CheckpointWriter: Saving checkpoint for time 1444396206000 ms to file 'file:/tmp/checkpoint-1444396206000'
15/10/09 08:10:08 INFO DStreamGraph: Cleared checkpoint data for time 1444396205000 ms
15/10/09 08:10:08 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:08 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396204000: 
15/10/09 08:10:08 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396204000
15/10/09 08:10:08 INFO InputInfoTracker: remove old batch metadata: 
15/10/09 08:10:08 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=25947, maxMem=555755765
15/10/09 08:10:08 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:08 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:08 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[11] at map at Events.scala:23)
15/10/09 08:10:08 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
15/10/09 08:10:08 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, ANY, 2085 bytes)
15/10/09 08:10:08 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
15/10/09 08:10:08 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:08 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:08 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396200000.bk
15/10/09 08:10:08 INFO CheckpointWriter: Checkpoint for time 1444396206000 ms saved to file 'file:/tmp/checkpoint-1444396206000', took 4060 bytes and 15 ms
15/10/09 08:10:08 INFO DStreamGraph: Clearing checkpoint data for time 1444396206000 ms
15/10/09 08:10:08 INFO CheckpointWriter: Saving checkpoint for time 1444396207000 ms to file 'file:/tmp/checkpoint-1444396207000'
15/10/09 08:10:08 INFO DStreamGraph: Cleared checkpoint data for time 1444396206000 ms
15/10/09 08:10:08 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:08 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396205000: 
15/10/09 08:10:08 INFO InputInfoTracker: remove old batch metadata: 1444396204000 ms
15/10/09 08:10:08 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396205000
15/10/09 08:10:08 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:08 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
15/10/09 08:10:08 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/10/09 08:10:08 INFO TaskSchedulerImpl: Cancelling stage 5
15/10/09 08:10:08 INFO DAGScheduler: ResultStage 5 (foreachRDD at Events.scala:26) failed in 0.019 s
15/10/09 08:10:08 INFO DAGScheduler: Job 5 failed: foreachRDD at Events.scala:26, took 0.026085 s
15/10/09 08:10:08 INFO JobScheduler: Finished job streaming job 1444396208000 ms.0 from job set of time 1444396208000 ms
15/10/09 08:10:08 INFO JobScheduler: Total delay: 0.875 s for time 1444396208000 ms (execution: 0.029 s)
15/10/09 08:10:08 INFO MapPartitionsRDD: Removing RDD 9 from persistence list
15/10/09 08:10:08 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396200000
15/10/09 08:10:08 INFO KafkaRDD: Removing RDD 8 from persistence list
15/10/09 08:10:08 INFO BlockManager: Removing RDD 9
15/10/09 08:10:08 INFO CheckpointWriter: Checkpoint for time 1444396207000 ms saved to file 'file:/tmp/checkpoint-1444396207000', took 4046 bytes and 10 ms
15/10/09 08:10:08 ERROR JobScheduler: Error running job streaming job 1444396208000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:08 INFO BlockManager: Removing RDD 8
15/10/09 08:10:08 INFO DStreamGraph: Clearing checkpoint data for time 1444396207000 ms
15/10/09 08:10:08 INFO DStreamGraph: Cleared checkpoint data for time 1444396207000 ms
15/10/09 08:10:08 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
Exception in thread "main" 15/10/09 08:10:08 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396206000: 
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:08 INFO InputInfoTracker: remove old batch metadata: 1444396205000 ms
15/10/09 08:10:08 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396206000
15/10/09 08:10:08 INFO JobGenerator: Checkpointing graph for time 1444396208000 ms
15/10/09 08:10:08 INFO DStreamGraph: Updating checkpoint data for time 1444396208000 ms
15/10/09 08:10:08 INFO DStreamGraph: Updated checkpoint data for time 1444396208000 ms
15/10/09 08:10:08 INFO CheckpointWriter: Saving checkpoint for time 1444396208000 ms to file 'file:/tmp/checkpoint-1444396208000'
15/10/09 08:10:08 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396201000.bk
15/10/09 08:10:08 INFO CheckpointWriter: Checkpoint for time 1444396208000 ms saved to file 'file:/tmp/checkpoint-1444396208000', took 4010 bytes and 9 ms
15/10/09 08:10:08 INFO DStreamGraph: Clearing checkpoint data for time 1444396208000 ms
15/10/09 08:10:08 INFO DStreamGraph: Cleared checkpoint data for time 1444396208000 ms
15/10/09 08:10:08 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:08 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396207000: 
15/10/09 08:10:08 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396207000
15/10/09 08:10:08 INFO InputInfoTracker: remove old batch metadata: 1444396206000 ms
15/10/09 08:10:09 INFO JobScheduler: Added jobs for time 1444396209000 ms
15/10/09 08:10:09 INFO JobGenerator: Checkpointing graph for time 1444396209000 ms
15/10/09 08:10:09 INFO DStreamGraph: Updating checkpoint data for time 1444396209000 ms
15/10/09 08:10:09 INFO JobScheduler: Starting job streaming job 1444396209000 ms.0 from job set of time 1444396209000 ms
15/10/09 08:10:09 INFO DStreamGraph: Updated checkpoint data for time 1444396209000 ms
15/10/09 08:10:09 INFO CheckpointWriter: Saving checkpoint for time 1444396209000 ms to file 'file:/tmp/checkpoint-1444396209000'
15/10/09 08:10:09 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:09 INFO DAGScheduler: Got job 6 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:09 INFO DAGScheduler: Final stage: ResultStage 6(foreachRDD at Events.scala:26)
15/10/09 08:10:09 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:09 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:09 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[13] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:09 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=27633, maxMem=555755765
15/10/09 08:10:09 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:09 INFO MemoryStore: ensureFreeSpace(1684) called with curMem=30553, maxMem=555755765
15/10/09 08:10:09 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 1684.0 B, free 530.0 MB)
15/10/09 08:10:09 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:57166 (size: 1684.0 B, free: 530.0 MB)
15/10/09 08:10:09 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[13] at map at Events.scala:23)
15/10/09 08:10:09 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
15/10/09 08:10:09 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, ANY, 2085 bytes)
15/10/09 08:10:09 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
15/10/09 08:10:09 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396201000
15/10/09 08:10:09 INFO CheckpointWriter: Checkpoint for time 1444396209000 ms saved to file 'file:/tmp/checkpoint-1444396209000', took 4045 bytes and 11 ms
15/10/09 08:10:09 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:09 ERROR Executor: Exception in task 0.0 in stage 6.0 (TID 6)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:09 WARN TaskSetManager: Lost task 0.0 in stage 6.0 (TID 6, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:09 ERROR TaskSetManager: Task 0 in stage 6.0 failed 1 times; aborting job
15/10/09 08:10:09 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/10/09 08:10:09 INFO TaskSchedulerImpl: Cancelling stage 6
15/10/09 08:10:09 INFO DAGScheduler: ResultStage 6 (foreachRDD at Events.scala:26) failed in 0.005 s
15/10/09 08:10:09 INFO DAGScheduler: Job 6 failed: foreachRDD at Events.scala:26, took 0.012061 s
15/10/09 08:10:09 INFO JobScheduler: Finished job streaming job 1444396209000 ms.0 from job set of time 1444396209000 ms
15/10/09 08:10:09 INFO JobScheduler: Total delay: 0.028 s for time 1444396209000 ms (execution: 0.016 s)
15/10/09 08:10:09 INFO MapPartitionsRDD: Removing RDD 11 from persistence list
15/10/09 08:10:09 ERROR JobScheduler: Error running job streaming job 1444396209000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:09 INFO BlockManager: Removing RDD 11
15/10/09 08:10:09 INFO KafkaRDD: Removing RDD 10 from persistence list
15/10/09 08:10:09 INFO JobGenerator: Checkpointing graph for time 1444396209000 ms
15/10/09 08:10:09 INFO BlockManager: Removing RDD 10
15/10/09 08:10:09 INFO DStreamGraph: Updating checkpoint data for time 1444396209000 ms
15/10/09 08:10:09 INFO DStreamGraph: Updated checkpoint data for time 1444396209000 ms
15/10/09 08:10:09 INFO CheckpointWriter: Saving checkpoint for time 1444396209000 ms to file 'file:/tmp/checkpoint-1444396209000'
15/10/09 08:10:09 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396202000.bk
15/10/09 08:10:09 INFO CheckpointWriter: Checkpoint for time 1444396209000 ms saved to file 'file:/tmp/checkpoint-1444396209000', took 4010 bytes and 9 ms
15/10/09 08:10:09 INFO DStreamGraph: Clearing checkpoint data for time 1444396209000 ms
15/10/09 08:10:09 INFO DStreamGraph: Cleared checkpoint data for time 1444396209000 ms
15/10/09 08:10:09 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:09 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396208000: 
15/10/09 08:10:09 INFO InputInfoTracker: remove old batch metadata: 1444396207000 ms
15/10/09 08:10:09 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396208000
15/10/09 08:10:10 INFO JobScheduler: Added jobs for time 1444396210000 ms
15/10/09 08:10:10 INFO JobGenerator: Checkpointing graph for time 1444396210000 ms
15/10/09 08:10:10 INFO DStreamGraph: Updating checkpoint data for time 1444396210000 ms
15/10/09 08:10:10 INFO JobScheduler: Starting job streaming job 1444396210000 ms.0 from job set of time 1444396210000 ms
15/10/09 08:10:10 INFO DStreamGraph: Updated checkpoint data for time 1444396210000 ms
15/10/09 08:10:10 INFO CheckpointWriter: Saving checkpoint for time 1444396210000 ms to file 'file:/tmp/checkpoint-1444396210000'
15/10/09 08:10:10 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:10 INFO DAGScheduler: Got job 7 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:10 INFO DAGScheduler: Final stage: ResultStage 7(foreachRDD at Events.scala:26)
15/10/09 08:10:10 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:10 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:10 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[15] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:10 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=32237, maxMem=555755765
15/10/09 08:10:10 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:10 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=35157, maxMem=555755765
15/10/09 08:10:10 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:10 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:10 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[15] at map at Events.scala:23)
15/10/09 08:10:10 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
15/10/09 08:10:10 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, ANY, 2085 bytes)
15/10/09 08:10:10 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
15/10/09 08:10:10 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396202000
15/10/09 08:10:10 INFO CheckpointWriter: Checkpoint for time 1444396210000 ms saved to file 'file:/tmp/checkpoint-1444396210000', took 4049 bytes and 12 ms
15/10/09 08:10:10 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:10 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 915 bytes result sent to driver
15/10/09 08:10:10 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 5 ms on localhost (1/1)
15/10/09 08:10:10 INFO DAGScheduler: ResultStage 7 (foreachRDD at Events.scala:26) finished in 0.005 s
15/10/09 08:10:10 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/10/09 08:10:10 INFO DAGScheduler: Job 7 finished: foreachRDD at Events.scala:26, took 0.012120 s
15/10/09 08:10:10 INFO JobScheduler: Finished job streaming job 1444396210000 ms.0 from job set of time 1444396210000 ms
15/10/09 08:10:10 INFO JobScheduler: Total delay: 0.023 s for time 1444396210000 ms (execution: 0.017 s)
15/10/09 08:10:10 INFO MapPartitionsRDD: Removing RDD 13 from persistence list
15/10/09 08:10:10 INFO KafkaRDD: Removing RDD 12 from persistence list
15/10/09 08:10:10 INFO BlockManager: Removing RDD 13
15/10/09 08:10:10 INFO JobGenerator: Checkpointing graph for time 1444396210000 ms
15/10/09 08:10:10 INFO BlockManager: Removing RDD 12
15/10/09 08:10:10 INFO DStreamGraph: Updating checkpoint data for time 1444396210000 ms
15/10/09 08:10:10 INFO DStreamGraph: Updated checkpoint data for time 1444396210000 ms
15/10/09 08:10:10 INFO CheckpointWriter: Saving checkpoint for time 1444396210000 ms to file 'file:/tmp/checkpoint-1444396210000'
15/10/09 08:10:10 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396203000.bk
15/10/09 08:10:10 INFO CheckpointWriter: Checkpoint for time 1444396210000 ms saved to file 'file:/tmp/checkpoint-1444396210000', took 4010 bytes and 10 ms
15/10/09 08:10:10 INFO DStreamGraph: Clearing checkpoint data for time 1444396210000 ms
15/10/09 08:10:10 INFO DStreamGraph: Cleared checkpoint data for time 1444396210000 ms
15/10/09 08:10:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:10 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396209000: 
15/10/09 08:10:10 INFO InputInfoTracker: remove old batch metadata: 1444396208000 ms
15/10/09 08:10:10 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396209000
15/10/09 08:10:11 INFO JobScheduler: Added jobs for time 1444396211000 ms
15/10/09 08:10:11 INFO JobGenerator: Checkpointing graph for time 1444396211000 ms
15/10/09 08:10:11 INFO JobScheduler: Starting job streaming job 1444396211000 ms.0 from job set of time 1444396211000 ms
15/10/09 08:10:11 INFO DStreamGraph: Updating checkpoint data for time 1444396211000 ms
15/10/09 08:10:11 INFO DStreamGraph: Updated checkpoint data for time 1444396211000 ms
15/10/09 08:10:11 INFO CheckpointWriter: Saving checkpoint for time 1444396211000 ms to file 'file:/tmp/checkpoint-1444396211000'
15/10/09 08:10:11 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:11 INFO DAGScheduler: Got job 8 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:11 INFO DAGScheduler: Final stage: ResultStage 8(foreachRDD at Events.scala:26)
15/10/09 08:10:11 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:11 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:11 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[17] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:11 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=36843, maxMem=555755765
15/10/09 08:10:11 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:11 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=39763, maxMem=555755765
15/10/09 08:10:11 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:11 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:11 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[17] at map at Events.scala:23)
15/10/09 08:10:11 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
15/10/09 08:10:11 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, ANY, 2085 bytes)
15/10/09 08:10:11 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396203000
15/10/09 08:10:11 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
15/10/09 08:10:11 INFO CheckpointWriter: Checkpoint for time 1444396211000 ms saved to file 'file:/tmp/checkpoint-1444396211000', took 4045 bytes and 13 ms
15/10/09 08:10:11 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:11 INFO Executor: Finished task 0.0 in stage 8.0 (TID 8). 915 bytes result sent to driver
15/10/09 08:10:11 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 3 ms on localhost (1/1)
15/10/09 08:10:11 INFO DAGScheduler: ResultStage 8 (foreachRDD at Events.scala:26) finished in 0.003 s
15/10/09 08:10:11 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/10/09 08:10:11 INFO DAGScheduler: Job 8 finished: foreachRDD at Events.scala:26, took 0.012890 s
15/10/09 08:10:11 INFO JobScheduler: Finished job streaming job 1444396211000 ms.0 from job set of time 1444396211000 ms
15/10/09 08:10:11 INFO JobScheduler: Total delay: 0.025 s for time 1444396211000 ms (execution: 0.017 s)
15/10/09 08:10:11 INFO MapPartitionsRDD: Removing RDD 15 from persistence list
15/10/09 08:10:11 INFO KafkaRDD: Removing RDD 14 from persistence list
15/10/09 08:10:11 INFO BlockManager: Removing RDD 15
15/10/09 08:10:11 INFO JobGenerator: Checkpointing graph for time 1444396211000 ms
15/10/09 08:10:11 INFO BlockManager: Removing RDD 14
15/10/09 08:10:11 INFO DStreamGraph: Updating checkpoint data for time 1444396211000 ms
15/10/09 08:10:11 INFO DStreamGraph: Updated checkpoint data for time 1444396211000 ms
15/10/09 08:10:11 INFO CheckpointWriter: Saving checkpoint for time 1444396211000 ms to file 'file:/tmp/checkpoint-1444396211000'
15/10/09 08:10:11 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396204000
15/10/09 08:10:11 INFO CheckpointWriter: Checkpoint for time 1444396211000 ms saved to file 'file:/tmp/checkpoint-1444396211000', took 4010 bytes and 9 ms
15/10/09 08:10:11 INFO DStreamGraph: Clearing checkpoint data for time 1444396211000 ms
15/10/09 08:10:11 INFO DStreamGraph: Cleared checkpoint data for time 1444396211000 ms
15/10/09 08:10:11 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:11 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396210000: 
15/10/09 08:10:11 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396210000
15/10/09 08:10:11 INFO InputInfoTracker: remove old batch metadata: 1444396209000 ms
15/10/09 08:10:12 INFO JobScheduler: Added jobs for time 1444396212000 ms
15/10/09 08:10:12 INFO JobGenerator: Checkpointing graph for time 1444396212000 ms
15/10/09 08:10:12 INFO JobScheduler: Starting job streaming job 1444396212000 ms.0 from job set of time 1444396212000 ms
15/10/09 08:10:12 INFO DStreamGraph: Updating checkpoint data for time 1444396212000 ms
15/10/09 08:10:12 INFO DStreamGraph: Updated checkpoint data for time 1444396212000 ms
15/10/09 08:10:12 INFO CheckpointWriter: Saving checkpoint for time 1444396212000 ms to file 'file:/tmp/checkpoint-1444396212000'
15/10/09 08:10:12 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:12 INFO DAGScheduler: Got job 9 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:12 INFO DAGScheduler: Final stage: ResultStage 9(foreachRDD at Events.scala:26)
15/10/09 08:10:12 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:12 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:12 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[19] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:12 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=41449, maxMem=555755765
15/10/09 08:10:12 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:12 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=44369, maxMem=555755765
15/10/09 08:10:12 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:12 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[19] at map at Events.scala:23)
15/10/09 08:10:12 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/10/09 08:10:12 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396205000
15/10/09 08:10:12 INFO CheckpointWriter: Checkpoint for time 1444396212000 ms saved to file 'file:/tmp/checkpoint-1444396212000', took 4049 bytes and 10 ms
15/10/09 08:10:12 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, ANY, 2085 bytes)
15/10/09 08:10:12 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
15/10/09 08:10:12 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:12 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 915 bytes result sent to driver
15/10/09 08:10:12 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 2 ms on localhost (1/1)
15/10/09 08:10:12 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/10/09 08:10:12 INFO DAGScheduler: ResultStage 9 (foreachRDD at Events.scala:26) finished in 0.003 s
15/10/09 08:10:12 INFO DAGScheduler: Job 9 finished: foreachRDD at Events.scala:26, took 0.011930 s
15/10/09 08:10:12 INFO JobScheduler: Finished job streaming job 1444396212000 ms.0 from job set of time 1444396212000 ms
15/10/09 08:10:12 INFO JobScheduler: Total delay: 0.026 s for time 1444396212000 ms (execution: 0.016 s)
15/10/09 08:10:12 INFO MapPartitionsRDD: Removing RDD 17 from persistence list
15/10/09 08:10:12 INFO KafkaRDD: Removing RDD 16 from persistence list
15/10/09 08:10:12 INFO BlockManager: Removing RDD 17
15/10/09 08:10:12 INFO JobGenerator: Checkpointing graph for time 1444396212000 ms
15/10/09 08:10:12 INFO BlockManager: Removing RDD 16
15/10/09 08:10:12 INFO DStreamGraph: Updating checkpoint data for time 1444396212000 ms
15/10/09 08:10:12 INFO DStreamGraph: Updated checkpoint data for time 1444396212000 ms
15/10/09 08:10:12 INFO CheckpointWriter: Saving checkpoint for time 1444396212000 ms to file 'file:/tmp/checkpoint-1444396212000'
15/10/09 08:10:12 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396206000
15/10/09 08:10:12 INFO CheckpointWriter: Checkpoint for time 1444396212000 ms saved to file 'file:/tmp/checkpoint-1444396212000', took 4010 bytes and 9 ms
15/10/09 08:10:12 INFO DStreamGraph: Clearing checkpoint data for time 1444396212000 ms
15/10/09 08:10:12 INFO DStreamGraph: Cleared checkpoint data for time 1444396212000 ms
15/10/09 08:10:12 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:12 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396211000: 
15/10/09 08:10:12 INFO InputInfoTracker: remove old batch metadata: 1444396210000 ms
15/10/09 08:10:12 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396211000
15/10/09 08:10:13 INFO JobScheduler: Added jobs for time 1444396213000 ms
15/10/09 08:10:13 INFO JobGenerator: Checkpointing graph for time 1444396213000 ms
15/10/09 08:10:13 INFO DStreamGraph: Updating checkpoint data for time 1444396213000 ms
15/10/09 08:10:13 INFO JobScheduler: Starting job streaming job 1444396213000 ms.0 from job set of time 1444396213000 ms
15/10/09 08:10:13 INFO DStreamGraph: Updated checkpoint data for time 1444396213000 ms
15/10/09 08:10:13 INFO CheckpointWriter: Saving checkpoint for time 1444396213000 ms to file 'file:/tmp/checkpoint-1444396213000'
15/10/09 08:10:13 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:13 INFO DAGScheduler: Got job 10 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:13 INFO DAGScheduler: Final stage: ResultStage 10(foreachRDD at Events.scala:26)
15/10/09 08:10:13 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:13 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:13 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[21] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:13 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=46055, maxMem=555755765
15/10/09 08:10:13 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:13 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=48975, maxMem=555755765
15/10/09 08:10:13 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:13 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:13 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[21] at map at Events.scala:23)
15/10/09 08:10:13 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
15/10/09 08:10:13 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396207000
15/10/09 08:10:13 INFO CheckpointWriter: Checkpoint for time 1444396213000 ms saved to file 'file:/tmp/checkpoint-1444396213000', took 4049 bytes and 10 ms
15/10/09 08:10:13 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, ANY, 2085 bytes)
15/10/09 08:10:13 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
15/10/09 08:10:13 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:13 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 915 bytes result sent to driver
15/10/09 08:10:13 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 3 ms on localhost (1/1)
15/10/09 08:10:13 INFO DAGScheduler: ResultStage 10 (foreachRDD at Events.scala:26) finished in 0.003 s
15/10/09 08:10:13 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/10/09 08:10:13 INFO DAGScheduler: Job 10 finished: foreachRDD at Events.scala:26, took 0.011477 s
15/10/09 08:10:13 INFO JobScheduler: Finished job streaming job 1444396213000 ms.0 from job set of time 1444396213000 ms
15/10/09 08:10:13 INFO JobScheduler: Total delay: 0.025 s for time 1444396213000 ms (execution: 0.015 s)
15/10/09 08:10:13 INFO MapPartitionsRDD: Removing RDD 19 from persistence list
15/10/09 08:10:13 INFO BlockManager: Removing RDD 19
15/10/09 08:10:13 INFO KafkaRDD: Removing RDD 18 from persistence list
15/10/09 08:10:13 INFO JobGenerator: Checkpointing graph for time 1444396213000 ms
15/10/09 08:10:13 INFO DStreamGraph: Updating checkpoint data for time 1444396213000 ms
15/10/09 08:10:13 INFO BlockManager: Removing RDD 18
15/10/09 08:10:13 INFO DStreamGraph: Updated checkpoint data for time 1444396213000 ms
15/10/09 08:10:13 INFO CheckpointWriter: Saving checkpoint for time 1444396213000 ms to file 'file:/tmp/checkpoint-1444396213000'
15/10/09 08:10:13 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396208000
15/10/09 08:10:13 INFO CheckpointWriter: Checkpoint for time 1444396213000 ms saved to file 'file:/tmp/checkpoint-1444396213000', took 4010 bytes and 9 ms
15/10/09 08:10:13 INFO DStreamGraph: Clearing checkpoint data for time 1444396213000 ms
15/10/09 08:10:13 INFO DStreamGraph: Cleared checkpoint data for time 1444396213000 ms
15/10/09 08:10:13 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:13 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396212000: 
15/10/09 08:10:13 INFO InputInfoTracker: remove old batch metadata: 1444396211000 ms
15/10/09 08:10:13 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396212000
15/10/09 08:10:14 INFO JobScheduler: Added jobs for time 1444396214000 ms
15/10/09 08:10:14 INFO JobGenerator: Checkpointing graph for time 1444396214000 ms
15/10/09 08:10:14 INFO DStreamGraph: Updating checkpoint data for time 1444396214000 ms
15/10/09 08:10:14 INFO JobScheduler: Starting job streaming job 1444396214000 ms.0 from job set of time 1444396214000 ms
15/10/09 08:10:14 INFO DStreamGraph: Updated checkpoint data for time 1444396214000 ms
15/10/09 08:10:14 INFO CheckpointWriter: Saving checkpoint for time 1444396214000 ms to file 'file:/tmp/checkpoint-1444396214000'
15/10/09 08:10:14 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:14 INFO DAGScheduler: Got job 11 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:14 INFO DAGScheduler: Final stage: ResultStage 11(foreachRDD at Events.scala:26)
15/10/09 08:10:14 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:14 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:14 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[23] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:14 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=50661, maxMem=555755765
15/10/09 08:10:14 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:14 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=53581, maxMem=555755765
15/10/09 08:10:14 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:14 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:14 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:14 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396209000.bk
15/10/09 08:10:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[23] at map at Events.scala:23)
15/10/09 08:10:14 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
15/10/09 08:10:14 INFO CheckpointWriter: Checkpoint for time 1444396214000 ms saved to file 'file:/tmp/checkpoint-1444396214000', took 4045 bytes and 10 ms
15/10/09 08:10:14 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, ANY, 2085 bytes)
15/10/09 08:10:14 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
15/10/09 08:10:14 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:14 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 915 bytes result sent to driver
15/10/09 08:10:14 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 2 ms on localhost (1/1)
15/10/09 08:10:14 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/10/09 08:10:14 INFO DAGScheduler: ResultStage 11 (foreachRDD at Events.scala:26) finished in 0.002 s
15/10/09 08:10:14 INFO DAGScheduler: Job 11 finished: foreachRDD at Events.scala:26, took 0.010284 s
15/10/09 08:10:14 INFO JobScheduler: Finished job streaming job 1444396214000 ms.0 from job set of time 1444396214000 ms
15/10/09 08:10:14 INFO JobScheduler: Total delay: 0.022 s for time 1444396214000 ms (execution: 0.014 s)
15/10/09 08:10:14 INFO MapPartitionsRDD: Removing RDD 21 from persistence list
15/10/09 08:10:14 INFO BlockManager: Removing RDD 21
15/10/09 08:10:14 INFO KafkaRDD: Removing RDD 20 from persistence list
15/10/09 08:10:14 INFO JobGenerator: Checkpointing graph for time 1444396214000 ms
15/10/09 08:10:14 INFO DStreamGraph: Updating checkpoint data for time 1444396214000 ms
15/10/09 08:10:14 INFO BlockManager: Removing RDD 20
15/10/09 08:10:14 INFO DStreamGraph: Updated checkpoint data for time 1444396214000 ms
15/10/09 08:10:14 INFO CheckpointWriter: Saving checkpoint for time 1444396214000 ms to file 'file:/tmp/checkpoint-1444396214000'
15/10/09 08:10:14 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396209000
15/10/09 08:10:14 INFO CheckpointWriter: Checkpoint for time 1444396214000 ms saved to file 'file:/tmp/checkpoint-1444396214000', took 4010 bytes and 8 ms
15/10/09 08:10:14 INFO DStreamGraph: Clearing checkpoint data for time 1444396214000 ms
15/10/09 08:10:14 INFO DStreamGraph: Cleared checkpoint data for time 1444396214000 ms
15/10/09 08:10:14 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:14 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396213000: 
15/10/09 08:10:14 INFO InputInfoTracker: remove old batch metadata: 1444396212000 ms
15/10/09 08:10:14 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396213000
15/10/09 08:10:15 INFO JobScheduler: Added jobs for time 1444396215000 ms
15/10/09 08:10:15 INFO JobGenerator: Checkpointing graph for time 1444396215000 ms
15/10/09 08:10:15 INFO DStreamGraph: Updating checkpoint data for time 1444396215000 ms
15/10/09 08:10:15 INFO JobScheduler: Starting job streaming job 1444396215000 ms.0 from job set of time 1444396215000 ms
15/10/09 08:10:15 INFO DStreamGraph: Updated checkpoint data for time 1444396215000 ms
15/10/09 08:10:15 INFO CheckpointWriter: Saving checkpoint for time 1444396215000 ms to file 'file:/tmp/checkpoint-1444396215000'
15/10/09 08:10:15 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:15 INFO DAGScheduler: Got job 12 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:15 INFO DAGScheduler: Final stage: ResultStage 12(foreachRDD at Events.scala:26)
15/10/09 08:10:15 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:15 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:15 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[25] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:15 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=55267, maxMem=555755765
15/10/09 08:10:15 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:15 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=58187, maxMem=555755765
15/10/09 08:10:15 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:15 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:15 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[25] at map at Events.scala:23)
15/10/09 08:10:15 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
15/10/09 08:10:15 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, ANY, 2085 bytes)
15/10/09 08:10:15 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
15/10/09 08:10:15 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396210000.bk
15/10/09 08:10:15 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:15 INFO CheckpointWriter: Checkpoint for time 1444396215000 ms saved to file 'file:/tmp/checkpoint-1444396215000', took 4045 bytes and 12 ms
15/10/09 08:10:15 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 915 bytes result sent to driver
15/10/09 08:10:15 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 3 ms on localhost (1/1)
15/10/09 08:10:15 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
15/10/09 08:10:15 INFO DAGScheduler: ResultStage 12 (foreachRDD at Events.scala:26) finished in 0.003 s
15/10/09 08:10:15 INFO DAGScheduler: Job 12 finished: foreachRDD at Events.scala:26, took 0.011502 s
15/10/09 08:10:15 INFO JobScheduler: Finished job streaming job 1444396215000 ms.0 from job set of time 1444396215000 ms
15/10/09 08:10:15 INFO JobScheduler: Total delay: 0.024 s for time 1444396215000 ms (execution: 0.016 s)
15/10/09 08:10:15 INFO MapPartitionsRDD: Removing RDD 23 from persistence list
15/10/09 08:10:15 INFO KafkaRDD: Removing RDD 22 from persistence list
15/10/09 08:10:15 INFO BlockManager: Removing RDD 23
15/10/09 08:10:15 INFO JobGenerator: Checkpointing graph for time 1444396215000 ms
15/10/09 08:10:15 INFO BlockManager: Removing RDD 22
15/10/09 08:10:15 INFO DStreamGraph: Updating checkpoint data for time 1444396215000 ms
15/10/09 08:10:15 INFO DStreamGraph: Updated checkpoint data for time 1444396215000 ms
15/10/09 08:10:15 INFO CheckpointWriter: Saving checkpoint for time 1444396215000 ms to file 'file:/tmp/checkpoint-1444396215000'
15/10/09 08:10:15 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396210000
15/10/09 08:10:15 INFO CheckpointWriter: Checkpoint for time 1444396215000 ms saved to file 'file:/tmp/checkpoint-1444396215000', took 4010 bytes and 8 ms
15/10/09 08:10:15 INFO DStreamGraph: Clearing checkpoint data for time 1444396215000 ms
15/10/09 08:10:15 INFO DStreamGraph: Cleared checkpoint data for time 1444396215000 ms
15/10/09 08:10:15 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:15 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396214000: 
15/10/09 08:10:15 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396214000
15/10/09 08:10:15 INFO InputInfoTracker: remove old batch metadata: 1444396213000 ms
15/10/09 08:10:16 INFO JobScheduler: Added jobs for time 1444396216000 ms
15/10/09 08:10:16 INFO JobGenerator: Checkpointing graph for time 1444396216000 ms
15/10/09 08:10:16 INFO DStreamGraph: Updating checkpoint data for time 1444396216000 ms
15/10/09 08:10:16 INFO JobScheduler: Starting job streaming job 1444396216000 ms.0 from job set of time 1444396216000 ms
15/10/09 08:10:16 INFO DStreamGraph: Updated checkpoint data for time 1444396216000 ms
15/10/09 08:10:16 INFO CheckpointWriter: Saving checkpoint for time 1444396216000 ms to file 'file:/tmp/checkpoint-1444396216000'
15/10/09 08:10:16 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:16 INFO DAGScheduler: Got job 13 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:16 INFO DAGScheduler: Final stage: ResultStage 13(foreachRDD at Events.scala:26)
15/10/09 08:10:16 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:16 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:16 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[27] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:16 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=59873, maxMem=555755765
15/10/09 08:10:16 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:16 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=62793, maxMem=555755765
15/10/09 08:10:16 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 1686.0 B, free 529.9 MB)
15/10/09 08:10:16 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:16 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[27] at map at Events.scala:23)
15/10/09 08:10:16 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
15/10/09 08:10:16 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, localhost, ANY, 2085 bytes)
15/10/09 08:10:16 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
15/10/09 08:10:16 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:16 INFO Executor: Finished task 0.0 in stage 13.0 (TID 13). 915 bytes result sent to driver
15/10/09 08:10:16 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396211000.bk
15/10/09 08:10:16 INFO CheckpointWriter: Checkpoint for time 1444396216000 ms saved to file 'file:/tmp/checkpoint-1444396216000', took 4049 bytes and 12 ms
15/10/09 08:10:16 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 13) in 3 ms on localhost (1/1)
15/10/09 08:10:16 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
15/10/09 08:10:16 INFO DAGScheduler: ResultStage 13 (foreachRDD at Events.scala:26) finished in 0.003 s
15/10/09 08:10:16 INFO DAGScheduler: Job 13 finished: foreachRDD at Events.scala:26, took 0.011193 s
15/10/09 08:10:16 INFO JobScheduler: Finished job streaming job 1444396216000 ms.0 from job set of time 1444396216000 ms
15/10/09 08:10:16 INFO JobScheduler: Total delay: 0.021 s for time 1444396216000 ms (execution: 0.014 s)
15/10/09 08:10:16 INFO MapPartitionsRDD: Removing RDD 25 from persistence list
15/10/09 08:10:16 INFO BlockManager: Removing RDD 25
15/10/09 08:10:16 INFO KafkaRDD: Removing RDD 24 from persistence list
15/10/09 08:10:16 INFO JobGenerator: Checkpointing graph for time 1444396216000 ms
15/10/09 08:10:16 INFO BlockManager: Removing RDD 24
15/10/09 08:10:16 INFO DStreamGraph: Updating checkpoint data for time 1444396216000 ms
15/10/09 08:10:16 INFO DStreamGraph: Updated checkpoint data for time 1444396216000 ms
15/10/09 08:10:16 INFO CheckpointWriter: Saving checkpoint for time 1444396216000 ms to file 'file:/tmp/checkpoint-1444396216000'
15/10/09 08:10:16 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396211000
15/10/09 08:10:16 INFO CheckpointWriter: Checkpoint for time 1444396216000 ms saved to file 'file:/tmp/checkpoint-1444396216000', took 4010 bytes and 10 ms
15/10/09 08:10:16 INFO DStreamGraph: Clearing checkpoint data for time 1444396216000 ms
15/10/09 08:10:16 INFO DStreamGraph: Cleared checkpoint data for time 1444396216000 ms
15/10/09 08:10:16 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:16 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396215000: 
15/10/09 08:10:16 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396215000
15/10/09 08:10:16 INFO InputInfoTracker: remove old batch metadata: 1444396214000 ms
15/10/09 08:10:17 INFO JobScheduler: Added jobs for time 1444396217000 ms
15/10/09 08:10:17 INFO JobGenerator: Checkpointing graph for time 1444396217000 ms
15/10/09 08:10:17 INFO DStreamGraph: Updating checkpoint data for time 1444396217000 ms
15/10/09 08:10:17 INFO JobScheduler: Starting job streaming job 1444396217000 ms.0 from job set of time 1444396217000 ms
15/10/09 08:10:17 INFO DStreamGraph: Updated checkpoint data for time 1444396217000 ms
15/10/09 08:10:17 INFO CheckpointWriter: Saving checkpoint for time 1444396217000 ms to file 'file:/tmp/checkpoint-1444396217000'
15/10/09 08:10:17 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:17 INFO DAGScheduler: Got job 14 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:17 INFO DAGScheduler: Final stage: ResultStage 14(foreachRDD at Events.scala:26)
15/10/09 08:10:17 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:17 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:17 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[29] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:17 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=64479, maxMem=555755765
15/10/09 08:10:17 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 2.9 KB, free 529.9 MB)
15/10/09 08:10:17 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=67399, maxMem=555755765
15/10/09 08:10:17 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 1686.0 B, free 529.9 MB)
15/10/09 08:10:17 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:17 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[29] at map at Events.scala:23)
15/10/09 08:10:17 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
15/10/09 08:10:17 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396212000.bk
15/10/09 08:10:17 INFO CheckpointWriter: Checkpoint for time 1444396217000 ms saved to file 'file:/tmp/checkpoint-1444396217000', took 4049 bytes and 10 ms
15/10/09 08:10:17 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, ANY, 2085 bytes)
15/10/09 08:10:17 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
15/10/09 08:10:17 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:17 ERROR Executor: Exception in task 0.0 in stage 14.0 (TID 14)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:17 WARN TaskSetManager: Lost task 0.0 in stage 14.0 (TID 14, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:17 ERROR TaskSetManager: Task 0 in stage 14.0 failed 1 times; aborting job
15/10/09 08:10:17 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
15/10/09 08:10:17 INFO TaskSchedulerImpl: Cancelling stage 14
15/10/09 08:10:17 INFO DAGScheduler: ResultStage 14 (foreachRDD at Events.scala:26) failed in 0.004 s
15/10/09 08:10:17 INFO DAGScheduler: Job 14 failed: foreachRDD at Events.scala:26, took 0.011549 s
15/10/09 08:10:17 INFO JobScheduler: Finished job streaming job 1444396217000 ms.0 from job set of time 1444396217000 ms
15/10/09 08:10:17 INFO JobScheduler: Total delay: 0.023 s for time 1444396217000 ms (execution: 0.016 s)
15/10/09 08:10:17 INFO MapPartitionsRDD: Removing RDD 27 from persistence list
15/10/09 08:10:17 ERROR JobScheduler: Error running job streaming job 1444396217000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 14.0 failed 1 times, most recent failure: Lost task 0.0 in stage 14.0 (TID 14, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:17 INFO BlockManager: Removing RDD 27
15/10/09 08:10:17 INFO KafkaRDD: Removing RDD 26 from persistence list
15/10/09 08:10:17 INFO BlockManager: Removing RDD 26
15/10/09 08:10:17 INFO JobGenerator: Checkpointing graph for time 1444396217000 ms
15/10/09 08:10:17 INFO DStreamGraph: Updating checkpoint data for time 1444396217000 ms
15/10/09 08:10:17 INFO DStreamGraph: Updated checkpoint data for time 1444396217000 ms
15/10/09 08:10:17 INFO CheckpointWriter: Saving checkpoint for time 1444396217000 ms to file 'file:/tmp/checkpoint-1444396217000'
15/10/09 08:10:17 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396212000
15/10/09 08:10:17 INFO CheckpointWriter: Checkpoint for time 1444396217000 ms saved to file 'file:/tmp/checkpoint-1444396217000', took 4010 bytes and 8 ms
15/10/09 08:10:17 INFO DStreamGraph: Clearing checkpoint data for time 1444396217000 ms
15/10/09 08:10:17 INFO DStreamGraph: Cleared checkpoint data for time 1444396217000 ms
15/10/09 08:10:17 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:17 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396216000: 
15/10/09 08:10:17 INFO InputInfoTracker: remove old batch metadata: 1444396215000 ms
15/10/09 08:10:17 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396216000
15/10/09 08:10:18 INFO JobScheduler: Added jobs for time 1444396218000 ms
15/10/09 08:10:18 INFO JobGenerator: Checkpointing graph for time 1444396218000 ms
15/10/09 08:10:18 INFO DStreamGraph: Updating checkpoint data for time 1444396218000 ms
15/10/09 08:10:18 INFO JobScheduler: Starting job streaming job 1444396218000 ms.0 from job set of time 1444396218000 ms
15/10/09 08:10:18 INFO DStreamGraph: Updated checkpoint data for time 1444396218000 ms
15/10/09 08:10:18 INFO CheckpointWriter: Saving checkpoint for time 1444396218000 ms to file 'file:/tmp/checkpoint-1444396218000'
15/10/09 08:10:18 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:18 INFO DAGScheduler: Got job 15 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:18 INFO DAGScheduler: Final stage: ResultStage 15(foreachRDD at Events.scala:26)
15/10/09 08:10:18 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:18 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:18 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[31] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:18 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=69085, maxMem=555755765
15/10/09 08:10:18 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 2.9 KB, free 529.9 MB)
15/10/09 08:10:18 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=72005, maxMem=555755765
15/10/09 08:10:18 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 1686.0 B, free 529.9 MB)
15/10/09 08:10:18 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:18 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[31] at map at Events.scala:23)
15/10/09 08:10:18 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
15/10/09 08:10:18 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, localhost, ANY, 2085 bytes)
15/10/09 08:10:18 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
15/10/09 08:10:18 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396213000.bk
15/10/09 08:10:18 INFO CheckpointWriter: Checkpoint for time 1444396218000 ms saved to file 'file:/tmp/checkpoint-1444396218000', took 4045 bytes and 12 ms
15/10/09 08:10:18 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:18 INFO Executor: Finished task 0.0 in stage 15.0 (TID 15). 915 bytes result sent to driver
15/10/09 08:10:18 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 4 ms on localhost (1/1)
15/10/09 08:10:18 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
15/10/09 08:10:18 INFO DAGScheduler: ResultStage 15 (foreachRDD at Events.scala:26) finished in 0.004 s
15/10/09 08:10:18 INFO DAGScheduler: Job 15 finished: foreachRDD at Events.scala:26, took 0.011199 s
15/10/09 08:10:18 INFO JobScheduler: Finished job streaming job 1444396218000 ms.0 from job set of time 1444396218000 ms
15/10/09 08:10:18 INFO JobScheduler: Total delay: 0.026 s for time 1444396218000 ms (execution: 0.014 s)
15/10/09 08:10:18 INFO MapPartitionsRDD: Removing RDD 29 from persistence list
15/10/09 08:10:18 INFO KafkaRDD: Removing RDD 28 from persistence list
15/10/09 08:10:18 INFO BlockManager: Removing RDD 29
15/10/09 08:10:18 INFO JobGenerator: Checkpointing graph for time 1444396218000 ms
15/10/09 08:10:18 INFO DStreamGraph: Updating checkpoint data for time 1444396218000 ms
15/10/09 08:10:18 INFO BlockManager: Removing RDD 28
15/10/09 08:10:18 INFO DStreamGraph: Updated checkpoint data for time 1444396218000 ms
15/10/09 08:10:18 INFO CheckpointWriter: Saving checkpoint for time 1444396218000 ms to file 'file:/tmp/checkpoint-1444396218000'
15/10/09 08:10:18 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396213000
15/10/09 08:10:18 INFO CheckpointWriter: Checkpoint for time 1444396218000 ms saved to file 'file:/tmp/checkpoint-1444396218000', took 4010 bytes and 9 ms
15/10/09 08:10:18 INFO DStreamGraph: Clearing checkpoint data for time 1444396218000 ms
15/10/09 08:10:18 INFO DStreamGraph: Cleared checkpoint data for time 1444396218000 ms
15/10/09 08:10:18 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:18 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396217000: 
15/10/09 08:10:18 INFO InputInfoTracker: remove old batch metadata: 1444396216000 ms
15/10/09 08:10:18 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396217000
15/10/09 08:10:19 INFO JobScheduler: Added jobs for time 1444396219000 ms
15/10/09 08:10:19 INFO JobGenerator: Checkpointing graph for time 1444396219000 ms
15/10/09 08:10:19 INFO DStreamGraph: Updating checkpoint data for time 1444396219000 ms
15/10/09 08:10:19 INFO JobScheduler: Starting job streaming job 1444396219000 ms.0 from job set of time 1444396219000 ms
15/10/09 08:10:19 INFO DStreamGraph: Updated checkpoint data for time 1444396219000 ms
15/10/09 08:10:19 INFO CheckpointWriter: Saving checkpoint for time 1444396219000 ms to file 'file:/tmp/checkpoint-1444396219000'
15/10/09 08:10:19 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:19 INFO DAGScheduler: Got job 16 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:19 INFO DAGScheduler: Final stage: ResultStage 16(foreachRDD at Events.scala:26)
15/10/09 08:10:19 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:19 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:19 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[33] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:19 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=73691, maxMem=555755765
15/10/09 08:10:19 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 2.9 KB, free 529.9 MB)
15/10/09 08:10:19 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=76611, maxMem=555755765
15/10/09 08:10:19 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 1686.0 B, free 529.9 MB)
15/10/09 08:10:19 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:19 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:19 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[33] at map at Events.scala:23)
15/10/09 08:10:19 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
15/10/09 08:10:19 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, localhost, ANY, 2085 bytes)
15/10/09 08:10:19 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
15/10/09 08:10:19 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:19 ERROR Executor: Exception in task 0.0 in stage 16.0 (TID 16)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:19 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396214000.bk
15/10/09 08:10:19 INFO CheckpointWriter: Checkpoint for time 1444396219000 ms saved to file 'file:/tmp/checkpoint-1444396219000', took 4049 bytes and 12 ms
15/10/09 08:10:19 WARN TaskSetManager: Lost task 0.0 in stage 16.0 (TID 16, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:19 ERROR TaskSetManager: Task 0 in stage 16.0 failed 1 times; aborting job
15/10/09 08:10:19 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
15/10/09 08:10:19 INFO TaskSchedulerImpl: Cancelling stage 16
15/10/09 08:10:19 INFO DAGScheduler: ResultStage 16 (foreachRDD at Events.scala:26) failed in 0.003 s
15/10/09 08:10:19 INFO DAGScheduler: Job 16 failed: foreachRDD at Events.scala:26, took 0.010736 s
15/10/09 08:10:19 INFO JobScheduler: Finished job streaming job 1444396219000 ms.0 from job set of time 1444396219000 ms
15/10/09 08:10:19 INFO JobScheduler: Total delay: 0.027 s for time 1444396219000 ms (execution: 0.014 s)
15/10/09 08:10:19 INFO MapPartitionsRDD: Removing RDD 31 from persistence list
15/10/09 08:10:19 ERROR JobScheduler: Error running job streaming job 1444396219000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 16.0 failed 1 times, most recent failure: Lost task 0.0 in stage 16.0 (TID 16, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:19 INFO BlockManager: Removing RDD 31
15/10/09 08:10:19 INFO KafkaRDD: Removing RDD 30 from persistence list
15/10/09 08:10:19 INFO JobGenerator: Checkpointing graph for time 1444396219000 ms
15/10/09 08:10:19 INFO DStreamGraph: Updating checkpoint data for time 1444396219000 ms
15/10/09 08:10:19 INFO BlockManager: Removing RDD 30
15/10/09 08:10:19 INFO DStreamGraph: Updated checkpoint data for time 1444396219000 ms
15/10/09 08:10:19 INFO CheckpointWriter: Saving checkpoint for time 1444396219000 ms to file 'file:/tmp/checkpoint-1444396219000'
15/10/09 08:10:19 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396214000
15/10/09 08:10:19 INFO CheckpointWriter: Checkpoint for time 1444396219000 ms saved to file 'file:/tmp/checkpoint-1444396219000', took 4010 bytes and 8 ms
15/10/09 08:10:19 INFO DStreamGraph: Clearing checkpoint data for time 1444396219000 ms
15/10/09 08:10:19 INFO DStreamGraph: Cleared checkpoint data for time 1444396219000 ms
15/10/09 08:10:19 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:19 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396218000: 
15/10/09 08:10:19 INFO InputInfoTracker: remove old batch metadata: 1444396217000 ms
15/10/09 08:10:19 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396218000
15/10/09 08:10:20 INFO JobScheduler: Added jobs for time 1444396220000 ms
15/10/09 08:10:20 INFO JobGenerator: Checkpointing graph for time 1444396220000 ms
15/10/09 08:10:20 INFO DStreamGraph: Updating checkpoint data for time 1444396220000 ms
15/10/09 08:10:20 INFO JobScheduler: Starting job streaming job 1444396220000 ms.0 from job set of time 1444396220000 ms
15/10/09 08:10:20 INFO DStreamGraph: Updated checkpoint data for time 1444396220000 ms
15/10/09 08:10:20 INFO CheckpointWriter: Saving checkpoint for time 1444396220000 ms to file 'file:/tmp/checkpoint-1444396220000'
15/10/09 08:10:20 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:20 INFO DAGScheduler: Got job 17 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:20 INFO DAGScheduler: Final stage: ResultStage 17(foreachRDD at Events.scala:26)
15/10/09 08:10:20 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:20 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:20 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[35] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:20 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=78297, maxMem=555755765
15/10/09 08:10:20 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 2.9 KB, free 529.9 MB)
15/10/09 08:10:20 INFO MemoryStore: ensureFreeSpace(1684) called with curMem=81217, maxMem=555755765
15/10/09 08:10:20 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 1684.0 B, free 529.9 MB)
15/10/09 08:10:20 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:57166 (size: 1684.0 B, free: 530.0 MB)
15/10/09 08:10:20 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[35] at map at Events.scala:23)
15/10/09 08:10:20 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
15/10/09 08:10:20 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17, localhost, ANY, 2085 bytes)
15/10/09 08:10:20 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
15/10/09 08:10:20 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:20 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 915 bytes result sent to driver
15/10/09 08:10:20 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396215000.bk
15/10/09 08:10:20 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 2 ms on localhost (1/1)
15/10/09 08:10:20 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
15/10/09 08:10:20 INFO DAGScheduler: ResultStage 17 (foreachRDD at Events.scala:26) finished in 0.002 s
15/10/09 08:10:20 INFO CheckpointWriter: Checkpoint for time 1444396220000 ms saved to file 'file:/tmp/checkpoint-1444396220000', took 4049 bytes and 11 ms
15/10/09 08:10:20 INFO DAGScheduler: Job 17 finished: foreachRDD at Events.scala:26, took 0.009087 s
15/10/09 08:10:20 INFO JobScheduler: Finished job streaming job 1444396220000 ms.0 from job set of time 1444396220000 ms
15/10/09 08:10:20 INFO JobScheduler: Total delay: 0.020 s for time 1444396220000 ms (execution: 0.013 s)
15/10/09 08:10:20 INFO MapPartitionsRDD: Removing RDD 33 from persistence list
15/10/09 08:10:20 INFO BlockManager: Removing RDD 33
15/10/09 08:10:20 INFO KafkaRDD: Removing RDD 32 from persistence list
15/10/09 08:10:20 INFO BlockManager: Removing RDD 32
15/10/09 08:10:20 INFO JobGenerator: Checkpointing graph for time 1444396220000 ms
15/10/09 08:10:20 INFO DStreamGraph: Updating checkpoint data for time 1444396220000 ms
15/10/09 08:10:20 INFO DStreamGraph: Updated checkpoint data for time 1444396220000 ms
15/10/09 08:10:20 INFO CheckpointWriter: Saving checkpoint for time 1444396220000 ms to file 'file:/tmp/checkpoint-1444396220000'
15/10/09 08:10:20 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396215000
15/10/09 08:10:20 INFO CheckpointWriter: Checkpoint for time 1444396220000 ms saved to file 'file:/tmp/checkpoint-1444396220000', took 4010 bytes and 8 ms
15/10/09 08:10:20 INFO DStreamGraph: Clearing checkpoint data for time 1444396220000 ms
15/10/09 08:10:20 INFO DStreamGraph: Cleared checkpoint data for time 1444396220000 ms
15/10/09 08:10:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:20 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396219000: 
15/10/09 08:10:20 INFO InputInfoTracker: remove old batch metadata: 1444396218000 ms
15/10/09 08:10:20 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396219000
15/10/09 08:10:21 INFO JobScheduler: Added jobs for time 1444396221000 ms
15/10/09 08:10:21 INFO JobGenerator: Checkpointing graph for time 1444396221000 ms
15/10/09 08:10:21 INFO DStreamGraph: Updating checkpoint data for time 1444396221000 ms
15/10/09 08:10:21 INFO JobScheduler: Starting job streaming job 1444396221000 ms.0 from job set of time 1444396221000 ms
15/10/09 08:10:21 INFO DStreamGraph: Updated checkpoint data for time 1444396221000 ms
15/10/09 08:10:21 INFO CheckpointWriter: Saving checkpoint for time 1444396221000 ms to file 'file:/tmp/checkpoint-1444396221000'
15/10/09 08:10:21 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:21 INFO DAGScheduler: Got job 18 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:21 INFO DAGScheduler: Final stage: ResultStage 18(foreachRDD at Events.scala:26)
15/10/09 08:10:21 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:21 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:21 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[37] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:21 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=82901, maxMem=555755765
15/10/09 08:10:21 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 2.9 KB, free 529.9 MB)
15/10/09 08:10:21 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=85821, maxMem=555755765
15/10/09 08:10:21 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 1686.0 B, free 529.9 MB)
15/10/09 08:10:21 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:21 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[37] at map at Events.scala:23)
15/10/09 08:10:21 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
15/10/09 08:10:21 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18, localhost, ANY, 2085 bytes)
15/10/09 08:10:21 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
15/10/09 08:10:21 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:21 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 915 bytes result sent to driver
15/10/09 08:10:21 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396216000.bk
15/10/09 08:10:21 INFO CheckpointWriter: Checkpoint for time 1444396221000 ms saved to file 'file:/tmp/checkpoint-1444396221000', took 4045 bytes and 10 ms
15/10/09 08:10:21 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 2 ms on localhost (1/1)
15/10/09 08:10:21 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
15/10/09 08:10:21 INFO DAGScheduler: ResultStage 18 (foreachRDD at Events.scala:26) finished in 0.003 s
15/10/09 08:10:21 INFO DAGScheduler: Job 18 finished: foreachRDD at Events.scala:26, took 0.009000 s
15/10/09 08:10:21 INFO JobScheduler: Finished job streaming job 1444396221000 ms.0 from job set of time 1444396221000 ms
15/10/09 08:10:21 INFO JobScheduler: Total delay: 0.020 s for time 1444396221000 ms (execution: 0.012 s)
15/10/09 08:10:21 INFO MapPartitionsRDD: Removing RDD 35 from persistence list
15/10/09 08:10:21 INFO KafkaRDD: Removing RDD 34 from persistence list
15/10/09 08:10:21 INFO BlockManager: Removing RDD 35
15/10/09 08:10:21 INFO JobGenerator: Checkpointing graph for time 1444396221000 ms
15/10/09 08:10:21 INFO DStreamGraph: Updating checkpoint data for time 1444396221000 ms
15/10/09 08:10:21 INFO BlockManager: Removing RDD 34
15/10/09 08:10:21 INFO DStreamGraph: Updated checkpoint data for time 1444396221000 ms
15/10/09 08:10:21 INFO CheckpointWriter: Saving checkpoint for time 1444396221000 ms to file 'file:/tmp/checkpoint-1444396221000'
15/10/09 08:10:21 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396216000
15/10/09 08:10:21 INFO CheckpointWriter: Checkpoint for time 1444396221000 ms saved to file 'file:/tmp/checkpoint-1444396221000', took 4010 bytes and 8 ms
15/10/09 08:10:21 INFO DStreamGraph: Clearing checkpoint data for time 1444396221000 ms
15/10/09 08:10:21 INFO DStreamGraph: Cleared checkpoint data for time 1444396221000 ms
15/10/09 08:10:21 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:21 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396220000: 
15/10/09 08:10:21 INFO InputInfoTracker: remove old batch metadata: 1444396219000 ms
15/10/09 08:10:21 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396220000
15/10/09 08:10:22 INFO JobScheduler: Added jobs for time 1444396222000 ms
15/10/09 08:10:22 INFO JobGenerator: Checkpointing graph for time 1444396222000 ms
15/10/09 08:10:22 INFO DStreamGraph: Updating checkpoint data for time 1444396222000 ms
15/10/09 08:10:22 INFO JobScheduler: Starting job streaming job 1444396222000 ms.0 from job set of time 1444396222000 ms
15/10/09 08:10:22 INFO DStreamGraph: Updated checkpoint data for time 1444396222000 ms
15/10/09 08:10:22 INFO CheckpointWriter: Saving checkpoint for time 1444396222000 ms to file 'file:/tmp/checkpoint-1444396222000'
15/10/09 08:10:22 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:22 INFO DAGScheduler: Got job 19 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:22 INFO DAGScheduler: Final stage: ResultStage 19(foreachRDD at Events.scala:26)
15/10/09 08:10:22 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:22 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:22 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[39] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:22 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=87507, maxMem=555755765
15/10/09 08:10:22 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 2.9 KB, free 529.9 MB)
15/10/09 08:10:22 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=90427, maxMem=555755765
15/10/09 08:10:22 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 1686.0 B, free 529.9 MB)
15/10/09 08:10:22 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:22 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[39] at map at Events.scala:23)
15/10/09 08:10:22 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
15/10/09 08:10:22 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, localhost, ANY, 2085 bytes)
15/10/09 08:10:22 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
15/10/09 08:10:22 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:22 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 915 bytes result sent to driver
15/10/09 08:10:22 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396217000.bk
15/10/09 08:10:22 INFO CheckpointWriter: Checkpoint for time 1444396222000 ms saved to file 'file:/tmp/checkpoint-1444396222000', took 4045 bytes and 11 ms
15/10/09 08:10:22 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 3 ms on localhost (1/1)
15/10/09 08:10:22 INFO DAGScheduler: ResultStage 19 (foreachRDD at Events.scala:26) finished in 0.003 s
15/10/09 08:10:22 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
15/10/09 08:10:22 INFO DAGScheduler: Job 19 finished: foreachRDD at Events.scala:26, took 0.009894 s
15/10/09 08:10:22 INFO JobScheduler: Finished job streaming job 1444396222000 ms.0 from job set of time 1444396222000 ms
15/10/09 08:10:22 INFO MapPartitionsRDD: Removing RDD 37 from persistence list
15/10/09 08:10:22 INFO KafkaRDD: Removing RDD 36 from persistence list
15/10/09 08:10:22 INFO BlockManager: Removing RDD 37
15/10/09 08:10:22 INFO JobScheduler: Total delay: 0.021 s for time 1444396222000 ms (execution: 0.013 s)
15/10/09 08:10:22 INFO JobGenerator: Checkpointing graph for time 1444396222000 ms
15/10/09 08:10:22 INFO DStreamGraph: Updating checkpoint data for time 1444396222000 ms
15/10/09 08:10:22 INFO BlockManager: Removing RDD 36
15/10/09 08:10:22 INFO DStreamGraph: Updated checkpoint data for time 1444396222000 ms
15/10/09 08:10:22 INFO CheckpointWriter: Saving checkpoint for time 1444396222000 ms to file 'file:/tmp/checkpoint-1444396222000'
15/10/09 08:10:22 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396217000
15/10/09 08:10:22 INFO CheckpointWriter: Checkpoint for time 1444396222000 ms saved to file 'file:/tmp/checkpoint-1444396222000', took 4010 bytes and 8 ms
15/10/09 08:10:22 INFO DStreamGraph: Clearing checkpoint data for time 1444396222000 ms
15/10/09 08:10:22 INFO DStreamGraph: Cleared checkpoint data for time 1444396222000 ms
15/10/09 08:10:22 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:22 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396221000: 
15/10/09 08:10:22 INFO InputInfoTracker: remove old batch metadata: 1444396220000 ms
15/10/09 08:10:22 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396221000
15/10/09 08:10:23 INFO JobScheduler: Added jobs for time 1444396223000 ms
15/10/09 08:10:23 INFO JobGenerator: Checkpointing graph for time 1444396223000 ms
15/10/09 08:10:23 INFO DStreamGraph: Updating checkpoint data for time 1444396223000 ms
15/10/09 08:10:23 INFO JobScheduler: Starting job streaming job 1444396223000 ms.0 from job set of time 1444396223000 ms
15/10/09 08:10:23 INFO DStreamGraph: Updated checkpoint data for time 1444396223000 ms
15/10/09 08:10:23 INFO CheckpointWriter: Saving checkpoint for time 1444396223000 ms to file 'file:/tmp/checkpoint-1444396223000'
15/10/09 08:10:23 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:23 INFO DAGScheduler: Got job 20 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:23 INFO DAGScheduler: Final stage: ResultStage 20(foreachRDD at Events.scala:26)
15/10/09 08:10:23 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:23 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:23 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[41] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:23 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=92113, maxMem=555755765
15/10/09 08:10:23 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 2.9 KB, free 529.9 MB)
15/10/09 08:10:23 INFO MemoryStore: ensureFreeSpace(1687) called with curMem=95033, maxMem=555755765
15/10/09 08:10:23 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 1687.0 B, free 529.9 MB)
15/10/09 08:10:23 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:57166 (size: 1687.0 B, free: 530.0 MB)
15/10/09 08:10:23 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[41] at map at Events.scala:23)
15/10/09 08:10:23 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
15/10/09 08:10:23 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20, localhost, ANY, 2085 bytes)
15/10/09 08:10:23 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
15/10/09 08:10:23 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:23 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 915 bytes result sent to driver
15/10/09 08:10:23 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 2 ms on localhost (1/1)
15/10/09 08:10:23 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
15/10/09 08:10:23 INFO DAGScheduler: ResultStage 20 (foreachRDD at Events.scala:26) finished in 0.002 s
15/10/09 08:10:23 INFO DAGScheduler: Job 20 finished: foreachRDD at Events.scala:26, took 0.009046 s
15/10/09 08:10:23 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396218000.bk
15/10/09 08:10:23 INFO JobScheduler: Finished job streaming job 1444396223000 ms.0 from job set of time 1444396223000 ms
15/10/09 08:10:23 INFO CheckpointWriter: Checkpoint for time 1444396223000 ms saved to file 'file:/tmp/checkpoint-1444396223000', took 4049 bytes and 12 ms
15/10/09 08:10:23 INFO JobScheduler: Total delay: 0.021 s for time 1444396223000 ms (execution: 0.013 s)
15/10/09 08:10:23 INFO MapPartitionsRDD: Removing RDD 39 from persistence list
15/10/09 08:10:23 INFO BlockManager: Removing RDD 39
15/10/09 08:10:23 INFO KafkaRDD: Removing RDD 38 from persistence list
15/10/09 08:10:23 INFO JobGenerator: Checkpointing graph for time 1444396223000 ms
15/10/09 08:10:23 INFO BlockManager: Removing RDD 38
15/10/09 08:10:23 INFO DStreamGraph: Updating checkpoint data for time 1444396223000 ms
15/10/09 08:10:23 INFO DStreamGraph: Updated checkpoint data for time 1444396223000 ms
15/10/09 08:10:23 INFO CheckpointWriter: Saving checkpoint for time 1444396223000 ms to file 'file:/tmp/checkpoint-1444396223000'
15/10/09 08:10:23 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396218000
15/10/09 08:10:23 INFO CheckpointWriter: Checkpoint for time 1444396223000 ms saved to file 'file:/tmp/checkpoint-1444396223000', took 4010 bytes and 7 ms
15/10/09 08:10:23 INFO DStreamGraph: Clearing checkpoint data for time 1444396223000 ms
15/10/09 08:10:23 INFO DStreamGraph: Cleared checkpoint data for time 1444396223000 ms
15/10/09 08:10:23 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:23 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396222000: 
15/10/09 08:10:23 INFO InputInfoTracker: remove old batch metadata: 1444396221000 ms
15/10/09 08:10:23 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396222000
15/10/09 08:10:24 INFO JobScheduler: Added jobs for time 1444396224000 ms
15/10/09 08:10:24 INFO JobGenerator: Checkpointing graph for time 1444396224000 ms
15/10/09 08:10:24 INFO DStreamGraph: Updating checkpoint data for time 1444396224000 ms
15/10/09 08:10:24 INFO JobScheduler: Starting job streaming job 1444396224000 ms.0 from job set of time 1444396224000 ms
15/10/09 08:10:24 INFO DStreamGraph: Updated checkpoint data for time 1444396224000 ms
15/10/09 08:10:24 INFO CheckpointWriter: Saving checkpoint for time 1444396224000 ms to file 'file:/tmp/checkpoint-1444396224000'
15/10/09 08:10:24 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:24 INFO DAGScheduler: Got job 21 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:24 INFO DAGScheduler: Final stage: ResultStage 21(foreachRDD at Events.scala:26)
15/10/09 08:10:24 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:24 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:24 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[43] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:24 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=96720, maxMem=555755765
15/10/09 08:10:24 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 2.9 KB, free 529.9 MB)
15/10/09 08:10:24 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=99640, maxMem=555755765
15/10/09 08:10:24 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 1686.0 B, free 529.9 MB)
15/10/09 08:10:24 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:24 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[43] at map at Events.scala:23)
15/10/09 08:10:24 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
15/10/09 08:10:24 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21, localhost, ANY, 2085 bytes)
15/10/09 08:10:24 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
15/10/09 08:10:24 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:24 ERROR Executor: Exception in task 0.0 in stage 21.0 (TID 21)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:24 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396219000.bk
15/10/09 08:10:24 INFO CheckpointWriter: Checkpoint for time 1444396224000 ms saved to file 'file:/tmp/checkpoint-1444396224000', took 4052 bytes and 11 ms
15/10/09 08:10:24 WARN TaskSetManager: Lost task 0.0 in stage 21.0 (TID 21, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:24 ERROR TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job
15/10/09 08:10:24 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
15/10/09 08:10:24 INFO TaskSchedulerImpl: Cancelling stage 21
15/10/09 08:10:24 INFO DAGScheduler: ResultStage 21 (foreachRDD at Events.scala:26) failed in 0.003 s
15/10/09 08:10:24 INFO DAGScheduler: Job 21 failed: foreachRDD at Events.scala:26, took 0.010020 s
15/10/09 08:10:24 INFO JobScheduler: Finished job streaming job 1444396224000 ms.0 from job set of time 1444396224000 ms
15/10/09 08:10:24 INFO JobScheduler: Total delay: 0.023 s for time 1444396224000 ms (execution: 0.014 s)
15/10/09 08:10:24 INFO MapPartitionsRDD: Removing RDD 41 from persistence list
15/10/09 08:10:24 ERROR JobScheduler: Error running job streaming job 1444396224000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:24 INFO BlockManager: Removing RDD 41
15/10/09 08:10:24 INFO KafkaRDD: Removing RDD 40 from persistence list
15/10/09 08:10:24 INFO JobGenerator: Checkpointing graph for time 1444396224000 ms
15/10/09 08:10:24 INFO DStreamGraph: Updating checkpoint data for time 1444396224000 ms
15/10/09 08:10:24 INFO BlockManager: Removing RDD 40
15/10/09 08:10:24 INFO DStreamGraph: Updated checkpoint data for time 1444396224000 ms
15/10/09 08:10:24 INFO CheckpointWriter: Saving checkpoint for time 1444396224000 ms to file 'file:/tmp/checkpoint-1444396224000'
15/10/09 08:10:24 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396219000
15/10/09 08:10:24 INFO CheckpointWriter: Checkpoint for time 1444396224000 ms saved to file 'file:/tmp/checkpoint-1444396224000', took 4012 bytes and 7 ms
15/10/09 08:10:24 INFO DStreamGraph: Clearing checkpoint data for time 1444396224000 ms
15/10/09 08:10:24 INFO DStreamGraph: Cleared checkpoint data for time 1444396224000 ms
15/10/09 08:10:24 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:24 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396223000: 
15/10/09 08:10:24 INFO InputInfoTracker: remove old batch metadata: 1444396222000 ms
15/10/09 08:10:24 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396223000
15/10/09 08:10:25 INFO JobScheduler: Added jobs for time 1444396225000 ms
15/10/09 08:10:25 INFO JobGenerator: Checkpointing graph for time 1444396225000 ms
15/10/09 08:10:25 INFO DStreamGraph: Updating checkpoint data for time 1444396225000 ms
15/10/09 08:10:25 INFO JobScheduler: Starting job streaming job 1444396225000 ms.0 from job set of time 1444396225000 ms
15/10/09 08:10:25 INFO DStreamGraph: Updated checkpoint data for time 1444396225000 ms
15/10/09 08:10:25 INFO CheckpointWriter: Saving checkpoint for time 1444396225000 ms to file 'file:/tmp/checkpoint-1444396225000'
15/10/09 08:10:25 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:25 INFO DAGScheduler: Got job 22 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:25 INFO DAGScheduler: Final stage: ResultStage 22(foreachRDD at Events.scala:26)
15/10/09 08:10:25 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:25 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:25 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[45] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:25 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=101326, maxMem=555755765
15/10/09 08:10:25 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 2.9 KB, free 529.9 MB)
15/10/09 08:10:25 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=104246, maxMem=555755765
15/10/09 08:10:25 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 1686.0 B, free 529.9 MB)
15/10/09 08:10:25 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[45] at map at Events.scala:23)
15/10/09 08:10:25 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
15/10/09 08:10:25 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, localhost, ANY, 2085 bytes)
15/10/09 08:10:25 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
15/10/09 08:10:25 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:25 ERROR Executor: Exception in task 0.0 in stage 22.0 (TID 22)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:25 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396220000.bk
15/10/09 08:10:25 INFO CheckpointWriter: Checkpoint for time 1444396225000 ms saved to file 'file:/tmp/checkpoint-1444396225000', took 4049 bytes and 16 ms
15/10/09 08:10:25 WARN TaskSetManager: Lost task 0.0 in stage 22.0 (TID 22, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:25 ERROR TaskSetManager: Task 0 in stage 22.0 failed 1 times; aborting job
15/10/09 08:10:25 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
15/10/09 08:10:25 INFO TaskSchedulerImpl: Cancelling stage 22
15/10/09 08:10:25 INFO DAGScheduler: ResultStage 22 (foreachRDD at Events.scala:26) failed in 0.004 s
15/10/09 08:10:25 INFO DAGScheduler: Job 22 failed: foreachRDD at Events.scala:26, took 0.015032 s
15/10/09 08:10:25 INFO JobScheduler: Finished job streaming job 1444396225000 ms.0 from job set of time 1444396225000 ms
15/10/09 08:10:25 INFO JobScheduler: Total delay: 0.027 s for time 1444396225000 ms (execution: 0.019 s)
15/10/09 08:10:25 INFO MapPartitionsRDD: Removing RDD 43 from persistence list
15/10/09 08:10:25 ERROR JobScheduler: Error running job streaming job 1444396225000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 22.0 failed 1 times, most recent failure: Lost task 0.0 in stage 22.0 (TID 22, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:25 INFO BlockManager: Removing RDD 43
15/10/09 08:10:25 INFO KafkaRDD: Removing RDD 42 from persistence list
15/10/09 08:10:25 INFO JobGenerator: Checkpointing graph for time 1444396225000 ms
15/10/09 08:10:25 INFO DStreamGraph: Updating checkpoint data for time 1444396225000 ms
15/10/09 08:10:25 INFO BlockManager: Removing RDD 42
15/10/09 08:10:25 INFO DStreamGraph: Updated checkpoint data for time 1444396225000 ms
15/10/09 08:10:25 INFO CheckpointWriter: Saving checkpoint for time 1444396225000 ms to file 'file:/tmp/checkpoint-1444396225000'
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 10
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396220000
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 9
15/10/09 08:10:25 INFO CheckpointWriter: Checkpoint for time 1444396225000 ms saved to file 'file:/tmp/checkpoint-1444396225000', took 4010 bytes and 10 ms
15/10/09 08:10:25 INFO DStreamGraph: Clearing checkpoint data for time 1444396225000 ms
15/10/09 08:10:25 INFO DStreamGraph: Cleared checkpoint data for time 1444396225000 ms
15/10/09 08:10:25 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:25 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396224000: 
15/10/09 08:10:25 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396224000
15/10/09 08:10:25 INFO InputInfoTracker: remove old batch metadata: 1444396223000 ms
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 8
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:57166 in memory (size: 1684.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 7
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 6
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 5
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 4
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:57166 in memory (size: 1685.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 3
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:57166 in memory (size: 1684.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 2
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 1
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_15_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 16
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 15
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 14
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 13
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 12
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 11
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 22
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:57166 in memory (size: 1687.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 21
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 20
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 19
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:57166 in memory (size: 1684.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 18
15/10/09 08:10:25 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:57166 in memory (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:25 INFO ContextCleaner: Cleaned accumulator 17
15/10/09 08:10:26 INFO JobScheduler: Added jobs for time 1444396226000 ms
15/10/09 08:10:26 INFO JobGenerator: Checkpointing graph for time 1444396226000 ms
15/10/09 08:10:26 INFO DStreamGraph: Updating checkpoint data for time 1444396226000 ms
15/10/09 08:10:26 INFO JobScheduler: Starting job streaming job 1444396226000 ms.0 from job set of time 1444396226000 ms
15/10/09 08:10:26 INFO DStreamGraph: Updated checkpoint data for time 1444396226000 ms
15/10/09 08:10:26 INFO CheckpointWriter: Saving checkpoint for time 1444396226000 ms to file 'file:/tmp/checkpoint-1444396226000'
15/10/09 08:10:26 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:26 INFO DAGScheduler: Got job 23 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:26 INFO DAGScheduler: Final stage: ResultStage 23(foreachRDD at Events.scala:26)
15/10/09 08:10:26 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:26 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:26 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[47] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:26 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=4606, maxMem=555755765
15/10/09 08:10:26 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:26 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=7526, maxMem=555755765
15/10/09 08:10:26 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:26 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:26 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[47] at map at Events.scala:23)
15/10/09 08:10:26 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
15/10/09 08:10:26 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23, localhost, ANY, 2085 bytes)
15/10/09 08:10:26 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
15/10/09 08:10:26 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:26 ERROR Executor: Exception in task 0.0 in stage 23.0 (TID 23)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:26 WARN TaskSetManager: Lost task 0.0 in stage 23.0 (TID 23, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:26 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396221000.bk
15/10/09 08:10:26 ERROR TaskSetManager: Task 0 in stage 23.0 failed 1 times; aborting job
15/10/09 08:10:26 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
15/10/09 08:10:26 INFO TaskSchedulerImpl: Cancelling stage 23
15/10/09 08:10:26 INFO CheckpointWriter: Checkpoint for time 1444396226000 ms saved to file 'file:/tmp/checkpoint-1444396226000', took 4049 bytes and 9 ms
15/10/09 08:10:26 INFO DAGScheduler: ResultStage 23 (foreachRDD at Events.scala:26) failed in 0.003 s
15/10/09 08:10:26 INFO DAGScheduler: Job 23 failed: foreachRDD at Events.scala:26, took 0.007711 s
15/10/09 08:10:26 INFO JobScheduler: Finished job streaming job 1444396226000 ms.0 from job set of time 1444396226000 ms
15/10/09 08:10:26 INFO JobScheduler: Total delay: 0.019 s for time 1444396226000 ms (execution: 0.011 s)
15/10/09 08:10:26 INFO MapPartitionsRDD: Removing RDD 45 from persistence list
15/10/09 08:10:26 ERROR JobScheduler: Error running job streaming job 1444396226000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 23, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:26 INFO BlockManager: Removing RDD 45
15/10/09 08:10:26 INFO KafkaRDD: Removing RDD 44 from persistence list
15/10/09 08:10:26 INFO JobGenerator: Checkpointing graph for time 1444396226000 ms
15/10/09 08:10:26 INFO BlockManager: Removing RDD 44
15/10/09 08:10:26 INFO DStreamGraph: Updating checkpoint data for time 1444396226000 ms
15/10/09 08:10:26 INFO DStreamGraph: Updated checkpoint data for time 1444396226000 ms
15/10/09 08:10:26 INFO CheckpointWriter: Saving checkpoint for time 1444396226000 ms to file 'file:/tmp/checkpoint-1444396226000'
15/10/09 08:10:26 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396221000
15/10/09 08:10:26 INFO CheckpointWriter: Checkpoint for time 1444396226000 ms saved to file 'file:/tmp/checkpoint-1444396226000', took 4010 bytes and 7 ms
15/10/09 08:10:26 INFO DStreamGraph: Clearing checkpoint data for time 1444396226000 ms
15/10/09 08:10:26 INFO DStreamGraph: Cleared checkpoint data for time 1444396226000 ms
15/10/09 08:10:26 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:26 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396225000: 
15/10/09 08:10:26 INFO InputInfoTracker: remove old batch metadata: 1444396224000 ms
15/10/09 08:10:26 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396225000
15/10/09 08:10:27 INFO JobScheduler: Added jobs for time 1444396227000 ms
15/10/09 08:10:27 INFO JobGenerator: Checkpointing graph for time 1444396227000 ms
15/10/09 08:10:27 INFO DStreamGraph: Updating checkpoint data for time 1444396227000 ms
15/10/09 08:10:27 INFO JobScheduler: Starting job streaming job 1444396227000 ms.0 from job set of time 1444396227000 ms
15/10/09 08:10:27 INFO DStreamGraph: Updated checkpoint data for time 1444396227000 ms
15/10/09 08:10:27 INFO CheckpointWriter: Saving checkpoint for time 1444396227000 ms to file 'file:/tmp/checkpoint-1444396227000'
15/10/09 08:10:27 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:27 INFO DAGScheduler: Got job 24 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:27 INFO DAGScheduler: Final stage: ResultStage 24(foreachRDD at Events.scala:26)
15/10/09 08:10:27 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:27 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:27 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[49] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:27 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=9212, maxMem=555755765
15/10/09 08:10:27 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:27 INFO MemoryStore: ensureFreeSpace(1685) called with curMem=12132, maxMem=555755765
15/10/09 08:10:27 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 1685.0 B, free 530.0 MB)
15/10/09 08:10:27 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:57166 (size: 1685.0 B, free: 530.0 MB)
15/10/09 08:10:27 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[49] at map at Events.scala:23)
15/10/09 08:10:27 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
15/10/09 08:10:27 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24, localhost, ANY, 2085 bytes)
15/10/09 08:10:27 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
15/10/09 08:10:27 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:27 ERROR Executor: Exception in task 0.0 in stage 24.0 (TID 24)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:27 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396222000.bk
15/10/09 08:10:27 WARN TaskSetManager: Lost task 0.0 in stage 24.0 (TID 24, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:27 ERROR TaskSetManager: Task 0 in stage 24.0 failed 1 times; aborting job
15/10/09 08:10:27 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
15/10/09 08:10:27 INFO CheckpointWriter: Checkpoint for time 1444396227000 ms saved to file 'file:/tmp/checkpoint-1444396227000', took 4049 bytes and 9 ms
15/10/09 08:10:27 INFO TaskSchedulerImpl: Cancelling stage 24
15/10/09 08:10:27 INFO DAGScheduler: ResultStage 24 (foreachRDD at Events.scala:26) failed in 0.003 s
15/10/09 08:10:27 INFO DAGScheduler: Job 24 failed: foreachRDD at Events.scala:26, took 0.007873 s
15/10/09 08:10:27 INFO JobScheduler: Finished job streaming job 1444396227000 ms.0 from job set of time 1444396227000 ms
15/10/09 08:10:27 INFO JobScheduler: Total delay: 0.019 s for time 1444396227000 ms (execution: 0.011 s)
15/10/09 08:10:27 INFO MapPartitionsRDD: Removing RDD 47 from persistence list
15/10/09 08:10:27 ERROR JobScheduler: Error running job streaming job 1444396227000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 24, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:27 INFO BlockManager: Removing RDD 47
15/10/09 08:10:27 INFO KafkaRDD: Removing RDD 46 from persistence list
15/10/09 08:10:27 INFO JobGenerator: Checkpointing graph for time 1444396227000 ms
15/10/09 08:10:27 INFO BlockManager: Removing RDD 46
15/10/09 08:10:27 INFO DStreamGraph: Updating checkpoint data for time 1444396227000 ms
15/10/09 08:10:27 INFO DStreamGraph: Updated checkpoint data for time 1444396227000 ms
15/10/09 08:10:27 INFO CheckpointWriter: Saving checkpoint for time 1444396227000 ms to file 'file:/tmp/checkpoint-1444396227000'
15/10/09 08:10:27 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396222000
15/10/09 08:10:27 INFO CheckpointWriter: Checkpoint for time 1444396227000 ms saved to file 'file:/tmp/checkpoint-1444396227000', took 4010 bytes and 7 ms
15/10/09 08:10:27 INFO DStreamGraph: Clearing checkpoint data for time 1444396227000 ms
15/10/09 08:10:27 INFO DStreamGraph: Cleared checkpoint data for time 1444396227000 ms
15/10/09 08:10:27 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:27 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396226000: 
15/10/09 08:10:27 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396226000
15/10/09 08:10:27 INFO InputInfoTracker: remove old batch metadata: 1444396225000 ms
15/10/09 08:10:28 INFO JobScheduler: Added jobs for time 1444396228000 ms
15/10/09 08:10:28 INFO JobGenerator: Checkpointing graph for time 1444396228000 ms
15/10/09 08:10:28 INFO DStreamGraph: Updating checkpoint data for time 1444396228000 ms
15/10/09 08:10:28 INFO JobScheduler: Starting job streaming job 1444396228000 ms.0 from job set of time 1444396228000 ms
15/10/09 08:10:28 INFO DStreamGraph: Updated checkpoint data for time 1444396228000 ms
15/10/09 08:10:28 INFO CheckpointWriter: Saving checkpoint for time 1444396228000 ms to file 'file:/tmp/checkpoint-1444396228000'
15/10/09 08:10:28 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:28 INFO DAGScheduler: Got job 25 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:28 INFO DAGScheduler: Final stage: ResultStage 25(foreachRDD at Events.scala:26)
15/10/09 08:10:28 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:28 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:28 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[51] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:28 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=13817, maxMem=555755765
15/10/09 08:10:28 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:28 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=16737, maxMem=555755765
15/10/09 08:10:28 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:28 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:28 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[51] at map at Events.scala:23)
15/10/09 08:10:28 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
15/10/09 08:10:28 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25, localhost, ANY, 2085 bytes)
15/10/09 08:10:28 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
15/10/09 08:10:28 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:28 INFO Executor: Finished task 0.0 in stage 25.0 (TID 25). 915 bytes result sent to driver
15/10/09 08:10:28 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396223000.bk
15/10/09 08:10:28 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 25) in 2 ms on localhost (1/1)
15/10/09 08:10:28 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
15/10/09 08:10:28 INFO CheckpointWriter: Checkpoint for time 1444396228000 ms saved to file 'file:/tmp/checkpoint-1444396228000', took 4045 bytes and 10 ms
15/10/09 08:10:28 INFO DAGScheduler: ResultStage 25 (foreachRDD at Events.scala:26) finished in 0.003 s
15/10/09 08:10:28 INFO DAGScheduler: Job 25 finished: foreachRDD at Events.scala:26, took 0.008131 s
15/10/09 08:10:28 INFO JobScheduler: Finished job streaming job 1444396228000 ms.0 from job set of time 1444396228000 ms
15/10/09 08:10:28 INFO JobScheduler: Total delay: 0.021 s for time 1444396228000 ms (execution: 0.011 s)
15/10/09 08:10:28 INFO MapPartitionsRDD: Removing RDD 49 from persistence list
15/10/09 08:10:28 INFO BlockManager: Removing RDD 49
15/10/09 08:10:28 INFO KafkaRDD: Removing RDD 48 from persistence list
15/10/09 08:10:28 INFO BlockManager: Removing RDD 48
15/10/09 08:10:28 INFO JobGenerator: Checkpointing graph for time 1444396228000 ms
15/10/09 08:10:28 INFO DStreamGraph: Updating checkpoint data for time 1444396228000 ms
15/10/09 08:10:28 INFO DStreamGraph: Updated checkpoint data for time 1444396228000 ms
15/10/09 08:10:28 INFO CheckpointWriter: Saving checkpoint for time 1444396228000 ms to file 'file:/tmp/checkpoint-1444396228000'
15/10/09 08:10:28 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396223000
15/10/09 08:10:28 INFO CheckpointWriter: Checkpoint for time 1444396228000 ms saved to file 'file:/tmp/checkpoint-1444396228000', took 4010 bytes and 7 ms
15/10/09 08:10:28 INFO DStreamGraph: Clearing checkpoint data for time 1444396228000 ms
15/10/09 08:10:28 INFO DStreamGraph: Cleared checkpoint data for time 1444396228000 ms
15/10/09 08:10:28 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:28 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396227000: 
15/10/09 08:10:28 INFO InputInfoTracker: remove old batch metadata: 1444396226000 ms
15/10/09 08:10:28 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396227000
15/10/09 08:10:29 INFO JobScheduler: Added jobs for time 1444396229000 ms
15/10/09 08:10:29 INFO JobGenerator: Checkpointing graph for time 1444396229000 ms
15/10/09 08:10:29 INFO DStreamGraph: Updating checkpoint data for time 1444396229000 ms
15/10/09 08:10:29 INFO JobScheduler: Starting job streaming job 1444396229000 ms.0 from job set of time 1444396229000 ms
15/10/09 08:10:29 INFO DStreamGraph: Updated checkpoint data for time 1444396229000 ms
15/10/09 08:10:29 INFO CheckpointWriter: Saving checkpoint for time 1444396229000 ms to file 'file:/tmp/checkpoint-1444396229000'
15/10/09 08:10:29 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:29 INFO DAGScheduler: Got job 26 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:29 INFO DAGScheduler: Final stage: ResultStage 26(foreachRDD at Events.scala:26)
15/10/09 08:10:29 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:29 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:29 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[53] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:29 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=18423, maxMem=555755765
15/10/09 08:10:29 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:29 INFO MemoryStore: ensureFreeSpace(1685) called with curMem=21343, maxMem=555755765
15/10/09 08:10:29 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 1685.0 B, free 530.0 MB)
15/10/09 08:10:29 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:57166 (size: 1685.0 B, free: 530.0 MB)
15/10/09 08:10:29 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[53] at map at Events.scala:23)
15/10/09 08:10:29 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
15/10/09 08:10:29 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26, localhost, ANY, 2085 bytes)
15/10/09 08:10:29 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
15/10/09 08:10:29 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:29 INFO Executor: Finished task 0.0 in stage 26.0 (TID 26). 915 bytes result sent to driver
15/10/09 08:10:29 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 26) in 2 ms on localhost (1/1)
15/10/09 08:10:29 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
15/10/09 08:10:29 INFO DAGScheduler: ResultStage 26 (foreachRDD at Events.scala:26) finished in 0.002 s
15/10/09 08:10:29 INFO DAGScheduler: Job 26 finished: foreachRDD at Events.scala:26, took 0.006874 s
15/10/09 08:10:29 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396224000.bk
15/10/09 08:10:29 INFO JobScheduler: Finished job streaming job 1444396229000 ms.0 from job set of time 1444396229000 ms
15/10/09 08:10:29 INFO CheckpointWriter: Checkpoint for time 1444396229000 ms saved to file 'file:/tmp/checkpoint-1444396229000', took 4049 bytes and 9 ms
15/10/09 08:10:29 INFO JobScheduler: Total delay: 0.019 s for time 1444396229000 ms (execution: 0.010 s)
15/10/09 08:10:29 INFO MapPartitionsRDD: Removing RDD 51 from persistence list
15/10/09 08:10:29 INFO KafkaRDD: Removing RDD 50 from persistence list
15/10/09 08:10:29 INFO BlockManager: Removing RDD 51
15/10/09 08:10:29 INFO JobGenerator: Checkpointing graph for time 1444396229000 ms
15/10/09 08:10:29 INFO BlockManager: Removing RDD 50
15/10/09 08:10:29 INFO DStreamGraph: Updating checkpoint data for time 1444396229000 ms
15/10/09 08:10:29 INFO DStreamGraph: Updated checkpoint data for time 1444396229000 ms
15/10/09 08:10:29 INFO CheckpointWriter: Saving checkpoint for time 1444396229000 ms to file 'file:/tmp/checkpoint-1444396229000'
15/10/09 08:10:29 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396224000
15/10/09 08:10:29 INFO CheckpointWriter: Checkpoint for time 1444396229000 ms saved to file 'file:/tmp/checkpoint-1444396229000', took 4010 bytes and 7 ms
15/10/09 08:10:29 INFO DStreamGraph: Clearing checkpoint data for time 1444396229000 ms
15/10/09 08:10:29 INFO DStreamGraph: Cleared checkpoint data for time 1444396229000 ms
15/10/09 08:10:29 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:29 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396228000: 
15/10/09 08:10:29 INFO InputInfoTracker: remove old batch metadata: 1444396227000 ms
15/10/09 08:10:29 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396228000
15/10/09 08:10:30 INFO JobScheduler: Added jobs for time 1444396230000 ms
15/10/09 08:10:30 INFO JobGenerator: Checkpointing graph for time 1444396230000 ms
15/10/09 08:10:30 INFO DStreamGraph: Updating checkpoint data for time 1444396230000 ms
15/10/09 08:10:30 INFO JobScheduler: Starting job streaming job 1444396230000 ms.0 from job set of time 1444396230000 ms
15/10/09 08:10:30 INFO DStreamGraph: Updated checkpoint data for time 1444396230000 ms
15/10/09 08:10:30 INFO CheckpointWriter: Saving checkpoint for time 1444396230000 ms to file 'file:/tmp/checkpoint-1444396230000'
15/10/09 08:10:30 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:30 INFO DAGScheduler: Got job 27 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:30 INFO DAGScheduler: Final stage: ResultStage 27(foreachRDD at Events.scala:26)
15/10/09 08:10:30 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:30 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:30 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[55] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:30 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=23028, maxMem=555755765
15/10/09 08:10:30 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:30 INFO MemoryStore: ensureFreeSpace(1685) called with curMem=25948, maxMem=555755765
15/10/09 08:10:30 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 1685.0 B, free 530.0 MB)
15/10/09 08:10:30 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:57166 (size: 1685.0 B, free: 530.0 MB)
15/10/09 08:10:30 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[55] at map at Events.scala:23)
15/10/09 08:10:30 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
15/10/09 08:10:30 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27, localhost, ANY, 2085 bytes)
15/10/09 08:10:30 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
15/10/09 08:10:30 INFO KafkaRDD: Beginning offset 532 is the same as ending offset skipping events 0
15/10/09 08:10:30 INFO Executor: Finished task 0.0 in stage 27.0 (TID 27). 915 bytes result sent to driver
15/10/09 08:10:30 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 27) in 2 ms on localhost (1/1)
15/10/09 08:10:30 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
15/10/09 08:10:30 INFO DAGScheduler: ResultStage 27 (foreachRDD at Events.scala:26) finished in 0.002 s
15/10/09 08:10:30 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396225000.bk
15/10/09 08:10:30 INFO DAGScheduler: Job 27 finished: foreachRDD at Events.scala:26, took 0.007296 s
15/10/09 08:10:30 INFO CheckpointWriter: Checkpoint for time 1444396230000 ms saved to file 'file:/tmp/checkpoint-1444396230000', took 4051 bytes and 10 ms
15/10/09 08:10:30 INFO JobScheduler: Finished job streaming job 1444396230000 ms.0 from job set of time 1444396230000 ms
15/10/09 08:10:30 INFO JobScheduler: Total delay: 0.019 s for time 1444396230000 ms (execution: 0.010 s)
15/10/09 08:10:30 INFO MapPartitionsRDD: Removing RDD 53 from persistence list
15/10/09 08:10:30 INFO BlockManager: Removing RDD 53
15/10/09 08:10:30 INFO KafkaRDD: Removing RDD 52 from persistence list
15/10/09 08:10:30 INFO JobGenerator: Checkpointing graph for time 1444396230000 ms
15/10/09 08:10:30 INFO BlockManager: Removing RDD 52
15/10/09 08:10:30 INFO DStreamGraph: Updating checkpoint data for time 1444396230000 ms
15/10/09 08:10:30 INFO DStreamGraph: Updated checkpoint data for time 1444396230000 ms
15/10/09 08:10:30 INFO CheckpointWriter: Saving checkpoint for time 1444396230000 ms to file 'file:/tmp/checkpoint-1444396230000'
15/10/09 08:10:30 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396225000
15/10/09 08:10:30 INFO CheckpointWriter: Checkpoint for time 1444396230000 ms saved to file 'file:/tmp/checkpoint-1444396230000', took 4012 bytes and 6 ms
15/10/09 08:10:30 INFO DStreamGraph: Clearing checkpoint data for time 1444396230000 ms
15/10/09 08:10:30 INFO DStreamGraph: Cleared checkpoint data for time 1444396230000 ms
15/10/09 08:10:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:30 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396229000: 
15/10/09 08:10:30 INFO InputInfoTracker: remove old batch metadata: 1444396228000 ms
15/10/09 08:10:30 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396229000
15/10/09 08:10:31 INFO JobScheduler: Added jobs for time 1444396231000 ms
15/10/09 08:10:31 INFO JobGenerator: Checkpointing graph for time 1444396231000 ms
15/10/09 08:10:31 INFO JobScheduler: Starting job streaming job 1444396231000 ms.0 from job set of time 1444396231000 ms
15/10/09 08:10:31 INFO DStreamGraph: Updating checkpoint data for time 1444396231000 ms
15/10/09 08:10:31 INFO DStreamGraph: Updated checkpoint data for time 1444396231000 ms
15/10/09 08:10:31 INFO CheckpointWriter: Saving checkpoint for time 1444396231000 ms to file 'file:/tmp/checkpoint-1444396231000'
15/10/09 08:10:31 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:31 INFO DAGScheduler: Got job 28 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:31 INFO DAGScheduler: Final stage: ResultStage 28(foreachRDD at Events.scala:26)
15/10/09 08:10:31 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:31 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:31 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[57] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:31 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=27633, maxMem=555755765
15/10/09 08:10:31 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:31 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=30553, maxMem=555755765
15/10/09 08:10:31 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:31 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:31 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[57] at map at Events.scala:23)
15/10/09 08:10:31 INFO TaskSchedulerImpl: Adding task set 28.0 with 1 tasks
15/10/09 08:10:31 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 28, localhost, ANY, 2085 bytes)
15/10/09 08:10:31 INFO Executor: Running task 0.0 in stage 28.0 (TID 28)
15/10/09 08:10:31 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396226000.bk
15/10/09 08:10:31 INFO CheckpointWriter: Checkpoint for time 1444396231000 ms saved to file 'file:/tmp/checkpoint-1444396231000', took 4052 bytes and 7 ms
15/10/09 08:10:31 INFO KafkaRDD: Computing topic events, partition 0 offsets 532 -> 533
15/10/09 08:10:31 INFO VerifiableProperties: Verifying properties
15/10/09 08:10:31 INFO VerifiableProperties: Property auto.offset.reset is overridden to smallest
15/10/09 08:10:31 INFO VerifiableProperties: Property group.id is overridden to 
15/10/09 08:10:31 INFO VerifiableProperties: Property zookeeper.connect is overridden to 
15/10/09 08:10:31 INFO Executor: Finished task 0.0 in stage 28.0 (TID 28). 915 bytes result sent to driver
15/10/09 08:10:31 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 28) in 26 ms on localhost (1/1)
15/10/09 08:10:31 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
15/10/09 08:10:31 INFO DAGScheduler: ResultStage 28 (foreachRDD at Events.scala:26) finished in 0.026 s
15/10/09 08:10:31 INFO DAGScheduler: Job 28 finished: foreachRDD at Events.scala:26, took 0.029865 s
15/10/09 08:10:31 INFO JobScheduler: Finished job streaming job 1444396231000 ms.0 from job set of time 1444396231000 ms
15/10/09 08:10:31 INFO JobScheduler: Total delay: 0.040 s for time 1444396231000 ms (execution: 0.032 s)
15/10/09 08:10:31 INFO MapPartitionsRDD: Removing RDD 55 from persistence list
15/10/09 08:10:31 INFO KafkaRDD: Removing RDD 54 from persistence list
15/10/09 08:10:31 INFO BlockManager: Removing RDD 55
15/10/09 08:10:31 INFO JobGenerator: Checkpointing graph for time 1444396231000 ms
15/10/09 08:10:31 INFO DStreamGraph: Updating checkpoint data for time 1444396231000 ms
15/10/09 08:10:31 INFO BlockManager: Removing RDD 54
15/10/09 08:10:31 INFO DStreamGraph: Updated checkpoint data for time 1444396231000 ms
15/10/09 08:10:31 INFO CheckpointWriter: Saving checkpoint for time 1444396231000 ms to file 'file:/tmp/checkpoint-1444396231000'
15/10/09 08:10:31 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396226000
15/10/09 08:10:31 INFO CheckpointWriter: Checkpoint for time 1444396231000 ms saved to file 'file:/tmp/checkpoint-1444396231000', took 4012 bytes and 7 ms
15/10/09 08:10:31 INFO DStreamGraph: Clearing checkpoint data for time 1444396231000 ms
15/10/09 08:10:31 INFO DStreamGraph: Cleared checkpoint data for time 1444396231000 ms
15/10/09 08:10:31 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:31 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396230000: 
15/10/09 08:10:31 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396230000
15/10/09 08:10:31 INFO InputInfoTracker: remove old batch metadata: 1444396229000 ms
15/10/09 08:10:32 INFO JobScheduler: Added jobs for time 1444396232000 ms
15/10/09 08:10:32 INFO JobGenerator: Checkpointing graph for time 1444396232000 ms
15/10/09 08:10:32 INFO DStreamGraph: Updating checkpoint data for time 1444396232000 ms
15/10/09 08:10:32 INFO JobScheduler: Starting job streaming job 1444396232000 ms.0 from job set of time 1444396232000 ms
15/10/09 08:10:32 INFO DStreamGraph: Updated checkpoint data for time 1444396232000 ms
15/10/09 08:10:32 INFO CheckpointWriter: Saving checkpoint for time 1444396232000 ms to file 'file:/tmp/checkpoint-1444396232000'
15/10/09 08:10:32 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:32 INFO DAGScheduler: Got job 29 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:32 INFO DAGScheduler: Final stage: ResultStage 29(foreachRDD at Events.scala:26)
15/10/09 08:10:32 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:32 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:32 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[59] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:32 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=32239, maxMem=555755765
15/10/09 08:10:32 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:32 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=35159, maxMem=555755765
15/10/09 08:10:32 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:32 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:32 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[59] at map at Events.scala:23)
15/10/09 08:10:32 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
15/10/09 08:10:32 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 29, localhost, ANY, 2085 bytes)
15/10/09 08:10:32 INFO Executor: Running task 0.0 in stage 29.0 (TID 29)
15/10/09 08:10:32 INFO KafkaRDD: Beginning offset 533 is the same as ending offset skipping events 0
15/10/09 08:10:32 ERROR Executor: Exception in task 0.0 in stage 29.0 (TID 29)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:32 WARN TaskSetManager: Lost task 0.0 in stage 29.0 (TID 29, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:32 ERROR TaskSetManager: Task 0 in stage 29.0 failed 1 times; aborting job
15/10/09 08:10:32 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
15/10/09 08:10:32 INFO TaskSchedulerImpl: Cancelling stage 29
15/10/09 08:10:32 INFO DAGScheduler: ResultStage 29 (foreachRDD at Events.scala:26) failed in 0.003 s
15/10/09 08:10:32 INFO DAGScheduler: Job 29 failed: foreachRDD at Events.scala:26, took 0.006465 s
15/10/09 08:10:32 INFO JobScheduler: Finished job streaming job 1444396232000 ms.0 from job set of time 1444396232000 ms
15/10/09 08:10:32 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396227000.bk
15/10/09 08:10:32 INFO JobScheduler: Total delay: 0.016 s for time 1444396232000 ms (execution: 0.009 s)
15/10/09 08:10:32 INFO MapPartitionsRDD: Removing RDD 57 from persistence list
15/10/09 08:10:32 ERROR JobScheduler: Error running job streaming job 1444396232000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 29, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:32 INFO CheckpointWriter: Checkpoint for time 1444396232000 ms saved to file 'file:/tmp/checkpoint-1444396232000', took 4051 bytes and 9 ms
15/10/09 08:10:32 INFO BlockManager: Removing RDD 57
15/10/09 08:10:32 INFO KafkaRDD: Removing RDD 56 from persistence list
15/10/09 08:10:32 INFO JobGenerator: Checkpointing graph for time 1444396232000 ms
15/10/09 08:10:32 INFO DStreamGraph: Updating checkpoint data for time 1444396232000 ms
15/10/09 08:10:32 INFO BlockManager: Removing RDD 56
15/10/09 08:10:32 INFO DStreamGraph: Updated checkpoint data for time 1444396232000 ms
15/10/09 08:10:32 INFO CheckpointWriter: Saving checkpoint for time 1444396232000 ms to file 'file:/tmp/checkpoint-1444396232000'
15/10/09 08:10:32 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396227000
15/10/09 08:10:32 INFO CheckpointWriter: Checkpoint for time 1444396232000 ms saved to file 'file:/tmp/checkpoint-1444396232000', took 4010 bytes and 7 ms
15/10/09 08:10:32 INFO DStreamGraph: Clearing checkpoint data for time 1444396232000 ms
15/10/09 08:10:32 INFO DStreamGraph: Cleared checkpoint data for time 1444396232000 ms
15/10/09 08:10:32 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:32 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396231000: 
15/10/09 08:10:32 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396231000
15/10/09 08:10:32 INFO InputInfoTracker: remove old batch metadata: 1444396230000 ms
15/10/09 08:10:33 INFO JobScheduler: Added jobs for time 1444396233000 ms
15/10/09 08:10:33 INFO JobGenerator: Checkpointing graph for time 1444396233000 ms
15/10/09 08:10:33 INFO DStreamGraph: Updating checkpoint data for time 1444396233000 ms
15/10/09 08:10:33 INFO JobScheduler: Starting job streaming job 1444396233000 ms.0 from job set of time 1444396233000 ms
15/10/09 08:10:33 INFO DStreamGraph: Updated checkpoint data for time 1444396233000 ms
15/10/09 08:10:33 INFO CheckpointWriter: Saving checkpoint for time 1444396233000 ms to file 'file:/tmp/checkpoint-1444396233000'
15/10/09 08:10:33 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:33 INFO DAGScheduler: Got job 30 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:33 INFO DAGScheduler: Final stage: ResultStage 30(foreachRDD at Events.scala:26)
15/10/09 08:10:33 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:33 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:33 INFO DAGScheduler: Submitting ResultStage 30 (MapPartitionsRDD[61] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:33 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=36845, maxMem=555755765
15/10/09 08:10:33 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:33 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=39765, maxMem=555755765
15/10/09 08:10:33 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:33 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:33 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 30 (MapPartitionsRDD[61] at map at Events.scala:23)
15/10/09 08:10:33 INFO TaskSchedulerImpl: Adding task set 30.0 with 1 tasks
15/10/09 08:10:33 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 30, localhost, ANY, 2085 bytes)
15/10/09 08:10:33 INFO Executor: Running task 0.0 in stage 30.0 (TID 30)
15/10/09 08:10:33 INFO KafkaRDD: Beginning offset 533 is the same as ending offset skipping events 0
15/10/09 08:10:33 ERROR Executor: Exception in task 0.0 in stage 30.0 (TID 30)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:33 WARN TaskSetManager: Lost task 0.0 in stage 30.0 (TID 30, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:33 ERROR TaskSetManager: Task 0 in stage 30.0 failed 1 times; aborting job
15/10/09 08:10:33 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
15/10/09 08:10:33 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396228000.bk
15/10/09 08:10:33 INFO TaskSchedulerImpl: Cancelling stage 30
15/10/09 08:10:33 INFO DAGScheduler: ResultStage 30 (foreachRDD at Events.scala:26) failed in 0.003 s
15/10/09 08:10:33 INFO DAGScheduler: Job 30 failed: foreachRDD at Events.scala:26, took 0.006838 s
15/10/09 08:10:33 INFO CheckpointWriter: Checkpoint for time 1444396233000 ms saved to file 'file:/tmp/checkpoint-1444396233000', took 4049 bytes and 8 ms
15/10/09 08:10:33 INFO JobScheduler: Finished job streaming job 1444396233000 ms.0 from job set of time 1444396233000 ms
15/10/09 08:10:33 INFO JobScheduler: Total delay: 0.020 s for time 1444396233000 ms (execution: 0.010 s)
15/10/09 08:10:33 INFO MapPartitionsRDD: Removing RDD 59 from persistence list
15/10/09 08:10:33 ERROR JobScheduler: Error running job streaming job 1444396233000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 30, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:33 INFO BlockManager: Removing RDD 59
15/10/09 08:10:33 INFO KafkaRDD: Removing RDD 58 from persistence list
15/10/09 08:10:33 INFO JobGenerator: Checkpointing graph for time 1444396233000 ms
15/10/09 08:10:33 INFO DStreamGraph: Updating checkpoint data for time 1444396233000 ms
15/10/09 08:10:33 INFO BlockManager: Removing RDD 58
15/10/09 08:10:33 INFO DStreamGraph: Updated checkpoint data for time 1444396233000 ms
15/10/09 08:10:33 INFO CheckpointWriter: Saving checkpoint for time 1444396233000 ms to file 'file:/tmp/checkpoint-1444396233000'
15/10/09 08:10:33 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396228000
15/10/09 08:10:33 INFO CheckpointWriter: Checkpoint for time 1444396233000 ms saved to file 'file:/tmp/checkpoint-1444396233000', took 4010 bytes and 7 ms
15/10/09 08:10:33 INFO DStreamGraph: Clearing checkpoint data for time 1444396233000 ms
15/10/09 08:10:33 INFO DStreamGraph: Cleared checkpoint data for time 1444396233000 ms
15/10/09 08:10:33 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:33 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396232000: 
15/10/09 08:10:33 INFO InputInfoTracker: remove old batch metadata: 1444396231000 ms
15/10/09 08:10:33 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396232000
15/10/09 08:10:34 INFO JobScheduler: Added jobs for time 1444396234000 ms
15/10/09 08:10:34 INFO JobGenerator: Checkpointing graph for time 1444396234000 ms
15/10/09 08:10:34 INFO DStreamGraph: Updating checkpoint data for time 1444396234000 ms
15/10/09 08:10:34 INFO JobScheduler: Starting job streaming job 1444396234000 ms.0 from job set of time 1444396234000 ms
15/10/09 08:10:34 INFO DStreamGraph: Updated checkpoint data for time 1444396234000 ms
15/10/09 08:10:34 INFO CheckpointWriter: Saving checkpoint for time 1444396234000 ms to file 'file:/tmp/checkpoint-1444396234000'
15/10/09 08:10:34 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:34 INFO DAGScheduler: Got job 31 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:34 INFO DAGScheduler: Final stage: ResultStage 31(foreachRDD at Events.scala:26)
15/10/09 08:10:34 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:34 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:34 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[63] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:34 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=41451, maxMem=555755765
15/10/09 08:10:34 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:34 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=44371, maxMem=555755765
15/10/09 08:10:34 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:34 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:34 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[63] at map at Events.scala:23)
15/10/09 08:10:34 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
15/10/09 08:10:34 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 31, localhost, ANY, 2085 bytes)
15/10/09 08:10:34 INFO Executor: Running task 0.0 in stage 31.0 (TID 31)
15/10/09 08:10:34 INFO KafkaRDD: Beginning offset 533 is the same as ending offset skipping events 0
15/10/09 08:10:34 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396229000.bk
15/10/09 08:10:34 ERROR Executor: Exception in task 0.0 in stage 31.0 (TID 31)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:34 INFO CheckpointWriter: Checkpoint for time 1444396234000 ms saved to file 'file:/tmp/checkpoint-1444396234000', took 4045 bytes and 8 ms
15/10/09 08:10:34 WARN TaskSetManager: Lost task 0.0 in stage 31.0 (TID 31, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:34 ERROR TaskSetManager: Task 0 in stage 31.0 failed 1 times; aborting job
15/10/09 08:10:34 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
15/10/09 08:10:34 INFO TaskSchedulerImpl: Cancelling stage 31
15/10/09 08:10:34 INFO DAGScheduler: ResultStage 31 (foreachRDD at Events.scala:26) failed in 0.003 s
15/10/09 08:10:34 INFO DAGScheduler: Job 31 failed: foreachRDD at Events.scala:26, took 0.008137 s
15/10/09 08:10:34 INFO JobScheduler: Finished job streaming job 1444396234000 ms.0 from job set of time 1444396234000 ms
15/10/09 08:10:34 INFO JobScheduler: Total delay: 0.021 s for time 1444396234000 ms (execution: 0.011 s)
15/10/09 08:10:34 INFO MapPartitionsRDD: Removing RDD 61 from persistence list
15/10/09 08:10:34 ERROR JobScheduler: Error running job streaming job 1444396234000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 31.0 failed 1 times, most recent failure: Lost task 0.0 in stage 31.0 (TID 31, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:34 INFO BlockManager: Removing RDD 61
15/10/09 08:10:34 INFO KafkaRDD: Removing RDD 60 from persistence list
15/10/09 08:10:34 INFO JobGenerator: Checkpointing graph for time 1444396234000 ms
15/10/09 08:10:34 INFO BlockManager: Removing RDD 60
15/10/09 08:10:34 INFO DStreamGraph: Updating checkpoint data for time 1444396234000 ms
15/10/09 08:10:34 INFO DStreamGraph: Updated checkpoint data for time 1444396234000 ms
15/10/09 08:10:34 INFO CheckpointWriter: Saving checkpoint for time 1444396234000 ms to file 'file:/tmp/checkpoint-1444396234000'
15/10/09 08:10:34 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396229000
15/10/09 08:10:34 INFO CheckpointWriter: Checkpoint for time 1444396234000 ms saved to file 'file:/tmp/checkpoint-1444396234000', took 4010 bytes and 6 ms
15/10/09 08:10:34 INFO DStreamGraph: Clearing checkpoint data for time 1444396234000 ms
15/10/09 08:10:34 INFO DStreamGraph: Cleared checkpoint data for time 1444396234000 ms
15/10/09 08:10:34 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:34 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396233000: 
15/10/09 08:10:34 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396233000
15/10/09 08:10:34 INFO InputInfoTracker: remove old batch metadata: 1444396232000 ms
15/10/09 08:10:35 INFO JobScheduler: Added jobs for time 1444396235000 ms
15/10/09 08:10:35 INFO JobGenerator: Checkpointing graph for time 1444396235000 ms
15/10/09 08:10:35 INFO DStreamGraph: Updating checkpoint data for time 1444396235000 ms
15/10/09 08:10:35 INFO JobScheduler: Starting job streaming job 1444396235000 ms.0 from job set of time 1444396235000 ms
15/10/09 08:10:35 INFO DStreamGraph: Updated checkpoint data for time 1444396235000 ms
15/10/09 08:10:35 INFO CheckpointWriter: Saving checkpoint for time 1444396235000 ms to file 'file:/tmp/checkpoint-1444396235000'
15/10/09 08:10:35 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:35 INFO DAGScheduler: Got job 32 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:35 INFO DAGScheduler: Final stage: ResultStage 32(foreachRDD at Events.scala:26)
15/10/09 08:10:35 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:35 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:35 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[65] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:35 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=46057, maxMem=555755765
15/10/09 08:10:35 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:35 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=48977, maxMem=555755765
15/10/09 08:10:35 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:35 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:35 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[65] at map at Events.scala:23)
15/10/09 08:10:35 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
15/10/09 08:10:35 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 32, localhost, ANY, 2085 bytes)
15/10/09 08:10:35 INFO Executor: Running task 0.0 in stage 32.0 (TID 32)
15/10/09 08:10:35 INFO KafkaRDD: Beginning offset 533 is the same as ending offset skipping events 0
15/10/09 08:10:35 ERROR Executor: Exception in task 0.0 in stage 32.0 (TID 32)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:35 WARN TaskSetManager: Lost task 0.0 in stage 32.0 (TID 32, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:35 ERROR TaskSetManager: Task 0 in stage 32.0 failed 1 times; aborting job
15/10/09 08:10:35 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
15/10/09 08:10:35 INFO TaskSchedulerImpl: Cancelling stage 32
15/10/09 08:10:35 INFO DAGScheduler: ResultStage 32 (foreachRDD at Events.scala:26) failed in 0.003 s
15/10/09 08:10:35 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396230000.bk
15/10/09 08:10:35 INFO DAGScheduler: Job 32 failed: foreachRDD at Events.scala:26, took 0.006714 s
15/10/09 08:10:35 INFO JobScheduler: Finished job streaming job 1444396235000 ms.0 from job set of time 1444396235000 ms
15/10/09 08:10:35 INFO CheckpointWriter: Checkpoint for time 1444396235000 ms saved to file 'file:/tmp/checkpoint-1444396235000', took 4045 bytes and 8 ms
15/10/09 08:10:35 INFO JobScheduler: Total delay: 0.019 s for time 1444396235000 ms (execution: 0.009 s)
15/10/09 08:10:35 INFO MapPartitionsRDD: Removing RDD 63 from persistence list
15/10/09 08:10:35 ERROR JobScheduler: Error running job streaming job 1444396235000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 32.0 failed 1 times, most recent failure: Lost task 0.0 in stage 32.0 (TID 32, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:35 INFO KafkaRDD: Removing RDD 62 from persistence list
15/10/09 08:10:35 INFO BlockManager: Removing RDD 63
15/10/09 08:10:35 INFO JobGenerator: Checkpointing graph for time 1444396235000 ms
15/10/09 08:10:35 INFO DStreamGraph: Updating checkpoint data for time 1444396235000 ms
15/10/09 08:10:35 INFO BlockManager: Removing RDD 62
15/10/09 08:10:35 INFO DStreamGraph: Updated checkpoint data for time 1444396235000 ms
15/10/09 08:10:35 INFO CheckpointWriter: Saving checkpoint for time 1444396235000 ms to file 'file:/tmp/checkpoint-1444396235000'
15/10/09 08:10:35 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396230000
15/10/09 08:10:35 INFO CheckpointWriter: Checkpoint for time 1444396235000 ms saved to file 'file:/tmp/checkpoint-1444396235000', took 4010 bytes and 6 ms
15/10/09 08:10:35 INFO DStreamGraph: Clearing checkpoint data for time 1444396235000 ms
15/10/09 08:10:35 INFO DStreamGraph: Cleared checkpoint data for time 1444396235000 ms
15/10/09 08:10:35 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:35 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396234000: 
15/10/09 08:10:35 INFO InputInfoTracker: remove old batch metadata: 1444396233000 ms
15/10/09 08:10:35 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396234000
15/10/09 08:10:36 INFO JobScheduler: Added jobs for time 1444396236000 ms
15/10/09 08:10:36 INFO JobGenerator: Checkpointing graph for time 1444396236000 ms
15/10/09 08:10:36 INFO DStreamGraph: Updating checkpoint data for time 1444396236000 ms
15/10/09 08:10:36 INFO JobScheduler: Starting job streaming job 1444396236000 ms.0 from job set of time 1444396236000 ms
15/10/09 08:10:36 INFO DStreamGraph: Updated checkpoint data for time 1444396236000 ms
15/10/09 08:10:36 INFO CheckpointWriter: Saving checkpoint for time 1444396236000 ms to file 'file:/tmp/checkpoint-1444396236000'
15/10/09 08:10:36 INFO SparkContext: Starting job: foreachRDD at Events.scala:26
15/10/09 08:10:36 INFO DAGScheduler: Got job 33 (foreachRDD at Events.scala:26) with 1 output partitions
15/10/09 08:10:36 INFO DAGScheduler: Final stage: ResultStage 33(foreachRDD at Events.scala:26)
15/10/09 08:10:36 INFO DAGScheduler: Parents of final stage: List()
15/10/09 08:10:36 INFO DAGScheduler: Missing parents: List()
15/10/09 08:10:36 INFO DAGScheduler: Submitting ResultStage 33 (MapPartitionsRDD[67] at map at Events.scala:23), which has no missing parents
15/10/09 08:10:36 INFO MemoryStore: ensureFreeSpace(2920) called with curMem=50663, maxMem=555755765
15/10/09 08:10:36 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 2.9 KB, free 530.0 MB)
15/10/09 08:10:36 INFO MemoryStore: ensureFreeSpace(1686) called with curMem=53583, maxMem=555755765
15/10/09 08:10:36 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 1686.0 B, free 530.0 MB)
15/10/09 08:10:36 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:57166 (size: 1686.0 B, free: 530.0 MB)
15/10/09 08:10:36 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:861
15/10/09 08:10:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (MapPartitionsRDD[67] at map at Events.scala:23)
15/10/09 08:10:36 INFO TaskSchedulerImpl: Adding task set 33.0 with 1 tasks
15/10/09 08:10:36 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 33, localhost, ANY, 2085 bytes)
15/10/09 08:10:36 INFO Executor: Running task 0.0 in stage 33.0 (TID 33)
15/10/09 08:10:36 INFO KafkaRDD: Beginning offset 533 is the same as ending offset skipping events 0
15/10/09 08:10:36 ERROR Executor: Exception in task 0.0 in stage 33.0 (TID 33)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 08:10:36 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396231000.bk
15/10/09 08:10:36 WARN TaskSetManager: Lost task 0.0 in stage 33.0 (TID 33, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 08:10:36 ERROR TaskSetManager: Task 0 in stage 33.0 failed 1 times; aborting job
15/10/09 08:10:36 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool 
15/10/09 08:10:36 INFO CheckpointWriter: Checkpoint for time 1444396236000 ms saved to file 'file:/tmp/checkpoint-1444396236000', took 4049 bytes and 8 ms
15/10/09 08:10:36 INFO TaskSchedulerImpl: Cancelling stage 33
15/10/09 08:10:36 INFO DAGScheduler: ResultStage 33 (foreachRDD at Events.scala:26) failed in 0.002 s
15/10/09 08:10:36 INFO DAGScheduler: Job 33 failed: foreachRDD at Events.scala:26, took 0.007206 s
15/10/09 08:10:36 INFO JobScheduler: Finished job streaming job 1444396236000 ms.0 from job set of time 1444396236000 ms
15/10/09 08:10:36 INFO JobScheduler: Total delay: 0.020 s for time 1444396236000 ms (execution: 0.010 s)
15/10/09 08:10:36 INFO MapPartitionsRDD: Removing RDD 65 from persistence list
15/10/09 08:10:36 ERROR JobScheduler: Error running job streaming job 1444396236000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 33.0 failed 1 times, most recent failure: Lost task 0.0 in stage 33.0 (TID 33, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:28)
	at Events$$anonfun$createContext$1.apply(Events.scala:26)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:31)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:28)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 08:10:36 INFO KafkaRDD: Removing RDD 64 from persistence list
15/10/09 08:10:36 INFO BlockManager: Removing RDD 65
15/10/09 08:10:36 INFO JobGenerator: Checkpointing graph for time 1444396236000 ms
15/10/09 08:10:36 INFO DStreamGraph: Updating checkpoint data for time 1444396236000 ms
15/10/09 08:10:36 INFO BlockManager: Removing RDD 64
15/10/09 08:10:36 INFO DStreamGraph: Updated checkpoint data for time 1444396236000 ms
15/10/09 08:10:36 INFO CheckpointWriter: Saving checkpoint for time 1444396236000 ms to file 'file:/tmp/checkpoint-1444396236000'
15/10/09 08:10:36 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444396231000
15/10/09 08:10:36 INFO CheckpointWriter: Checkpoint for time 1444396236000 ms saved to file 'file:/tmp/checkpoint-1444396236000', took 4010 bytes and 7 ms
15/10/09 08:10:36 INFO DStreamGraph: Clearing checkpoint data for time 1444396236000 ms
15/10/09 08:10:36 INFO DStreamGraph: Cleared checkpoint data for time 1444396236000 ms
15/10/09 08:10:36 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 08:10:36 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444396235000: 
15/10/09 08:10:36 INFO InputInfoTracker: remove old batch metadata: 1444396234000 ms
15/10/09 08:10:36 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444396235000
15/10/09 08:10:36 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
15/10/09 08:10:36 INFO JobGenerator: Stopping JobGenerator immediately
15/10/09 08:10:36 INFO RecurringTimer: Stopped timer for JobGenerator after time 1444396236000
15/10/09 08:10:37 INFO CheckpointWriter: CheckpointWriter executor terminated ? true, waited for 0 ms.
15/10/09 08:10:37 INFO JobGenerator: Stopped JobGenerator
15/10/09 08:10:37 INFO JobScheduler: Stopped JobScheduler
15/10/09 08:10:37 INFO StreamingContext: StreamingContext stopped successfully
15/10/09 08:10:37 INFO SparkContext: Invoking stop() from shutdown hook
15/10/09 08:10:37 INFO SparkUI: Stopped Spark web UI at http://10.101.21.127:4040
15/10/09 08:10:37 INFO DAGScheduler: Stopping DAGScheduler
15/10/09 08:10:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/10/09 08:10:37 INFO MemoryStore: MemoryStore cleared
15/10/09 08:10:37 INFO BlockManager: BlockManager stopped
15/10/09 08:10:37 INFO BlockManagerMaster: BlockManagerMaster stopped
15/10/09 08:10:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/10/09 08:10:37 INFO SparkContext: Successfully stopped SparkContext
15/10/09 08:10:37 INFO ShutdownHookManager: Shutdown hook called
15/10/09 08:10:37 INFO ShutdownHookManager: Deleting directory /private/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/spark-2703f33f-6f61-4da2-b63e-f391886287a0
15/10/09 08:10:37 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
