log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
15/10/09 06:43:07 INFO SparkContext: Running Spark version 1.5.1
15/10/09 06:43:07 INFO SecurityManager: Changing view acls to: andrewclarkson
15/10/09 06:43:07 INFO SecurityManager: Changing modify acls to: andrewclarkson
15/10/09 06:43:07 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(andrewclarkson); users with modify permissions: Set(andrewclarkson)
15/10/09 06:43:07 INFO Slf4jLogger: Slf4jLogger started
15/10/09 06:43:07 INFO Remoting: Starting remoting
15/10/09 06:43:07 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriver@10.101.21.127:55617]
15/10/09 06:43:07 INFO Utils: Successfully started service 'sparkDriver' on port 55617.
15/10/09 06:43:07 INFO SparkEnv: Registering MapOutputTracker
15/10/09 06:43:07 INFO SparkEnv: Registering BlockManagerMaster
15/10/09 06:43:07 INFO DiskBlockManager: Created local directory at /private/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/blockmgr-8bf06f10-e23c-479b-b60b-e8a7fefea987
15/10/09 06:43:07 INFO MemoryStore: MemoryStore started with capacity 530.0 MB
15/10/09 06:43:07 INFO HttpFileServer: HTTP File server directory is /private/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/spark-e8c0e34a-82c4-4275-a5e7-1a397218521b/httpd-2586d81f-f1d8-480c-819a-39e5c920f0f4
15/10/09 06:43:07 INFO HttpServer: Starting HTTP Server
15/10/09 06:43:08 INFO Utils: Successfully started service 'HTTP file server' on port 55618.
15/10/09 06:43:08 INFO SparkEnv: Registering OutputCommitCoordinator
15/10/09 06:43:08 INFO Utils: Successfully started service 'SparkUI' on port 4040.
15/10/09 06:43:08 INFO SparkUI: Started SparkUI at http://10.101.21.127:4040
15/10/09 06:43:08 INFO SparkContext: Added JAR file:/Users/andrewclarkson/code/resilient-kafka-streaming-in-spark/target/scala-2.10/events-assembly-1.0.jar at http://10.101.21.127:55618/jars/events-assembly-1.0.jar with timestamp 1444390988274
15/10/09 06:43:08 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set.
15/10/09 06:43:08 INFO Executor: Starting executor ID driver on host localhost
15/10/09 06:43:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55619.
15/10/09 06:43:08 INFO NettyBlockTransferService: Server created on 55619
15/10/09 06:43:08 INFO BlockManagerMaster: Trying to register BlockManager
15/10/09 06:43:08 INFO BlockManagerMasterEndpoint: Registering block manager localhost:55619 with 530.0 MB RAM, BlockManagerId(driver, localhost, 55619)
15/10/09 06:43:08 INFO BlockManagerMaster: Registered BlockManager
15/10/09 06:43:08 INFO ReceiverTracker: Starting 1 receivers
15/10/09 06:43:08 INFO ReceiverTracker: ReceiverTracker started
15/10/09 06:43:08 INFO ForEachDStream: metadataCleanupDelay = -1
15/10/09 06:43:08 INFO MappedDStream: metadataCleanupDelay = -1
15/10/09 06:43:08 INFO KafkaInputDStream: metadataCleanupDelay = -1
15/10/09 06:43:08 INFO KafkaInputDStream: Slide time = 1000 ms
15/10/09 06:43:08 INFO KafkaInputDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/10/09 06:43:08 INFO KafkaInputDStream: Checkpoint interval = null
15/10/09 06:43:08 INFO KafkaInputDStream: Remember duration = 1000 ms
15/10/09 06:43:08 INFO KafkaInputDStream: Initialized and validated org.apache.spark.streaming.kafka.KafkaInputDStream@27e5b378
15/10/09 06:43:08 INFO MappedDStream: Slide time = 1000 ms
15/10/09 06:43:08 INFO MappedDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/10/09 06:43:08 INFO MappedDStream: Checkpoint interval = null
15/10/09 06:43:08 INFO MappedDStream: Remember duration = 1000 ms
15/10/09 06:43:08 INFO MappedDStream: Initialized and validated org.apache.spark.streaming.dstream.MappedDStream@5e048149
15/10/09 06:43:08 INFO ForEachDStream: Slide time = 1000 ms
15/10/09 06:43:08 INFO ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)
15/10/09 06:43:08 INFO ForEachDStream: Checkpoint interval = null
15/10/09 06:43:08 INFO ForEachDStream: Remember duration = 1000 ms
15/10/09 06:43:08 INFO ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@2f61d591
15/10/09 06:43:08 INFO RecurringTimer: Started timer for JobGenerator at time 1444390989000
15/10/09 06:43:08 INFO JobGenerator: Started JobGenerator at 1444390989000 ms
15/10/09 06:43:08 INFO JobScheduler: Started JobScheduler
15/10/09 06:43:08 INFO StreamingContext: StreamingContext started
15/10/09 06:43:08 INFO ReceiverTracker: Receiver 0 started
15/10/09 06:43:08 INFO DAGScheduler: Got job 0 (start at Events.scala:35) with 1 output partitions
15/10/09 06:43:08 INFO DAGScheduler: Final stage: ResultStage 0(start at Events.scala:35)
15/10/09 06:43:08 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:08 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:08 INFO DAGScheduler: Submitting ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:554), which has no missing parents
15/10/09 06:43:09 INFO MemoryStore: ensureFreeSpace(62280) called with curMem=0, maxMem=555755765
15/10/09 06:43:09 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 60.8 KB, free 530.0 MB)
15/10/09 06:43:09 INFO MemoryStore: ensureFreeSpace(20695) called with curMem=62280, maxMem=555755765
15/10/09 06:43:09 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 20.2 KB, free 529.9 MB)
15/10/09 06:43:09 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:55619 (size: 20.2 KB, free: 530.0 MB)
15/10/09 06:43:09 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:09 INFO JobScheduler: Added jobs for time 1444390989000 ms
15/10/09 06:43:09 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (Receiver 0 ParallelCollectionRDD[0] at makeRDD at ReceiverTracker.scala:554)
15/10/09 06:43:09 INFO JobGenerator: Checkpointing graph for time 1444390989000 ms
15/10/09 06:43:09 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
15/10/09 06:43:09 INFO JobScheduler: Starting job streaming job 1444390989000 ms.0 from job set of time 1444390989000 ms
15/10/09 06:43:09 INFO DStreamGraph: Updating checkpoint data for time 1444390989000 ms
15/10/09 06:43:09 INFO DStreamGraph: Updated checkpoint data for time 1444390989000 ms
15/10/09 06:43:09 INFO CheckpointWriter: Saving checkpoint for time 1444390989000 ms to file 'file:/tmp/checkpoint-1444390989000'
15/10/09 06:43:09 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:09 INFO DAGScheduler: Job 1 finished: foreachRDD at Events.scala:18, took 0.000620 s
15/10/09 06:43:09 INFO JobScheduler: Finished job streaming job 1444390989000 ms.0 from job set of time 1444390989000 ms
15/10/09 06:43:09 INFO JobScheduler: Total delay: 0.037 s for time 1444390989000 ms (execution: 0.009 s)
15/10/09 06:43:09 INFO JobGenerator: Checkpointing graph for time 1444390989000 ms
15/10/09 06:43:09 INFO DStreamGraph: Updating checkpoint data for time 1444390989000 ms
15/10/09 06:43:09 INFO DStreamGraph: Updated checkpoint data for time 1444390989000 ms
15/10/09 06:43:09 INFO CheckpointWriter: Checkpoint for time 1444390989000 ms saved to file 'file:/tmp/checkpoint-1444390989000', took 3619 bytes and 22 ms
15/10/09 06:43:09 INFO CheckpointWriter: Saving checkpoint for time 1444390989000 ms to file 'file:/tmp/checkpoint-1444390989000'
15/10/09 06:43:09 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, ANY, 3613 bytes)
15/10/09 06:43:09 INFO CheckpointWriter: Checkpoint for time 1444390989000 ms saved to file 'file:/tmp/checkpoint-1444390989000', took 3615 bytes and 10 ms
15/10/09 06:43:09 INFO DStreamGraph: Clearing checkpoint data for time 1444390989000 ms
15/10/09 06:43:09 INFO DStreamGraph: Cleared checkpoint data for time 1444390989000 ms
15/10/09 06:43:09 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 06:43:09 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
15/10/09 06:43:09 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390988000: 
15/10/09 06:43:09 INFO ReceiverTracker: Cleanup old received batch data: 1444390988000 ms
15/10/09 06:43:09 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390988000
15/10/09 06:43:09 INFO InputInfoTracker: remove old batch metadata: 
15/10/09 06:43:09 INFO Executor: Fetching http://10.101.21.127:55618/jars/events-assembly-1.0.jar with timestamp 1444390988274
15/10/09 06:43:09 INFO Utils: Fetching http://10.101.21.127:55618/jars/events-assembly-1.0.jar to /private/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/spark-e8c0e34a-82c4-4275-a5e7-1a397218521b/userFiles-5ee90710-2283-4e1e-a74f-bf42a3d39399/fetchFileTemp9118956106498371935.tmp
15/10/09 06:43:09 INFO Executor: Adding file:/private/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/spark-e8c0e34a-82c4-4275-a5e7-1a397218521b/userFiles-5ee90710-2283-4e1e-a74f-bf42a3d39399/events-assembly-1.0.jar to class loader
15/10/09 06:43:09 WARN WriteAheadLogBasedBlockHandler: Storage level replication 2 is unnecessary when write ahead log is enabled, change to replication 1
15/10/09 06:43:09 WARN WriteAheadLogBasedBlockHandler: User defined storage level StorageLevel(true, true, false, false, 2) is changed to effective storage level StorageLevel(true, true, false, false, 1) when write ahead log is enabled
15/10/09 06:43:09 INFO RecurringTimer: Started timer for BlockGenerator at time 1444390989400
15/10/09 06:43:09 INFO BlockGenerator: Started BlockGenerator
15/10/09 06:43:09 INFO BlockGenerator: Started block pushing thread
15/10/09 06:43:09 INFO ReceiverTracker: Registered receiver for stream 0 from 10.101.21.127:55617
15/10/09 06:43:09 INFO ReceiverSupervisorImpl: Starting receiver
15/10/09 06:43:09 INFO ReliableKafkaReceiver: Starting Kafka Consumer Stream with group: Events
15/10/09 06:43:09 INFO VerifiableProperties: Verifying properties
15/10/09 06:43:09 INFO VerifiableProperties: Property auto.commit.enable is overridden to false
15/10/09 06:43:09 INFO VerifiableProperties: Property group.id is overridden to Events
15/10/09 06:43:09 INFO VerifiableProperties: Property zookeeper.connect is overridden to localhost
15/10/09 06:43:09 INFO VerifiableProperties: Property zookeeper.connection.timeout.ms is overridden to 10000
15/10/09 06:43:09 INFO ReliableKafkaReceiver: Connecting to Zookeeper: localhost
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], Connecting to zookeeper instance at localhost
15/10/09 06:43:09 INFO ZkEventThread: Starting ZkClient event thread.
15/10/09 06:43:09 INFO ZooKeeper: Client environment:zookeeper.version=3.4.6-1569965, built on 02/20/2014 09:09 GMT
15/10/09 06:43:09 INFO ZooKeeper: Client environment:host.name=10.101.21.127
15/10/09 06:43:09 INFO ZooKeeper: Client environment:java.version=1.8.0_51
15/10/09 06:43:09 INFO ZooKeeper: Client environment:java.vendor=Oracle Corporation
15/10/09 06:43:09 INFO ZooKeeper: Client environment:java.home=/Library/Java/JavaVirtualMachines/jdk1.8.0_51.jdk/Contents/Home/jre
15/10/09 06:43:09 INFO ZooKeeper: Client environment:java.class.path=/Users/andrewclarkson/code/resilient-kafka-streaming-in-spark/spark-1.5.1-bin-hadoop2.6/conf/:/Users/andrewclarkson/code/resilient-kafka-streaming-in-spark/spark-1.5.1-bin-hadoop2.6/lib/spark-assembly-1.5.1-hadoop2.6.0.jar:/Users/andrewclarkson/code/resilient-kafka-streaming-in-spark/spark-1.5.1-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar:/Users/andrewclarkson/code/resilient-kafka-streaming-in-spark/spark-1.5.1-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar:/Users/andrewclarkson/code/resilient-kafka-streaming-in-spark/spark-1.5.1-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar
15/10/09 06:43:09 INFO ZooKeeper: Client environment:java.library.path=/Users/andrewclarkson/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
15/10/09 06:43:09 INFO ZooKeeper: Client environment:java.io.tmpdir=/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/
15/10/09 06:43:09 INFO ZooKeeper: Client environment:java.compiler=<NA>
15/10/09 06:43:09 INFO ZooKeeper: Client environment:os.name=Mac OS X
15/10/09 06:43:09 INFO ZooKeeper: Client environment:os.arch=x86_64
15/10/09 06:43:09 INFO ZooKeeper: Client environment:os.version=10.10.4
15/10/09 06:43:09 INFO ZooKeeper: Client environment:user.name=andrewclarkson
15/10/09 06:43:09 INFO ZooKeeper: Client environment:user.home=/Users/andrewclarkson
15/10/09 06:43:09 INFO ZooKeeper: Client environment:user.dir=/Users/andrewclarkson/code/resilient-kafka-streaming-in-spark
15/10/09 06:43:09 INFO ZooKeeper: Initiating client connection, connectString=localhost sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@4817ac33
15/10/09 06:43:09 INFO ClientCnxn: Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
15/10/09 06:43:09 INFO ClientCnxn: Socket connection established to localhost/127.0.0.1:2181, initiating session
15/10/09 06:43:09 INFO ClientCnxn: Session establishment complete on server localhost/127.0.0.1:2181, sessionid = 0x15048f8a9500015, negotiated timeout = 6000
15/10/09 06:43:09 INFO ZkClient: zookeeper state changed (SyncConnected)
15/10/09 06:43:09 WARN AppInfo$: Can't read Kafka version from MANIFEST.MF. Possible cause: java.lang.NullPointerException
15/10/09 06:43:09 INFO ReliableKafkaReceiver: Connected to Zookeeper: localhost
15/10/09 06:43:09 INFO ZooKeeper: Initiating client connection, connectString=localhost sessionTimeout=6000 watcher=org.I0Itec.zkclient.ZkClient@71162d8b
15/10/09 06:43:09 INFO ZkEventThread: Starting ZkClient event thread.
15/10/09 06:43:09 INFO ClientCnxn: Opening socket connection to server localhost/0:0:0:0:0:0:0:1:2181. Will not attempt to authenticate using SASL (unknown error)
15/10/09 06:43:09 INFO ClientCnxn: Socket connection established to localhost/0:0:0:0:0:0:0:1:2181, initiating session
15/10/09 06:43:09 INFO ClientCnxn: Session establishment complete on server localhost/0:0:0:0:0:0:0:1:2181, sessionid = 0x15048f8a9500016, negotiated timeout = 6000
15/10/09 06:43:09 INFO ZkClient: zookeeper state changed (SyncConnected)
15/10/09 06:43:09 INFO RecurringTimer: Started timer for BlockGenerator at time 1444390989600
15/10/09 06:43:09 INFO BlockGenerator: Started BlockGenerator
15/10/09 06:43:09 INFO BlockGenerator: Started block pushing thread
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], begin registering consumer Events_Andrew-C-MBP.local-1444390989356-f9caca59 in ZK
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], end registering consumer Events_Andrew-C-MBP.local-1444390989356-f9caca59 in ZK
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], starting watcher executor thread for consumer Events_Andrew-C-MBP.local-1444390989356-f9caca59
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], begin rebalancing consumer Events_Andrew-C-MBP.local-1444390989356-f9caca59 try #0
15/10/09 06:43:09 INFO ConsumerFetcherManager: [ConsumerFetcherManager-1444390989404] Stopping leader finder thread
15/10/09 06:43:09 INFO ConsumerFetcherManager: [ConsumerFetcherManager-1444390989404] Stopping all fetchers
15/10/09 06:43:09 INFO ConsumerFetcherManager: [ConsumerFetcherManager-1444390989404] All connections stopped
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], Cleared all relevant queues for this fetcher
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], Cleared the data chunks in all the consumer message iterators
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], Committing all offsets after clearing the fetcher queues
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], Releasing partition ownership
15/10/09 06:43:09 INFO RangeAssignor: Consumer Events_Andrew-C-MBP.local-1444390989356-f9caca59 rebalancing the following partitions: ArrayBuffer(0) for topic events with consumers: List(Events_Andrew-C-MBP.local-1444390989356-f9caca59-0)
15/10/09 06:43:09 INFO RangeAssignor: Events_Andrew-C-MBP.local-1444390989356-f9caca59-0 attempting to claim partition 0
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], Events_Andrew-C-MBP.local-1444390989356-f9caca59-0 successfully owned partition 0 for topic events
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], Consumer Events_Andrew-C-MBP.local-1444390989356-f9caca59 selected partitions : events:0: fetched offset = 328: consumed offset = 328
15/10/09 06:43:09 INFO ConsumerFetcherManager$LeaderFinderThread: [Events_Andrew-C-MBP.local-1444390989356-f9caca59-leader-finder-thread], Starting 
15/10/09 06:43:09 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], end rebalancing consumer Events_Andrew-C-MBP.local-1444390989356-f9caca59 try #0
15/10/09 06:43:09 INFO ReceiverSupervisorImpl: Called receiver onStart
15/10/09 06:43:09 INFO ReceiverSupervisorImpl: Waiting for receiver to be stopped
15/10/09 06:43:09 INFO VerifiableProperties: Verifying properties
15/10/09 06:43:09 INFO VerifiableProperties: Property client.id is overridden to Events
15/10/09 06:43:09 INFO VerifiableProperties: Property metadata.broker.list is overridden to 10.101.21.127:9092
15/10/09 06:43:09 INFO VerifiableProperties: Property request.timeout.ms is overridden to 30000
15/10/09 06:43:09 INFO ClientUtils$: Fetching metadata from broker id:0,host:10.101.21.127,port:9092 with correlation id 0 for 1 topic(s) Set(events)
15/10/09 06:43:09 INFO SyncProducer: Connected to 10.101.21.127:9092 for producing
15/10/09 06:43:09 INFO SyncProducer: Disconnecting from 10.101.21.127:9092
15/10/09 06:43:09 INFO ConsumerFetcherThread: [ConsumerFetcherThread-Events_Andrew-C-MBP.local-1444390989356-f9caca59-0-0], Starting 
15/10/09 06:43:09 INFO ConsumerFetcherManager: [ConsumerFetcherManager-1444390989404] Added fetcher for partitions ArrayBuffer([[events,0], initOffset 328 to broker id:0,host:10.101.21.127,port:9092] )
15/10/09 06:43:09 INFO MemoryStore: ensureFreeSpace(1097) called with curMem=82975, maxMem=555755765
15/10/09 06:43:09 INFO MemoryStore: Block input-0-1444390989311 stored as bytes in memory (estimated size 1097.0 B, free 529.9 MB)
15/10/09 06:43:09 INFO BlockManagerInfo: Added input-0-1444390989311 in memory on localhost:55619 (size: 1097.0 B, free: 530.0 MB)
15/10/09 06:43:09 INFO ReliableKafkaReceiver: Committed offset 421 for topic events, partition 0
15/10/09 06:43:09 INFO BlockGenerator: Pushed block input-0-1444390989600
15/10/09 06:43:10 INFO JobScheduler: Added jobs for time 1444390990000 ms
15/10/09 06:43:10 INFO JobGenerator: Checkpointing graph for time 1444390990000 ms
15/10/09 06:43:10 INFO DStreamGraph: Updating checkpoint data for time 1444390990000 ms
15/10/09 06:43:10 INFO JobScheduler: Starting job streaming job 1444390990000 ms.0 from job set of time 1444390990000 ms
15/10/09 06:43:10 INFO DStreamGraph: Updated checkpoint data for time 1444390990000 ms
15/10/09 06:43:10 INFO CheckpointWriter: Saving checkpoint for time 1444390990000 ms to file 'file:/tmp/checkpoint-1444390990000'
15/10/09 06:43:10 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:10 INFO DAGScheduler: Got job 2 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:10 INFO DAGScheduler: Final stage: ResultStage 1(foreachRDD at Events.scala:18)
15/10/09 06:43:10 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:10 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:10 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[4] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:10 INFO CheckpointWriter: Checkpoint for time 1444390990000 ms saved to file 'file:/tmp/checkpoint-1444390990000', took 3619 bytes and 10 ms
15/10/09 06:43:10 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=84072, maxMem=555755765
15/10/09 06:43:10 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 61.1 KB, free 529.9 MB)
15/10/09 06:43:10 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=146680, maxMem=555755765
15/10/09 06:43:10 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.9 MB)
15/10/09 06:43:10 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 530.0 MB)
15/10/09 06:43:10 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:10 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[4] at map at Events.scala:13)
15/10/09 06:43:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
15/10/09 06:43:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
15/10/09 06:43:10 INFO BlockManager: Found block input-0-1444390989311 locally
15/10/09 06:43:10 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:43:10 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:43:10 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job
15/10/09 06:43:10 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
15/10/09 06:43:10 INFO TaskSchedulerImpl: Cancelling stage 1
15/10/09 06:43:10 INFO DAGScheduler: ResultStage 1 (foreachRDD at Events.scala:18) failed in 0.038 s
15/10/09 06:43:10 INFO DAGScheduler: Job 2 failed: foreachRDD at Events.scala:18, took 0.059903 s
15/10/09 06:43:10 INFO JobScheduler: Finished job streaming job 1444390990000 ms.0 from job set of time 1444390990000 ms
15/10/09 06:43:10 INFO JobScheduler: Total delay: 0.079 s for time 1444390990000 ms (execution: 0.066 s)
15/10/09 06:43:10 INFO MapPartitionsRDD: Removing RDD 2 from persistence list
15/10/09 06:43:10 ERROR JobScheduler: Error running job streaming job 1444390990000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:43:10 INFO WriteAheadLogBackedBlockRDD: Removing RDD 1 from persistence list
15/10/09 06:43:10 INFO BlockManager: Removing RDD 2
15/10/09 06:43:10 INFO BlockManager: Removing RDD 1
15/10/09 06:43:10 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[1] at createStream at Events.scala:13 of time 1444390990000 ms
15/10/09 06:43:10 INFO JobGenerator: Checkpointing graph for time 1444390990000 ms
15/10/09 06:43:10 INFO DStreamGraph: Updating checkpoint data for time 1444390990000 ms
15/10/09 06:43:10 INFO DStreamGraph: Updated checkpoint data for time 1444390990000 ms
15/10/09 06:43:10 INFO CheckpointWriter: Saving checkpoint for time 1444390990000 ms to file 'file:/tmp/checkpoint-1444390990000'
15/10/09 06:43:10 INFO CheckpointWriter: Checkpoint for time 1444390990000 ms saved to file 'file:/tmp/checkpoint-1444390990000', took 3615 bytes and 10 ms
15/10/09 06:43:10 INFO DStreamGraph: Clearing checkpoint data for time 1444390990000 ms
15/10/09 06:43:10 INFO DStreamGraph: Cleared checkpoint data for time 1444390990000 ms
15/10/09 06:43:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer()
15/10/09 06:43:10 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390989000: 
15/10/09 06:43:10 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390989000
15/10/09 06:43:10 INFO ReceiverTracker: Cleanup old received batch data: 1444390989000 ms
15/10/09 06:43:10 INFO InputInfoTracker: remove old batch metadata: 
15/10/09 06:43:10 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390989000: 
15/10/09 06:43:10 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390989000
15/10/09 06:43:11 INFO JobScheduler: Added jobs for time 1444390991000 ms
15/10/09 06:43:11 INFO JobGenerator: Checkpointing graph for time 1444390991000 ms
15/10/09 06:43:11 INFO DStreamGraph: Updating checkpoint data for time 1444390991000 ms
15/10/09 06:43:11 INFO JobScheduler: Starting job streaming job 1444390991000 ms.0 from job set of time 1444390991000 ms
15/10/09 06:43:11 INFO DStreamGraph: Updated checkpoint data for time 1444390991000 ms
15/10/09 06:43:11 INFO CheckpointWriter: Saving checkpoint for time 1444390991000 ms to file 'file:/tmp/checkpoint-1444390991000'
15/10/09 06:43:11 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:11 INFO DAGScheduler: Job 3 finished: foreachRDD at Events.scala:18, took 0.000031 s
15/10/09 06:43:11 INFO JobScheduler: Finished job streaming job 1444390991000 ms.0 from job set of time 1444390991000 ms
15/10/09 06:43:11 INFO JobScheduler: Total delay: 0.011 s for time 1444390991000 ms (execution: 0.007 s)
15/10/09 06:43:11 INFO MapPartitionsRDD: Removing RDD 4 from persistence list
15/10/09 06:43:11 INFO BlockManager: Removing RDD 4
15/10/09 06:43:11 INFO WriteAheadLogBackedBlockRDD: Removing RDD 3 from persistence list
15/10/09 06:43:11 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[3] at createStream at Events.scala:13 of time 1444390991000 ms
15/10/09 06:43:11 INFO BlockManager: Removing RDD 3
15/10/09 06:43:11 INFO JobGenerator: Checkpointing graph for time 1444390991000 ms
15/10/09 06:43:11 INFO DStreamGraph: Updating checkpoint data for time 1444390991000 ms
15/10/09 06:43:11 INFO DStreamGraph: Updated checkpoint data for time 1444390991000 ms
15/10/09 06:43:11 INFO BlockManagerInfo: Removed input-0-1444390989311 on localhost:55619 in memory (size: 1097.0 B, free: 530.0 MB)
15/10/09 06:43:11 INFO CheckpointWriter: Checkpoint for time 1444390991000 ms saved to file 'file:/tmp/checkpoint-1444390991000', took 3619 bytes and 11 ms
15/10/09 06:43:11 INFO CheckpointWriter: Saving checkpoint for time 1444390991000 ms to file 'file:/tmp/checkpoint-1444390991000'
15/10/09 06:43:11 INFO CheckpointWriter: Checkpoint for time 1444390991000 ms saved to file 'file:/tmp/checkpoint-1444390991000', took 3615 bytes and 8 ms
15/10/09 06:43:11 INFO DStreamGraph: Clearing checkpoint data for time 1444390991000 ms
15/10/09 06:43:11 INFO DStreamGraph: Cleared checkpoint data for time 1444390991000 ms
15/10/09 06:43:11 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390989000 ms)
15/10/09 06:43:11 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390990000: 
15/10/09 06:43:11 INFO ReceiverTracker: Cleanup old received batch data: 1444390990000 ms
15/10/09 06:43:11 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390990000
15/10/09 06:43:11 INFO InputInfoTracker: remove old batch metadata: 1444390989000 ms
15/10/09 06:43:11 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390990000: 
15/10/09 06:43:11 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390990000
15/10/09 06:43:12 INFO JobScheduler: Added jobs for time 1444390992000 ms
15/10/09 06:43:12 INFO JobGenerator: Checkpointing graph for time 1444390992000 ms
15/10/09 06:43:12 INFO DStreamGraph: Updating checkpoint data for time 1444390992000 ms
15/10/09 06:43:12 INFO JobScheduler: Starting job streaming job 1444390992000 ms.0 from job set of time 1444390992000 ms
15/10/09 06:43:12 INFO DStreamGraph: Updated checkpoint data for time 1444390992000 ms
15/10/09 06:43:12 INFO CheckpointWriter: Saving checkpoint for time 1444390992000 ms to file 'file:/tmp/checkpoint-1444390992000'
15/10/09 06:43:12 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:12 INFO DAGScheduler: Job 4 finished: foreachRDD at Events.scala:18, took 0.000024 s
15/10/09 06:43:12 INFO JobScheduler: Finished job streaming job 1444390992000 ms.0 from job set of time 1444390992000 ms
15/10/09 06:43:12 INFO JobScheduler: Total delay: 0.013 s for time 1444390992000 ms (execution: 0.006 s)
15/10/09 06:43:12 INFO MapPartitionsRDD: Removing RDD 6 from persistence list
15/10/09 06:43:12 INFO BlockManager: Removing RDD 6
15/10/09 06:43:12 INFO WriteAheadLogBackedBlockRDD: Removing RDD 5 from persistence list
15/10/09 06:43:12 INFO BlockManager: Removing RDD 5
15/10/09 06:43:12 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[5] at createStream at Events.scala:13 of time 1444390992000 ms
15/10/09 06:43:12 INFO JobGenerator: Checkpointing graph for time 1444390992000 ms
15/10/09 06:43:12 INFO DStreamGraph: Updating checkpoint data for time 1444390992000 ms
15/10/09 06:43:12 INFO DStreamGraph: Updated checkpoint data for time 1444390992000 ms
15/10/09 06:43:12 INFO CheckpointWriter: Checkpoint for time 1444390992000 ms saved to file 'file:/tmp/checkpoint-1444390992000', took 3619 bytes and 11 ms
15/10/09 06:43:12 INFO CheckpointWriter: Saving checkpoint for time 1444390992000 ms to file 'file:/tmp/checkpoint-1444390992000'
15/10/09 06:43:12 INFO CheckpointWriter: Checkpoint for time 1444390992000 ms saved to file 'file:/tmp/checkpoint-1444390992000', took 3615 bytes and 8 ms
15/10/09 06:43:12 INFO DStreamGraph: Clearing checkpoint data for time 1444390992000 ms
15/10/09 06:43:12 INFO DStreamGraph: Cleared checkpoint data for time 1444390992000 ms
15/10/09 06:43:12 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390990000 ms)
15/10/09 06:43:12 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390991000: 
15/10/09 06:43:12 INFO ReceiverTracker: Cleanup old received batch data: 1444390991000 ms
15/10/09 06:43:12 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390991000
15/10/09 06:43:12 INFO InputInfoTracker: remove old batch metadata: 1444390990000 ms
15/10/09 06:43:12 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390991000: 
15/10/09 06:43:12 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390991000
15/10/09 06:43:13 INFO JobScheduler: Added jobs for time 1444390993000 ms
15/10/09 06:43:13 INFO JobGenerator: Checkpointing graph for time 1444390993000 ms
15/10/09 06:43:13 INFO DStreamGraph: Updating checkpoint data for time 1444390993000 ms
15/10/09 06:43:13 INFO JobScheduler: Starting job streaming job 1444390993000 ms.0 from job set of time 1444390993000 ms
15/10/09 06:43:13 INFO DStreamGraph: Updated checkpoint data for time 1444390993000 ms
15/10/09 06:43:13 INFO CheckpointWriter: Saving checkpoint for time 1444390993000 ms to file 'file:/tmp/checkpoint-1444390993000'
15/10/09 06:43:13 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:13 INFO DAGScheduler: Job 5 finished: foreachRDD at Events.scala:18, took 0.000024 s
15/10/09 06:43:13 INFO JobScheduler: Finished job streaming job 1444390993000 ms.0 from job set of time 1444390993000 ms
15/10/09 06:43:13 INFO JobScheduler: Total delay: 0.009 s for time 1444390993000 ms (execution: 0.005 s)
15/10/09 06:43:13 INFO MapPartitionsRDD: Removing RDD 8 from persistence list
15/10/09 06:43:13 INFO BlockManager: Removing RDD 8
15/10/09 06:43:13 INFO WriteAheadLogBackedBlockRDD: Removing RDD 7 from persistence list
15/10/09 06:43:13 INFO BlockManager: Removing RDD 7
15/10/09 06:43:13 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[7] at createStream at Events.scala:13 of time 1444390993000 ms
15/10/09 06:43:13 INFO JobGenerator: Checkpointing graph for time 1444390993000 ms
15/10/09 06:43:13 INFO DStreamGraph: Updating checkpoint data for time 1444390993000 ms
15/10/09 06:43:13 INFO DStreamGraph: Updated checkpoint data for time 1444390993000 ms
15/10/09 06:43:13 INFO CheckpointWriter: Checkpoint for time 1444390993000 ms saved to file 'file:/tmp/checkpoint-1444390993000', took 3619 bytes and 11 ms
15/10/09 06:43:13 INFO CheckpointWriter: Saving checkpoint for time 1444390993000 ms to file 'file:/tmp/checkpoint-1444390993000'
15/10/09 06:43:13 INFO CheckpointWriter: Checkpoint for time 1444390993000 ms saved to file 'file:/tmp/checkpoint-1444390993000', took 3615 bytes and 9 ms
15/10/09 06:43:13 INFO DStreamGraph: Clearing checkpoint data for time 1444390993000 ms
15/10/09 06:43:13 INFO DStreamGraph: Cleared checkpoint data for time 1444390993000 ms
15/10/09 06:43:13 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390991000 ms)
15/10/09 06:43:13 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390992000: 
15/10/09 06:43:13 INFO ReceiverTracker: Cleanup old received batch data: 1444390992000 ms
15/10/09 06:43:13 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390992000
15/10/09 06:43:13 INFO InputInfoTracker: remove old batch metadata: 1444390991000 ms
15/10/09 06:43:13 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390992000: 
15/10/09 06:43:13 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390992000
15/10/09 06:43:14 INFO JobScheduler: Added jobs for time 1444390994000 ms
15/10/09 06:43:14 INFO JobGenerator: Checkpointing graph for time 1444390994000 ms
15/10/09 06:43:14 INFO DStreamGraph: Updating checkpoint data for time 1444390994000 ms
15/10/09 06:43:14 INFO JobScheduler: Starting job streaming job 1444390994000 ms.0 from job set of time 1444390994000 ms
15/10/09 06:43:14 INFO DStreamGraph: Updated checkpoint data for time 1444390994000 ms
15/10/09 06:43:14 INFO CheckpointWriter: Saving checkpoint for time 1444390994000 ms to file 'file:/tmp/checkpoint-1444390994000'
15/10/09 06:43:14 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:14 INFO DAGScheduler: Job 6 finished: foreachRDD at Events.scala:18, took 0.000042 s
15/10/09 06:43:14 INFO JobScheduler: Finished job streaming job 1444390994000 ms.0 from job set of time 1444390994000 ms
15/10/09 06:43:14 INFO JobScheduler: Total delay: 0.011 s for time 1444390994000 ms (execution: 0.006 s)
15/10/09 06:43:14 INFO MapPartitionsRDD: Removing RDD 10 from persistence list
15/10/09 06:43:14 INFO BlockManager: Removing RDD 10
15/10/09 06:43:14 INFO WriteAheadLogBackedBlockRDD: Removing RDD 9 from persistence list
15/10/09 06:43:14 INFO BlockManager: Removing RDD 9
15/10/09 06:43:14 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[9] at createStream at Events.scala:13 of time 1444390994000 ms
15/10/09 06:43:14 INFO JobGenerator: Checkpointing graph for time 1444390994000 ms
15/10/09 06:43:14 INFO DStreamGraph: Updating checkpoint data for time 1444390994000 ms
15/10/09 06:43:14 INFO DStreamGraph: Updated checkpoint data for time 1444390994000 ms
15/10/09 06:43:14 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390989000.bk
15/10/09 06:43:14 INFO CheckpointWriter: Checkpoint for time 1444390994000 ms saved to file 'file:/tmp/checkpoint-1444390994000', took 3619 bytes and 13 ms
15/10/09 06:43:14 INFO CheckpointWriter: Saving checkpoint for time 1444390994000 ms to file 'file:/tmp/checkpoint-1444390994000'
15/10/09 06:43:14 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390989000
15/10/09 06:43:14 INFO CheckpointWriter: Checkpoint for time 1444390994000 ms saved to file 'file:/tmp/checkpoint-1444390994000', took 3615 bytes and 9 ms
15/10/09 06:43:14 INFO DStreamGraph: Clearing checkpoint data for time 1444390994000 ms
15/10/09 06:43:14 INFO DStreamGraph: Cleared checkpoint data for time 1444390994000 ms
15/10/09 06:43:14 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390992000 ms)
15/10/09 06:43:14 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390993000: 
15/10/09 06:43:14 INFO ReceiverTracker: Cleanup old received batch data: 1444390993000 ms
15/10/09 06:43:14 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390993000
15/10/09 06:43:14 INFO InputInfoTracker: remove old batch metadata: 1444390992000 ms
15/10/09 06:43:14 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390993000: 
15/10/09 06:43:14 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390993000
15/10/09 06:43:15 INFO JobScheduler: Added jobs for time 1444390995000 ms
15/10/09 06:43:15 INFO JobGenerator: Checkpointing graph for time 1444390995000 ms
15/10/09 06:43:15 INFO DStreamGraph: Updating checkpoint data for time 1444390995000 ms
15/10/09 06:43:15 INFO JobScheduler: Starting job streaming job 1444390995000 ms.0 from job set of time 1444390995000 ms
15/10/09 06:43:15 INFO DStreamGraph: Updated checkpoint data for time 1444390995000 ms
15/10/09 06:43:15 INFO CheckpointWriter: Saving checkpoint for time 1444390995000 ms to file 'file:/tmp/checkpoint-1444390995000'
15/10/09 06:43:15 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:15 INFO DAGScheduler: Job 7 finished: foreachRDD at Events.scala:18, took 0.000024 s
15/10/09 06:43:15 INFO JobScheduler: Finished job streaming job 1444390995000 ms.0 from job set of time 1444390995000 ms
15/10/09 06:43:15 INFO JobScheduler: Total delay: 0.011 s for time 1444390995000 ms (execution: 0.004 s)
15/10/09 06:43:15 INFO MapPartitionsRDD: Removing RDD 12 from persistence list
15/10/09 06:43:15 INFO BlockManager: Removing RDD 12
15/10/09 06:43:15 INFO WriteAheadLogBackedBlockRDD: Removing RDD 11 from persistence list
15/10/09 06:43:15 INFO BlockManager: Removing RDD 11
15/10/09 06:43:15 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[11] at createStream at Events.scala:13 of time 1444390995000 ms
15/10/09 06:43:15 INFO JobGenerator: Checkpointing graph for time 1444390995000 ms
15/10/09 06:43:15 INFO DStreamGraph: Updating checkpoint data for time 1444390995000 ms
15/10/09 06:43:15 INFO DStreamGraph: Updated checkpoint data for time 1444390995000 ms
15/10/09 06:43:15 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390990000.bk
15/10/09 06:43:15 INFO CheckpointWriter: Checkpoint for time 1444390995000 ms saved to file 'file:/tmp/checkpoint-1444390995000', took 3619 bytes and 11 ms
15/10/09 06:43:15 INFO CheckpointWriter: Saving checkpoint for time 1444390995000 ms to file 'file:/tmp/checkpoint-1444390995000'
15/10/09 06:43:15 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390990000
15/10/09 06:43:15 INFO CheckpointWriter: Checkpoint for time 1444390995000 ms saved to file 'file:/tmp/checkpoint-1444390995000', took 3615 bytes and 8 ms
15/10/09 06:43:15 INFO DStreamGraph: Clearing checkpoint data for time 1444390995000 ms
15/10/09 06:43:15 INFO DStreamGraph: Cleared checkpoint data for time 1444390995000 ms
15/10/09 06:43:15 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390993000 ms)
15/10/09 06:43:15 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390994000: 
15/10/09 06:43:15 INFO ReceiverTracker: Cleanup old received batch data: 1444390994000 ms
15/10/09 06:43:15 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390994000
15/10/09 06:43:15 INFO InputInfoTracker: remove old batch metadata: 1444390993000 ms
15/10/09 06:43:15 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390994000: 
15/10/09 06:43:15 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390994000
15/10/09 06:43:16 INFO JobScheduler: Added jobs for time 1444390996000 ms
15/10/09 06:43:16 INFO JobGenerator: Checkpointing graph for time 1444390996000 ms
15/10/09 06:43:16 INFO DStreamGraph: Updating checkpoint data for time 1444390996000 ms
15/10/09 06:43:16 INFO JobScheduler: Starting job streaming job 1444390996000 ms.0 from job set of time 1444390996000 ms
15/10/09 06:43:16 INFO DStreamGraph: Updated checkpoint data for time 1444390996000 ms
15/10/09 06:43:16 INFO CheckpointWriter: Saving checkpoint for time 1444390996000 ms to file 'file:/tmp/checkpoint-1444390996000'
15/10/09 06:43:16 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:16 INFO DAGScheduler: Job 8 finished: foreachRDD at Events.scala:18, took 0.000026 s
15/10/09 06:43:16 INFO JobScheduler: Finished job streaming job 1444390996000 ms.0 from job set of time 1444390996000 ms
15/10/09 06:43:16 INFO JobScheduler: Total delay: 0.011 s for time 1444390996000 ms (execution: 0.003 s)
15/10/09 06:43:16 INFO MapPartitionsRDD: Removing RDD 14 from persistence list
15/10/09 06:43:16 INFO WriteAheadLogBackedBlockRDD: Removing RDD 13 from persistence list
15/10/09 06:43:16 INFO BlockManager: Removing RDD 14
15/10/09 06:43:16 INFO BlockManager: Removing RDD 13
15/10/09 06:43:16 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[13] at createStream at Events.scala:13 of time 1444390996000 ms
15/10/09 06:43:16 INFO JobGenerator: Checkpointing graph for time 1444390996000 ms
15/10/09 06:43:16 INFO DStreamGraph: Updating checkpoint data for time 1444390996000 ms
15/10/09 06:43:16 INFO DStreamGraph: Updated checkpoint data for time 1444390996000 ms
15/10/09 06:43:16 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390991000.bk
15/10/09 06:43:16 INFO CheckpointWriter: Checkpoint for time 1444390996000 ms saved to file 'file:/tmp/checkpoint-1444390996000', took 3619 bytes and 9 ms
15/10/09 06:43:16 INFO CheckpointWriter: Saving checkpoint for time 1444390996000 ms to file 'file:/tmp/checkpoint-1444390996000'
15/10/09 06:43:16 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390991000
15/10/09 06:43:16 INFO CheckpointWriter: Checkpoint for time 1444390996000 ms saved to file 'file:/tmp/checkpoint-1444390996000', took 3615 bytes and 8 ms
15/10/09 06:43:16 INFO DStreamGraph: Clearing checkpoint data for time 1444390996000 ms
15/10/09 06:43:16 INFO DStreamGraph: Cleared checkpoint data for time 1444390996000 ms
15/10/09 06:43:16 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390994000 ms)
15/10/09 06:43:16 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390995000: 
15/10/09 06:43:16 INFO ReceiverTracker: Cleanup old received batch data: 1444390995000 ms
15/10/09 06:43:16 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390995000
15/10/09 06:43:16 INFO InputInfoTracker: remove old batch metadata: 1444390994000 ms
15/10/09 06:43:16 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390995000: 
15/10/09 06:43:16 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390995000
15/10/09 06:43:17 INFO JobScheduler: Added jobs for time 1444390997000 ms
15/10/09 06:43:17 INFO JobGenerator: Checkpointing graph for time 1444390997000 ms
15/10/09 06:43:17 INFO DStreamGraph: Updating checkpoint data for time 1444390997000 ms
15/10/09 06:43:17 INFO DStreamGraph: Updated checkpoint data for time 1444390997000 ms
15/10/09 06:43:17 INFO JobScheduler: Starting job streaming job 1444390997000 ms.0 from job set of time 1444390997000 ms
15/10/09 06:43:17 INFO CheckpointWriter: Saving checkpoint for time 1444390997000 ms to file 'file:/tmp/checkpoint-1444390997000'
15/10/09 06:43:17 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:17 INFO DAGScheduler: Job 9 finished: foreachRDD at Events.scala:18, took 0.000024 s
15/10/09 06:43:17 INFO JobScheduler: Finished job streaming job 1444390997000 ms.0 from job set of time 1444390997000 ms
15/10/09 06:43:17 INFO JobScheduler: Total delay: 0.009 s for time 1444390997000 ms (execution: 0.004 s)
15/10/09 06:43:17 INFO MapPartitionsRDD: Removing RDD 16 from persistence list
15/10/09 06:43:17 INFO WriteAheadLogBackedBlockRDD: Removing RDD 15 from persistence list
15/10/09 06:43:17 INFO BlockManager: Removing RDD 16
15/10/09 06:43:17 INFO BlockManager: Removing RDD 15
15/10/09 06:43:17 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[15] at createStream at Events.scala:13 of time 1444390997000 ms
15/10/09 06:43:17 INFO JobGenerator: Checkpointing graph for time 1444390997000 ms
15/10/09 06:43:17 INFO DStreamGraph: Updating checkpoint data for time 1444390997000 ms
15/10/09 06:43:17 INFO DStreamGraph: Updated checkpoint data for time 1444390997000 ms
15/10/09 06:43:17 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390992000.bk
15/10/09 06:43:17 INFO CheckpointWriter: Checkpoint for time 1444390997000 ms saved to file 'file:/tmp/checkpoint-1444390997000', took 3619 bytes and 11 ms
15/10/09 06:43:17 INFO CheckpointWriter: Saving checkpoint for time 1444390997000 ms to file 'file:/tmp/checkpoint-1444390997000'
15/10/09 06:43:17 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390992000
15/10/09 06:43:17 INFO CheckpointWriter: Checkpoint for time 1444390997000 ms saved to file 'file:/tmp/checkpoint-1444390997000', took 3615 bytes and 8 ms
15/10/09 06:43:17 INFO DStreamGraph: Clearing checkpoint data for time 1444390997000 ms
15/10/09 06:43:17 INFO DStreamGraph: Cleared checkpoint data for time 1444390997000 ms
15/10/09 06:43:17 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390995000 ms)
15/10/09 06:43:17 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390996000: 
15/10/09 06:43:17 INFO ReceiverTracker: Cleanup old received batch data: 1444390996000 ms
15/10/09 06:43:17 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390996000
15/10/09 06:43:17 INFO InputInfoTracker: remove old batch metadata: 1444390995000 ms
15/10/09 06:43:17 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390996000: 
15/10/09 06:43:17 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390996000
15/10/09 06:43:18 INFO JobScheduler: Added jobs for time 1444390998000 ms
15/10/09 06:43:18 INFO JobGenerator: Checkpointing graph for time 1444390998000 ms
15/10/09 06:43:18 INFO DStreamGraph: Updating checkpoint data for time 1444390998000 ms
15/10/09 06:43:18 INFO JobScheduler: Starting job streaming job 1444390998000 ms.0 from job set of time 1444390998000 ms
15/10/09 06:43:18 INFO DStreamGraph: Updated checkpoint data for time 1444390998000 ms
15/10/09 06:43:18 INFO CheckpointWriter: Saving checkpoint for time 1444390998000 ms to file 'file:/tmp/checkpoint-1444390998000'
15/10/09 06:43:18 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:18 INFO DAGScheduler: Job 10 finished: foreachRDD at Events.scala:18, took 0.000024 s
15/10/09 06:43:18 INFO JobScheduler: Finished job streaming job 1444390998000 ms.0 from job set of time 1444390998000 ms
15/10/09 06:43:18 INFO JobScheduler: Total delay: 0.010 s for time 1444390998000 ms (execution: 0.005 s)
15/10/09 06:43:18 INFO MapPartitionsRDD: Removing RDD 18 from persistence list
15/10/09 06:43:18 INFO BlockManager: Removing RDD 18
15/10/09 06:43:18 INFO WriteAheadLogBackedBlockRDD: Removing RDD 17 from persistence list
15/10/09 06:43:18 INFO BlockManager: Removing RDD 17
15/10/09 06:43:18 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[17] at createStream at Events.scala:13 of time 1444390998000 ms
15/10/09 06:43:18 INFO JobGenerator: Checkpointing graph for time 1444390998000 ms
15/10/09 06:43:18 INFO DStreamGraph: Updating checkpoint data for time 1444390998000 ms
15/10/09 06:43:18 INFO DStreamGraph: Updated checkpoint data for time 1444390998000 ms
15/10/09 06:43:18 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390993000.bk
15/10/09 06:43:18 INFO CheckpointWriter: Checkpoint for time 1444390998000 ms saved to file 'file:/tmp/checkpoint-1444390998000', took 3619 bytes and 11 ms
15/10/09 06:43:18 INFO CheckpointWriter: Saving checkpoint for time 1444390998000 ms to file 'file:/tmp/checkpoint-1444390998000'
15/10/09 06:43:18 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390993000
15/10/09 06:43:18 INFO CheckpointWriter: Checkpoint for time 1444390998000 ms saved to file 'file:/tmp/checkpoint-1444390998000', took 3615 bytes and 8 ms
15/10/09 06:43:18 INFO DStreamGraph: Clearing checkpoint data for time 1444390998000 ms
15/10/09 06:43:18 INFO DStreamGraph: Cleared checkpoint data for time 1444390998000 ms
15/10/09 06:43:18 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390996000 ms)
15/10/09 06:43:18 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390997000: 
15/10/09 06:43:18 INFO ReceiverTracker: Cleanup old received batch data: 1444390997000 ms
15/10/09 06:43:18 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390997000
15/10/09 06:43:18 INFO InputInfoTracker: remove old batch metadata: 1444390996000 ms
15/10/09 06:43:18 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390997000: 
15/10/09 06:43:18 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390997000
15/10/09 06:43:19 INFO JobScheduler: Added jobs for time 1444390999000 ms
15/10/09 06:43:19 INFO JobGenerator: Checkpointing graph for time 1444390999000 ms
15/10/09 06:43:19 INFO DStreamGraph: Updating checkpoint data for time 1444390999000 ms
15/10/09 06:43:19 INFO JobScheduler: Starting job streaming job 1444390999000 ms.0 from job set of time 1444390999000 ms
15/10/09 06:43:19 INFO DStreamGraph: Updated checkpoint data for time 1444390999000 ms
15/10/09 06:43:19 INFO CheckpointWriter: Saving checkpoint for time 1444390999000 ms to file 'file:/tmp/checkpoint-1444390999000'
15/10/09 06:43:19 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:19 INFO DAGScheduler: Job 11 finished: foreachRDD at Events.scala:18, took 0.000023 s
15/10/09 06:43:19 INFO JobScheduler: Finished job streaming job 1444390999000 ms.0 from job set of time 1444390999000 ms
15/10/09 06:43:19 INFO JobScheduler: Total delay: 0.010 s for time 1444390999000 ms (execution: 0.004 s)
15/10/09 06:43:19 INFO MapPartitionsRDD: Removing RDD 20 from persistence list
15/10/09 06:43:19 INFO BlockManager: Removing RDD 20
15/10/09 06:43:19 INFO WriteAheadLogBackedBlockRDD: Removing RDD 19 from persistence list
15/10/09 06:43:19 INFO BlockManager: Removing RDD 19
15/10/09 06:43:19 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[19] at createStream at Events.scala:13 of time 1444390999000 ms
15/10/09 06:43:19 INFO JobGenerator: Checkpointing graph for time 1444390999000 ms
15/10/09 06:43:19 INFO DStreamGraph: Updating checkpoint data for time 1444390999000 ms
15/10/09 06:43:19 INFO DStreamGraph: Updated checkpoint data for time 1444390999000 ms
15/10/09 06:43:19 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390994000.bk
15/10/09 06:43:19 INFO CheckpointWriter: Checkpoint for time 1444390999000 ms saved to file 'file:/tmp/checkpoint-1444390999000', took 3619 bytes and 11 ms
15/10/09 06:43:19 INFO CheckpointWriter: Saving checkpoint for time 1444390999000 ms to file 'file:/tmp/checkpoint-1444390999000'
15/10/09 06:43:19 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390994000
15/10/09 06:43:19 INFO CheckpointWriter: Checkpoint for time 1444390999000 ms saved to file 'file:/tmp/checkpoint-1444390999000', took 3615 bytes and 7 ms
15/10/09 06:43:19 INFO DStreamGraph: Clearing checkpoint data for time 1444390999000 ms
15/10/09 06:43:19 INFO DStreamGraph: Cleared checkpoint data for time 1444390999000 ms
15/10/09 06:43:19 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390997000 ms)
15/10/09 06:43:19 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390998000: 
15/10/09 06:43:19 INFO ReceiverTracker: Cleanup old received batch data: 1444390998000 ms
15/10/09 06:43:19 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390998000
15/10/09 06:43:19 INFO InputInfoTracker: remove old batch metadata: 1444390997000 ms
15/10/09 06:43:19 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390998000: 
15/10/09 06:43:19 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390998000
15/10/09 06:43:20 INFO JobScheduler: Added jobs for time 1444391000000 ms
15/10/09 06:43:20 INFO JobGenerator: Checkpointing graph for time 1444391000000 ms
15/10/09 06:43:20 INFO DStreamGraph: Updating checkpoint data for time 1444391000000 ms
15/10/09 06:43:20 INFO JobScheduler: Starting job streaming job 1444391000000 ms.0 from job set of time 1444391000000 ms
15/10/09 06:43:20 INFO DStreamGraph: Updated checkpoint data for time 1444391000000 ms
15/10/09 06:43:20 INFO CheckpointWriter: Saving checkpoint for time 1444391000000 ms to file 'file:/tmp/checkpoint-1444391000000'
15/10/09 06:43:20 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:20 INFO DAGScheduler: Job 12 finished: foreachRDD at Events.scala:18, took 0.000024 s
15/10/09 06:43:20 INFO JobScheduler: Finished job streaming job 1444391000000 ms.0 from job set of time 1444391000000 ms
15/10/09 06:43:20 INFO JobScheduler: Total delay: 0.011 s for time 1444391000000 ms (execution: 0.003 s)
15/10/09 06:43:20 INFO MapPartitionsRDD: Removing RDD 22 from persistence list
15/10/09 06:43:20 INFO WriteAheadLogBackedBlockRDD: Removing RDD 21 from persistence list
15/10/09 06:43:20 INFO BlockManager: Removing RDD 22
15/10/09 06:43:20 INFO BlockManager: Removing RDD 21
15/10/09 06:43:20 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[21] at createStream at Events.scala:13 of time 1444391000000 ms
15/10/09 06:43:20 INFO JobGenerator: Checkpointing graph for time 1444391000000 ms
15/10/09 06:43:20 INFO DStreamGraph: Updating checkpoint data for time 1444391000000 ms
15/10/09 06:43:20 INFO DStreamGraph: Updated checkpoint data for time 1444391000000 ms
15/10/09 06:43:20 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390995000.bk
15/10/09 06:43:20 INFO CheckpointWriter: Checkpoint for time 1444391000000 ms saved to file 'file:/tmp/checkpoint-1444391000000', took 3619 bytes and 10 ms
15/10/09 06:43:20 INFO CheckpointWriter: Saving checkpoint for time 1444391000000 ms to file 'file:/tmp/checkpoint-1444391000000'
15/10/09 06:43:20 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390995000
15/10/09 06:43:20 INFO CheckpointWriter: Checkpoint for time 1444391000000 ms saved to file 'file:/tmp/checkpoint-1444391000000', took 3615 bytes and 8 ms
15/10/09 06:43:20 INFO DStreamGraph: Clearing checkpoint data for time 1444391000000 ms
15/10/09 06:43:20 INFO DStreamGraph: Cleared checkpoint data for time 1444391000000 ms
15/10/09 06:43:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390998000 ms)
15/10/09 06:43:20 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444390999000: 
15/10/09 06:43:20 INFO ReceiverTracker: Cleanup old received batch data: 1444390999000 ms
15/10/09 06:43:20 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444390999000
15/10/09 06:43:20 INFO InputInfoTracker: remove old batch metadata: 1444390998000 ms
15/10/09 06:43:20 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444390999000: 
15/10/09 06:43:20 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444390999000
15/10/09 06:43:21 INFO JobScheduler: Added jobs for time 1444391001000 ms
15/10/09 06:43:21 INFO JobGenerator: Checkpointing graph for time 1444391001000 ms
15/10/09 06:43:21 INFO DStreamGraph: Updating checkpoint data for time 1444391001000 ms
15/10/09 06:43:21 INFO JobScheduler: Starting job streaming job 1444391001000 ms.0 from job set of time 1444391001000 ms
15/10/09 06:43:21 INFO DStreamGraph: Updated checkpoint data for time 1444391001000 ms
15/10/09 06:43:21 INFO CheckpointWriter: Saving checkpoint for time 1444391001000 ms to file 'file:/tmp/checkpoint-1444391001000'
15/10/09 06:43:21 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:21 INFO DAGScheduler: Job 13 finished: foreachRDD at Events.scala:18, took 0.000026 s
15/10/09 06:43:21 INFO JobScheduler: Finished job streaming job 1444391001000 ms.0 from job set of time 1444391001000 ms
15/10/09 06:43:21 INFO JobScheduler: Total delay: 0.011 s for time 1444391001000 ms (execution: 0.004 s)
15/10/09 06:43:21 INFO MapPartitionsRDD: Removing RDD 24 from persistence list
15/10/09 06:43:21 INFO WriteAheadLogBackedBlockRDD: Removing RDD 23 from persistence list
15/10/09 06:43:21 INFO BlockManager: Removing RDD 24
15/10/09 06:43:21 INFO BlockManager: Removing RDD 23
15/10/09 06:43:21 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[23] at createStream at Events.scala:13 of time 1444391001000 ms
15/10/09 06:43:21 INFO JobGenerator: Checkpointing graph for time 1444391001000 ms
15/10/09 06:43:21 INFO DStreamGraph: Updating checkpoint data for time 1444391001000 ms
15/10/09 06:43:21 INFO DStreamGraph: Updated checkpoint data for time 1444391001000 ms
15/10/09 06:43:21 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390996000.bk
15/10/09 06:43:21 INFO CheckpointWriter: Checkpoint for time 1444391001000 ms saved to file 'file:/tmp/checkpoint-1444391001000', took 3619 bytes and 10 ms
15/10/09 06:43:21 INFO CheckpointWriter: Saving checkpoint for time 1444391001000 ms to file 'file:/tmp/checkpoint-1444391001000'
15/10/09 06:43:21 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390996000
15/10/09 06:43:21 INFO CheckpointWriter: Checkpoint for time 1444391001000 ms saved to file 'file:/tmp/checkpoint-1444391001000', took 3615 bytes and 8 ms
15/10/09 06:43:21 INFO DStreamGraph: Clearing checkpoint data for time 1444391001000 ms
15/10/09 06:43:21 INFO DStreamGraph: Cleared checkpoint data for time 1444391001000 ms
15/10/09 06:43:21 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444390999000 ms)
15/10/09 06:43:21 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391000000: 
15/10/09 06:43:21 INFO ReceiverTracker: Cleanup old received batch data: 1444391000000 ms
15/10/09 06:43:21 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391000000
15/10/09 06:43:21 INFO InputInfoTracker: remove old batch metadata: 1444390999000 ms
15/10/09 06:43:21 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391000000: 
15/10/09 06:43:21 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391000000
15/10/09 06:43:22 INFO JobScheduler: Added jobs for time 1444391002000 ms
15/10/09 06:43:22 INFO JobGenerator: Checkpointing graph for time 1444391002000 ms
15/10/09 06:43:22 INFO DStreamGraph: Updating checkpoint data for time 1444391002000 ms
15/10/09 06:43:22 INFO JobScheduler: Starting job streaming job 1444391002000 ms.0 from job set of time 1444391002000 ms
15/10/09 06:43:22 INFO DStreamGraph: Updated checkpoint data for time 1444391002000 ms
15/10/09 06:43:22 INFO CheckpointWriter: Saving checkpoint for time 1444391002000 ms to file 'file:/tmp/checkpoint-1444391002000'
15/10/09 06:43:22 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:22 INFO DAGScheduler: Job 14 finished: foreachRDD at Events.scala:18, took 0.000022 s
15/10/09 06:43:22 INFO JobScheduler: Finished job streaming job 1444391002000 ms.0 from job set of time 1444391002000 ms
15/10/09 06:43:22 INFO JobScheduler: Total delay: 0.005 s for time 1444391002000 ms (execution: 0.002 s)
15/10/09 06:43:22 INFO MapPartitionsRDD: Removing RDD 26 from persistence list
15/10/09 06:43:22 INFO BlockManager: Removing RDD 26
15/10/09 06:43:22 INFO WriteAheadLogBackedBlockRDD: Removing RDD 25 from persistence list
15/10/09 06:43:22 INFO BlockManager: Removing RDD 25
15/10/09 06:43:22 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[25] at createStream at Events.scala:13 of time 1444391002000 ms
15/10/09 06:43:22 INFO JobGenerator: Checkpointing graph for time 1444391002000 ms
15/10/09 06:43:22 INFO DStreamGraph: Updating checkpoint data for time 1444391002000 ms
15/10/09 06:43:22 INFO DStreamGraph: Updated checkpoint data for time 1444391002000 ms
15/10/09 06:43:22 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390997000.bk
15/10/09 06:43:22 INFO CheckpointWriter: Checkpoint for time 1444391002000 ms saved to file 'file:/tmp/checkpoint-1444391002000', took 3619 bytes and 9 ms
15/10/09 06:43:22 INFO CheckpointWriter: Saving checkpoint for time 1444391002000 ms to file 'file:/tmp/checkpoint-1444391002000'
15/10/09 06:43:22 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390997000
15/10/09 06:43:22 INFO CheckpointWriter: Checkpoint for time 1444391002000 ms saved to file 'file:/tmp/checkpoint-1444391002000', took 3615 bytes and 7 ms
15/10/09 06:43:22 INFO DStreamGraph: Clearing checkpoint data for time 1444391002000 ms
15/10/09 06:43:22 INFO DStreamGraph: Cleared checkpoint data for time 1444391002000 ms
15/10/09 06:43:22 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391000000 ms)
15/10/09 06:43:22 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391001000: 
15/10/09 06:43:22 INFO ReceiverTracker: Cleanup old received batch data: 1444391001000 ms
15/10/09 06:43:22 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391001000
15/10/09 06:43:22 INFO InputInfoTracker: remove old batch metadata: 1444391000000 ms
15/10/09 06:43:22 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391001000: 
15/10/09 06:43:22 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391001000
15/10/09 06:43:23 INFO JobScheduler: Added jobs for time 1444391003000 ms
15/10/09 06:43:23 INFO JobGenerator: Checkpointing graph for time 1444391003000 ms
15/10/09 06:43:23 INFO DStreamGraph: Updating checkpoint data for time 1444391003000 ms
15/10/09 06:43:23 INFO JobScheduler: Starting job streaming job 1444391003000 ms.0 from job set of time 1444391003000 ms
15/10/09 06:43:23 INFO DStreamGraph: Updated checkpoint data for time 1444391003000 ms
15/10/09 06:43:23 INFO CheckpointWriter: Saving checkpoint for time 1444391003000 ms to file 'file:/tmp/checkpoint-1444391003000'
15/10/09 06:43:23 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:23 INFO DAGScheduler: Job 15 finished: foreachRDD at Events.scala:18, took 0.000023 s
15/10/09 06:43:23 INFO JobScheduler: Finished job streaming job 1444391003000 ms.0 from job set of time 1444391003000 ms
15/10/09 06:43:23 INFO JobScheduler: Total delay: 0.010 s for time 1444391003000 ms (execution: 0.005 s)
15/10/09 06:43:23 INFO MapPartitionsRDD: Removing RDD 28 from persistence list
15/10/09 06:43:23 INFO BlockManager: Removing RDD 28
15/10/09 06:43:23 INFO WriteAheadLogBackedBlockRDD: Removing RDD 27 from persistence list
15/10/09 06:43:23 INFO BlockManager: Removing RDD 27
15/10/09 06:43:23 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[27] at createStream at Events.scala:13 of time 1444391003000 ms
15/10/09 06:43:23 INFO JobGenerator: Checkpointing graph for time 1444391003000 ms
15/10/09 06:43:23 INFO DStreamGraph: Updating checkpoint data for time 1444391003000 ms
15/10/09 06:43:23 INFO DStreamGraph: Updated checkpoint data for time 1444391003000 ms
15/10/09 06:43:23 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390998000.bk
15/10/09 06:43:23 INFO CheckpointWriter: Checkpoint for time 1444391003000 ms saved to file 'file:/tmp/checkpoint-1444391003000', took 3619 bytes and 10 ms
15/10/09 06:43:23 INFO CheckpointWriter: Saving checkpoint for time 1444391003000 ms to file 'file:/tmp/checkpoint-1444391003000'
15/10/09 06:43:23 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390998000
15/10/09 06:43:23 INFO CheckpointWriter: Checkpoint for time 1444391003000 ms saved to file 'file:/tmp/checkpoint-1444391003000', took 3615 bytes and 7 ms
15/10/09 06:43:23 INFO DStreamGraph: Clearing checkpoint data for time 1444391003000 ms
15/10/09 06:43:23 INFO DStreamGraph: Cleared checkpoint data for time 1444391003000 ms
15/10/09 06:43:23 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391001000 ms)
15/10/09 06:43:23 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391002000: 
15/10/09 06:43:23 INFO ReceiverTracker: Cleanup old received batch data: 1444391002000 ms
15/10/09 06:43:23 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391002000
15/10/09 06:43:23 INFO InputInfoTracker: remove old batch metadata: 1444391001000 ms
15/10/09 06:43:23 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391002000: 
15/10/09 06:43:23 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391002000
15/10/09 06:43:24 INFO JobScheduler: Added jobs for time 1444391004000 ms
15/10/09 06:43:24 INFO JobGenerator: Checkpointing graph for time 1444391004000 ms
15/10/09 06:43:24 INFO JobScheduler: Starting job streaming job 1444391004000 ms.0 from job set of time 1444391004000 ms
15/10/09 06:43:24 INFO DStreamGraph: Updating checkpoint data for time 1444391004000 ms
15/10/09 06:43:24 INFO DStreamGraph: Updated checkpoint data for time 1444391004000 ms
15/10/09 06:43:24 INFO CheckpointWriter: Saving checkpoint for time 1444391004000 ms to file 'file:/tmp/checkpoint-1444391004000'
15/10/09 06:43:24 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:24 INFO DAGScheduler: Job 16 finished: foreachRDD at Events.scala:18, took 0.000030 s
15/10/09 06:43:24 INFO JobScheduler: Finished job streaming job 1444391004000 ms.0 from job set of time 1444391004000 ms
15/10/09 06:43:24 INFO JobScheduler: Total delay: 0.012 s for time 1444391004000 ms (execution: 0.005 s)
15/10/09 06:43:24 INFO MapPartitionsRDD: Removing RDD 30 from persistence list
15/10/09 06:43:24 INFO WriteAheadLogBackedBlockRDD: Removing RDD 29 from persistence list
15/10/09 06:43:24 INFO BlockManager: Removing RDD 30
15/10/09 06:43:24 INFO BlockManager: Removing RDD 29
15/10/09 06:43:24 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[29] at createStream at Events.scala:13 of time 1444391004000 ms
15/10/09 06:43:24 INFO JobGenerator: Checkpointing graph for time 1444391004000 ms
15/10/09 06:43:24 INFO DStreamGraph: Updating checkpoint data for time 1444391004000 ms
15/10/09 06:43:24 INFO DStreamGraph: Updated checkpoint data for time 1444391004000 ms
15/10/09 06:43:24 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390999000.bk
15/10/09 06:43:24 INFO CheckpointWriter: Checkpoint for time 1444391004000 ms saved to file 'file:/tmp/checkpoint-1444391004000', took 3619 bytes and 11 ms
15/10/09 06:43:24 INFO CheckpointWriter: Saving checkpoint for time 1444391004000 ms to file 'file:/tmp/checkpoint-1444391004000'
15/10/09 06:43:24 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444390999000
15/10/09 06:43:24 INFO CheckpointWriter: Checkpoint for time 1444391004000 ms saved to file 'file:/tmp/checkpoint-1444391004000', took 3615 bytes and 7 ms
15/10/09 06:43:24 INFO DStreamGraph: Clearing checkpoint data for time 1444391004000 ms
15/10/09 06:43:24 INFO DStreamGraph: Cleared checkpoint data for time 1444391004000 ms
15/10/09 06:43:24 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391002000 ms)
15/10/09 06:43:24 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391003000: 
15/10/09 06:43:24 INFO ReceiverTracker: Cleanup old received batch data: 1444391003000 ms
15/10/09 06:43:24 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391003000
15/10/09 06:43:24 INFO InputInfoTracker: remove old batch metadata: 1444391002000 ms
15/10/09 06:43:24 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391003000: 
15/10/09 06:43:24 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391003000
15/10/09 06:43:25 INFO JobScheduler: Added jobs for time 1444391005000 ms
15/10/09 06:43:25 INFO JobGenerator: Checkpointing graph for time 1444391005000 ms
15/10/09 06:43:25 INFO DStreamGraph: Updating checkpoint data for time 1444391005000 ms
15/10/09 06:43:25 INFO DStreamGraph: Updated checkpoint data for time 1444391005000 ms
15/10/09 06:43:25 INFO JobScheduler: Starting job streaming job 1444391005000 ms.0 from job set of time 1444391005000 ms
15/10/09 06:43:25 INFO CheckpointWriter: Saving checkpoint for time 1444391005000 ms to file 'file:/tmp/checkpoint-1444391005000'
15/10/09 06:43:25 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:25 INFO DAGScheduler: Job 17 finished: foreachRDD at Events.scala:18, took 0.000026 s
15/10/09 06:43:25 INFO JobScheduler: Finished job streaming job 1444391005000 ms.0 from job set of time 1444391005000 ms
15/10/09 06:43:25 INFO JobScheduler: Total delay: 0.007 s for time 1444391005000 ms (execution: 0.003 s)
15/10/09 06:43:25 INFO MapPartitionsRDD: Removing RDD 32 from persistence list
15/10/09 06:43:25 INFO WriteAheadLogBackedBlockRDD: Removing RDD 31 from persistence list
15/10/09 06:43:25 INFO BlockManager: Removing RDD 32
15/10/09 06:43:25 INFO BlockManager: Removing RDD 31
15/10/09 06:43:25 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[31] at createStream at Events.scala:13 of time 1444391005000 ms
15/10/09 06:43:25 INFO JobGenerator: Checkpointing graph for time 1444391005000 ms
15/10/09 06:43:25 INFO DStreamGraph: Updating checkpoint data for time 1444391005000 ms
15/10/09 06:43:25 INFO DStreamGraph: Updated checkpoint data for time 1444391005000 ms
15/10/09 06:43:25 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391000000.bk
15/10/09 06:43:25 INFO CheckpointWriter: Checkpoint for time 1444391005000 ms saved to file 'file:/tmp/checkpoint-1444391005000', took 3619 bytes and 10 ms
15/10/09 06:43:25 INFO CheckpointWriter: Saving checkpoint for time 1444391005000 ms to file 'file:/tmp/checkpoint-1444391005000'
15/10/09 06:43:25 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391000000
15/10/09 06:43:25 INFO CheckpointWriter: Checkpoint for time 1444391005000 ms saved to file 'file:/tmp/checkpoint-1444391005000', took 3615 bytes and 8 ms
15/10/09 06:43:25 INFO DStreamGraph: Clearing checkpoint data for time 1444391005000 ms
15/10/09 06:43:25 INFO DStreamGraph: Cleared checkpoint data for time 1444391005000 ms
15/10/09 06:43:25 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391003000 ms)
15/10/09 06:43:25 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391004000: 
15/10/09 06:43:25 INFO ReceiverTracker: Cleanup old received batch data: 1444391004000 ms
15/10/09 06:43:25 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391004000
15/10/09 06:43:25 INFO InputInfoTracker: remove old batch metadata: 1444391003000 ms
15/10/09 06:43:25 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391004000: 
15/10/09 06:43:25 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391004000
15/10/09 06:43:26 INFO JobScheduler: Added jobs for time 1444391006000 ms
15/10/09 06:43:26 INFO JobGenerator: Checkpointing graph for time 1444391006000 ms
15/10/09 06:43:26 INFO DStreamGraph: Updating checkpoint data for time 1444391006000 ms
15/10/09 06:43:26 INFO JobScheduler: Starting job streaming job 1444391006000 ms.0 from job set of time 1444391006000 ms
15/10/09 06:43:26 INFO DStreamGraph: Updated checkpoint data for time 1444391006000 ms
15/10/09 06:43:26 INFO CheckpointWriter: Saving checkpoint for time 1444391006000 ms to file 'file:/tmp/checkpoint-1444391006000'
15/10/09 06:43:26 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:26 INFO DAGScheduler: Job 18 finished: foreachRDD at Events.scala:18, took 0.000026 s
15/10/09 06:43:26 INFO JobScheduler: Finished job streaming job 1444391006000 ms.0 from job set of time 1444391006000 ms
15/10/09 06:43:26 INFO JobScheduler: Total delay: 0.011 s for time 1444391006000 ms (execution: 0.003 s)
15/10/09 06:43:26 INFO MapPartitionsRDD: Removing RDD 34 from persistence list
15/10/09 06:43:26 INFO WriteAheadLogBackedBlockRDD: Removing RDD 33 from persistence list
15/10/09 06:43:26 INFO BlockManager: Removing RDD 34
15/10/09 06:43:26 INFO BlockManager: Removing RDD 33
15/10/09 06:43:26 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[33] at createStream at Events.scala:13 of time 1444391006000 ms
15/10/09 06:43:26 INFO JobGenerator: Checkpointing graph for time 1444391006000 ms
15/10/09 06:43:26 INFO DStreamGraph: Updating checkpoint data for time 1444391006000 ms
15/10/09 06:43:26 INFO DStreamGraph: Updated checkpoint data for time 1444391006000 ms
15/10/09 06:43:26 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391001000.bk
15/10/09 06:43:26 INFO CheckpointWriter: Checkpoint for time 1444391006000 ms saved to file 'file:/tmp/checkpoint-1444391006000', took 3619 bytes and 9 ms
15/10/09 06:43:26 INFO CheckpointWriter: Saving checkpoint for time 1444391006000 ms to file 'file:/tmp/checkpoint-1444391006000'
15/10/09 06:43:26 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391001000
15/10/09 06:43:26 INFO CheckpointWriter: Checkpoint for time 1444391006000 ms saved to file 'file:/tmp/checkpoint-1444391006000', took 3615 bytes and 7 ms
15/10/09 06:43:26 INFO DStreamGraph: Clearing checkpoint data for time 1444391006000 ms
15/10/09 06:43:26 INFO DStreamGraph: Cleared checkpoint data for time 1444391006000 ms
15/10/09 06:43:26 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391004000 ms)
15/10/09 06:43:26 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391005000: 
15/10/09 06:43:26 INFO ReceiverTracker: Cleanup old received batch data: 1444391005000 ms
15/10/09 06:43:26 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391005000
15/10/09 06:43:26 INFO InputInfoTracker: remove old batch metadata: 1444391004000 ms
15/10/09 06:43:26 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391005000: 
15/10/09 06:43:26 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391005000
15/10/09 06:43:27 INFO JobScheduler: Added jobs for time 1444391007000 ms
15/10/09 06:43:27 INFO JobGenerator: Checkpointing graph for time 1444391007000 ms
15/10/09 06:43:27 INFO DStreamGraph: Updating checkpoint data for time 1444391007000 ms
15/10/09 06:43:27 INFO JobScheduler: Starting job streaming job 1444391007000 ms.0 from job set of time 1444391007000 ms
15/10/09 06:43:27 INFO DStreamGraph: Updated checkpoint data for time 1444391007000 ms
15/10/09 06:43:27 INFO CheckpointWriter: Saving checkpoint for time 1444391007000 ms to file 'file:/tmp/checkpoint-1444391007000'
15/10/09 06:43:27 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:27 INFO DAGScheduler: Job 19 finished: foreachRDD at Events.scala:18, took 0.000023 s
15/10/09 06:43:27 INFO JobScheduler: Finished job streaming job 1444391007000 ms.0 from job set of time 1444391007000 ms
15/10/09 06:43:27 INFO JobScheduler: Total delay: 0.011 s for time 1444391007000 ms (execution: 0.003 s)
15/10/09 06:43:27 INFO MapPartitionsRDD: Removing RDD 36 from persistence list
15/10/09 06:43:27 INFO WriteAheadLogBackedBlockRDD: Removing RDD 35 from persistence list
15/10/09 06:43:27 INFO BlockManager: Removing RDD 36
15/10/09 06:43:27 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[35] at createStream at Events.scala:13 of time 1444391007000 ms
15/10/09 06:43:27 INFO BlockManager: Removing RDD 35
15/10/09 06:43:27 INFO JobGenerator: Checkpointing graph for time 1444391007000 ms
15/10/09 06:43:27 INFO DStreamGraph: Updating checkpoint data for time 1444391007000 ms
15/10/09 06:43:27 INFO DStreamGraph: Updated checkpoint data for time 1444391007000 ms
15/10/09 06:43:27 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391002000.bk
15/10/09 06:43:27 INFO CheckpointWriter: Checkpoint for time 1444391007000 ms saved to file 'file:/tmp/checkpoint-1444391007000', took 3619 bytes and 9 ms
15/10/09 06:43:27 INFO CheckpointWriter: Saving checkpoint for time 1444391007000 ms to file 'file:/tmp/checkpoint-1444391007000'
15/10/09 06:43:27 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391002000
15/10/09 06:43:27 INFO CheckpointWriter: Checkpoint for time 1444391007000 ms saved to file 'file:/tmp/checkpoint-1444391007000', took 3615 bytes and 6 ms
15/10/09 06:43:27 INFO DStreamGraph: Clearing checkpoint data for time 1444391007000 ms
15/10/09 06:43:27 INFO DStreamGraph: Cleared checkpoint data for time 1444391007000 ms
15/10/09 06:43:27 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391005000 ms)
15/10/09 06:43:27 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391006000: 
15/10/09 06:43:27 INFO ReceiverTracker: Cleanup old received batch data: 1444391006000 ms
15/10/09 06:43:27 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391006000
15/10/09 06:43:27 INFO InputInfoTracker: remove old batch metadata: 1444391005000 ms
15/10/09 06:43:27 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391006000: 
15/10/09 06:43:27 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391006000
15/10/09 06:43:28 INFO JobScheduler: Added jobs for time 1444391008000 ms
15/10/09 06:43:28 INFO JobGenerator: Checkpointing graph for time 1444391008000 ms
15/10/09 06:43:28 INFO DStreamGraph: Updating checkpoint data for time 1444391008000 ms
15/10/09 06:43:28 INFO DStreamGraph: Updated checkpoint data for time 1444391008000 ms
15/10/09 06:43:28 INFO JobScheduler: Starting job streaming job 1444391008000 ms.0 from job set of time 1444391008000 ms
15/10/09 06:43:28 INFO CheckpointWriter: Saving checkpoint for time 1444391008000 ms to file 'file:/tmp/checkpoint-1444391008000'
15/10/09 06:43:28 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:28 INFO DAGScheduler: Job 20 finished: foreachRDD at Events.scala:18, took 0.000022 s
15/10/09 06:43:28 INFO JobScheduler: Finished job streaming job 1444391008000 ms.0 from job set of time 1444391008000 ms
15/10/09 06:43:28 INFO JobScheduler: Total delay: 0.009 s for time 1444391008000 ms (execution: 0.004 s)
15/10/09 06:43:28 INFO MapPartitionsRDD: Removing RDD 38 from persistence list
15/10/09 06:43:28 INFO WriteAheadLogBackedBlockRDD: Removing RDD 37 from persistence list
15/10/09 06:43:28 INFO BlockManager: Removing RDD 38
15/10/09 06:43:28 INFO BlockManager: Removing RDD 37
15/10/09 06:43:28 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[37] at createStream at Events.scala:13 of time 1444391008000 ms
15/10/09 06:43:28 INFO JobGenerator: Checkpointing graph for time 1444391008000 ms
15/10/09 06:43:28 INFO DStreamGraph: Updating checkpoint data for time 1444391008000 ms
15/10/09 06:43:28 INFO DStreamGraph: Updated checkpoint data for time 1444391008000 ms
15/10/09 06:43:28 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391003000.bk
15/10/09 06:43:28 INFO CheckpointWriter: Checkpoint for time 1444391008000 ms saved to file 'file:/tmp/checkpoint-1444391008000', took 3619 bytes and 10 ms
15/10/09 06:43:28 INFO CheckpointWriter: Saving checkpoint for time 1444391008000 ms to file 'file:/tmp/checkpoint-1444391008000'
15/10/09 06:43:28 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391003000
15/10/09 06:43:28 INFO CheckpointWriter: Checkpoint for time 1444391008000 ms saved to file 'file:/tmp/checkpoint-1444391008000', took 3615 bytes and 7 ms
15/10/09 06:43:28 INFO DStreamGraph: Clearing checkpoint data for time 1444391008000 ms
15/10/09 06:43:28 INFO DStreamGraph: Cleared checkpoint data for time 1444391008000 ms
15/10/09 06:43:28 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391006000 ms)
15/10/09 06:43:28 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391007000: 
15/10/09 06:43:28 INFO ReceiverTracker: Cleanup old received batch data: 1444391007000 ms
15/10/09 06:43:28 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391007000
15/10/09 06:43:28 INFO InputInfoTracker: remove old batch metadata: 1444391006000 ms
15/10/09 06:43:28 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391007000: 
15/10/09 06:43:28 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391007000
15/10/09 06:43:28 INFO MemoryStore: ensureFreeSpace(74) called with curMem=166543, maxMem=555755765
15/10/09 06:43:28 INFO MemoryStore: Block input-0-1444390989312 stored as bytes in memory (estimated size 74.0 B, free 529.9 MB)
15/10/09 06:43:28 INFO BlockManagerInfo: Added input-0-1444390989312 in memory on localhost:55619 (size: 74.0 B, free: 530.0 MB)
15/10/09 06:43:28 INFO ReliableKafkaReceiver: Committed offset 422 for topic events, partition 0
15/10/09 06:43:28 INFO BlockGenerator: Pushed block input-0-1444391008000
15/10/09 06:43:29 INFO JobScheduler: Added jobs for time 1444391009000 ms
15/10/09 06:43:29 INFO JobGenerator: Checkpointing graph for time 1444391009000 ms
15/10/09 06:43:29 INFO DStreamGraph: Updating checkpoint data for time 1444391009000 ms
15/10/09 06:43:29 INFO JobScheduler: Starting job streaming job 1444391009000 ms.0 from job set of time 1444391009000 ms
15/10/09 06:43:29 INFO DStreamGraph: Updated checkpoint data for time 1444391009000 ms
15/10/09 06:43:29 INFO CheckpointWriter: Saving checkpoint for time 1444391009000 ms to file 'file:/tmp/checkpoint-1444391009000'
15/10/09 06:43:29 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:29 INFO DAGScheduler: Got job 21 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:29 INFO DAGScheduler: Final stage: ResultStage 2(foreachRDD at Events.scala:18)
15/10/09 06:43:29 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:29 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:29 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[42] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:29 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391004000.bk
15/10/09 06:43:29 INFO CheckpointWriter: Checkpoint for time 1444391009000 ms saved to file 'file:/tmp/checkpoint-1444391009000', took 3619 bytes and 10 ms
15/10/09 06:43:29 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=166617, maxMem=555755765
15/10/09 06:43:29 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 61.1 KB, free 529.8 MB)
15/10/09 06:43:29 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=229225, maxMem=555755765
15/10/09 06:43:29 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.8 MB)
15/10/09 06:43:29 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 530.0 MB)
15/10/09 06:43:29 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:29 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[42] at map at Events.scala:13)
15/10/09 06:43:29 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/10/09 06:43:29 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:29 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
15/10/09 06:43:29 INFO BlockManager: Found block input-0-1444390989312 locally
15/10/09 06:43:29 ERROR Executor: Exception in task 0.0 in stage 2.0 (TID 2)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:43:29 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:43:29 ERROR TaskSetManager: Task 0 in stage 2.0 failed 1 times; aborting job
15/10/09 06:43:29 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/10/09 06:43:29 INFO TaskSchedulerImpl: Cancelling stage 2
15/10/09 06:43:29 INFO DAGScheduler: ResultStage 2 (foreachRDD at Events.scala:18) failed in 0.011 s
15/10/09 06:43:29 INFO DAGScheduler: Job 21 failed: foreachRDD at Events.scala:18, took 0.025481 s
15/10/09 06:43:29 INFO JobScheduler: Finished job streaming job 1444391009000 ms.0 from job set of time 1444391009000 ms
15/10/09 06:43:29 INFO MapPartitionsRDD: Removing RDD 40 from persistence list
15/10/09 06:43:29 INFO JobScheduler: Total delay: 0.033 s for time 1444391009000 ms (execution: 0.029 s)
15/10/09 06:43:29 ERROR JobScheduler: Error running job streaming job 1444391009000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:43:29 INFO WriteAheadLogBackedBlockRDD: Removing RDD 39 from persistence list
15/10/09 06:43:29 INFO BlockManager: Removing RDD 40
15/10/09 06:43:29 INFO BlockManager: Removing RDD 39
15/10/09 06:43:29 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[39] at createStream at Events.scala:13 of time 1444391009000 ms
15/10/09 06:43:29 INFO JobGenerator: Checkpointing graph for time 1444391009000 ms
15/10/09 06:43:29 INFO DStreamGraph: Updating checkpoint data for time 1444391009000 ms
15/10/09 06:43:29 INFO DStreamGraph: Updated checkpoint data for time 1444391009000 ms
15/10/09 06:43:29 INFO CheckpointWriter: Saving checkpoint for time 1444391009000 ms to file 'file:/tmp/checkpoint-1444391009000'
15/10/09 06:43:29 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391004000
15/10/09 06:43:29 INFO CheckpointWriter: Checkpoint for time 1444391009000 ms saved to file 'file:/tmp/checkpoint-1444391009000', took 3615 bytes and 8 ms
15/10/09 06:43:29 INFO DStreamGraph: Clearing checkpoint data for time 1444391009000 ms
15/10/09 06:43:29 INFO DStreamGraph: Cleared checkpoint data for time 1444391009000 ms
15/10/09 06:43:29 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391007000 ms)
15/10/09 06:43:29 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391008000: 
15/10/09 06:43:29 INFO ReceiverTracker: Cleanup old received batch data: 1444391008000 ms
15/10/09 06:43:29 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391008000
15/10/09 06:43:29 INFO InputInfoTracker: remove old batch metadata: 1444391007000 ms
15/10/09 06:43:29 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391008000: 
15/10/09 06:43:29 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391008000
15/10/09 06:43:29 INFO MemoryStore: ensureFreeSpace(74) called with curMem=250185, maxMem=555755765
15/10/09 06:43:29 INFO MemoryStore: Block input-0-1444390989313 stored as bytes in memory (estimated size 74.0 B, free 529.8 MB)
15/10/09 06:43:29 INFO BlockManagerInfo: Added input-0-1444390989313 in memory on localhost:55619 (size: 74.0 B, free: 530.0 MB)
15/10/09 06:43:29 INFO ReliableKafkaReceiver: Committed offset 423 for topic events, partition 0
15/10/09 06:43:29 INFO BlockGenerator: Pushed block input-0-1444391009000
15/10/09 06:43:30 INFO JobScheduler: Added jobs for time 1444391010000 ms
15/10/09 06:43:30 INFO JobGenerator: Checkpointing graph for time 1444391010000 ms
15/10/09 06:43:30 INFO DStreamGraph: Updating checkpoint data for time 1444391010000 ms
15/10/09 06:43:30 INFO DStreamGraph: Updated checkpoint data for time 1444391010000 ms
15/10/09 06:43:30 INFO JobScheduler: Starting job streaming job 1444391010000 ms.0 from job set of time 1444391010000 ms
15/10/09 06:43:30 INFO CheckpointWriter: Saving checkpoint for time 1444391010000 ms to file 'file:/tmp/checkpoint-1444391010000'
15/10/09 06:43:30 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:30 INFO DAGScheduler: Got job 22 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:30 INFO DAGScheduler: Final stage: ResultStage 3(foreachRDD at Events.scala:18)
15/10/09 06:43:30 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:30 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:30 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[44] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:30 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391005000.bk
15/10/09 06:43:30 INFO CheckpointWriter: Checkpoint for time 1444391010000 ms saved to file 'file:/tmp/checkpoint-1444391010000', took 3619 bytes and 9 ms
15/10/09 06:43:30 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=250259, maxMem=555755765
15/10/09 06:43:30 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 61.1 KB, free 529.7 MB)
15/10/09 06:43:30 INFO MemoryStore: ensureFreeSpace(20958) called with curMem=312867, maxMem=555755765
15/10/09 06:43:30 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.7 MB)
15/10/09 06:43:30 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:43:30 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:30 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[44] at map at Events.scala:13)
15/10/09 06:43:30 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks
15/10/09 06:43:30 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:30 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
15/10/09 06:43:30 INFO BlockManager: Found block input-0-1444390989313 locally
15/10/09 06:43:30 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 3)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:43:30 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 3, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:43:30 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job
15/10/09 06:43:30 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
15/10/09 06:43:30 INFO TaskSchedulerImpl: Cancelling stage 3
15/10/09 06:43:30 INFO DAGScheduler: ResultStage 3 (foreachRDD at Events.scala:18) failed in 0.009 s
15/10/09 06:43:30 INFO DAGScheduler: Job 22 failed: foreachRDD at Events.scala:18, took 0.023125 s
15/10/09 06:43:30 INFO JobScheduler: Finished job streaming job 1444391010000 ms.0 from job set of time 1444391010000 ms
15/10/09 06:43:30 INFO JobScheduler: Total delay: 0.031 s for time 1444391010000 ms (execution: 0.027 s)
15/10/09 06:43:30 INFO MapPartitionsRDD: Removing RDD 42 from persistence list
15/10/09 06:43:30 ERROR JobScheduler: Error running job streaming job 1444391010000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 3, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:43:30 INFO BlockManager: Removing RDD 42
15/10/09 06:43:30 INFO WriteAheadLogBackedBlockRDD: Removing RDD 41 from persistence list
15/10/09 06:43:30 INFO BlockManager: Removing RDD 41
15/10/09 06:43:30 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[41] at createStream at Events.scala:13 of time 1444391010000 ms
15/10/09 06:43:30 INFO JobGenerator: Checkpointing graph for time 1444391010000 ms
15/10/09 06:43:30 INFO DStreamGraph: Updating checkpoint data for time 1444391010000 ms
15/10/09 06:43:30 INFO DStreamGraph: Updated checkpoint data for time 1444391010000 ms
15/10/09 06:43:30 INFO BlockManagerInfo: Removed input-0-1444390989312 on localhost:55619 in memory (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:30 INFO CheckpointWriter: Saving checkpoint for time 1444391010000 ms to file 'file:/tmp/checkpoint-1444391010000'
15/10/09 06:43:30 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391005000
15/10/09 06:43:30 INFO CheckpointWriter: Checkpoint for time 1444391010000 ms saved to file 'file:/tmp/checkpoint-1444391010000', took 3615 bytes and 8 ms
15/10/09 06:43:30 INFO DStreamGraph: Clearing checkpoint data for time 1444391010000 ms
15/10/09 06:43:30 INFO DStreamGraph: Cleared checkpoint data for time 1444391010000 ms
15/10/09 06:43:30 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391008000 ms)
15/10/09 06:43:30 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391009000: 
15/10/09 06:43:30 INFO ReceiverTracker: Cleanup old received batch data: 1444391009000 ms
15/10/09 06:43:30 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391009000
15/10/09 06:43:30 INFO InputInfoTracker: remove old batch metadata: 1444391008000 ms
15/10/09 06:43:30 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391009000: 
15/10/09 06:43:30 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391009000
15/10/09 06:43:31 INFO JobScheduler: Added jobs for time 1444391011000 ms
15/10/09 06:43:31 INFO JobGenerator: Checkpointing graph for time 1444391011000 ms
15/10/09 06:43:31 INFO DStreamGraph: Updating checkpoint data for time 1444391011000 ms
15/10/09 06:43:31 INFO JobScheduler: Starting job streaming job 1444391011000 ms.0 from job set of time 1444391011000 ms
15/10/09 06:43:31 INFO DStreamGraph: Updated checkpoint data for time 1444391011000 ms
15/10/09 06:43:31 INFO CheckpointWriter: Saving checkpoint for time 1444391011000 ms to file 'file:/tmp/checkpoint-1444391011000'
15/10/09 06:43:31 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:31 INFO DAGScheduler: Job 23 finished: foreachRDD at Events.scala:18, took 0.000019 s
15/10/09 06:43:31 INFO JobScheduler: Finished job streaming job 1444391011000 ms.0 from job set of time 1444391011000 ms
15/10/09 06:43:31 INFO JobScheduler: Total delay: 0.005 s for time 1444391011000 ms (execution: 0.002 s)
15/10/09 06:43:31 INFO MapPartitionsRDD: Removing RDD 44 from persistence list
15/10/09 06:43:31 INFO WriteAheadLogBackedBlockRDD: Removing RDD 43 from persistence list
15/10/09 06:43:31 INFO BlockManager: Removing RDD 44
15/10/09 06:43:31 INFO BlockManager: Removing RDD 43
15/10/09 06:43:31 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[43] at createStream at Events.scala:13 of time 1444391011000 ms
15/10/09 06:43:31 INFO JobGenerator: Checkpointing graph for time 1444391011000 ms
15/10/09 06:43:31 INFO DStreamGraph: Updating checkpoint data for time 1444391011000 ms
15/10/09 06:43:31 INFO DStreamGraph: Updated checkpoint data for time 1444391011000 ms
15/10/09 06:43:31 INFO BlockManagerInfo: Removed input-0-1444390989313 on localhost:55619 in memory (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:31 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391006000.bk
15/10/09 06:43:31 INFO CheckpointWriter: Checkpoint for time 1444391011000 ms saved to file 'file:/tmp/checkpoint-1444391011000', took 3619 bytes and 8 ms
15/10/09 06:43:31 INFO CheckpointWriter: Saving checkpoint for time 1444391011000 ms to file 'file:/tmp/checkpoint-1444391011000'
15/10/09 06:43:31 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391006000
15/10/09 06:43:31 INFO CheckpointWriter: Checkpoint for time 1444391011000 ms saved to file 'file:/tmp/checkpoint-1444391011000', took 3615 bytes and 6 ms
15/10/09 06:43:31 INFO DStreamGraph: Clearing checkpoint data for time 1444391011000 ms
15/10/09 06:43:31 INFO DStreamGraph: Cleared checkpoint data for time 1444391011000 ms
15/10/09 06:43:31 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391009000 ms)
15/10/09 06:43:31 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391010000: 
15/10/09 06:43:31 INFO ReceiverTracker: Cleanup old received batch data: 1444391010000 ms
15/10/09 06:43:31 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391010000
15/10/09 06:43:31 INFO InputInfoTracker: remove old batch metadata: 1444391009000 ms
15/10/09 06:43:31 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391010000: 
15/10/09 06:43:31 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391010000
15/10/09 06:43:31 INFO MemoryStore: ensureFreeSpace(74) called with curMem=333677, maxMem=555755765
15/10/09 06:43:31 INFO MemoryStore: Block input-0-1444390989314 stored as bytes in memory (estimated size 74.0 B, free 529.7 MB)
15/10/09 06:43:31 INFO BlockManagerInfo: Added input-0-1444390989314 in memory on localhost:55619 (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:31 INFO ReliableKafkaReceiver: Committed offset 424 for topic events, partition 0
15/10/09 06:43:31 INFO BlockGenerator: Pushed block input-0-1444391011000
15/10/09 06:43:32 INFO JobScheduler: Added jobs for time 1444391012000 ms
15/10/09 06:43:32 INFO JobGenerator: Checkpointing graph for time 1444391012000 ms
15/10/09 06:43:32 INFO DStreamGraph: Updating checkpoint data for time 1444391012000 ms
15/10/09 06:43:32 INFO JobScheduler: Starting job streaming job 1444391012000 ms.0 from job set of time 1444391012000 ms
15/10/09 06:43:32 INFO DStreamGraph: Updated checkpoint data for time 1444391012000 ms
15/10/09 06:43:32 INFO CheckpointWriter: Saving checkpoint for time 1444391012000 ms to file 'file:/tmp/checkpoint-1444391012000'
15/10/09 06:43:32 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:32 INFO DAGScheduler: Got job 24 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:32 INFO DAGScheduler: Final stage: ResultStage 4(foreachRDD at Events.scala:18)
15/10/09 06:43:32 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:32 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:32 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[48] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:32 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391007000.bk
15/10/09 06:43:32 INFO CheckpointWriter: Checkpoint for time 1444391012000 ms saved to file 'file:/tmp/checkpoint-1444391012000', took 3619 bytes and 10 ms
15/10/09 06:43:32 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=333751, maxMem=555755765
15/10/09 06:43:32 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 61.1 KB, free 529.6 MB)
15/10/09 06:43:32 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=396359, maxMem=555755765
15/10/09 06:43:32 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.6 MB)
15/10/09 06:43:32 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:43:32 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[48] at map at Events.scala:13)
15/10/09 06:43:32 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
15/10/09 06:43:32 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:32 INFO Executor: Running task 0.0 in stage 4.0 (TID 4)
15/10/09 06:43:32 INFO BlockManager: Found block input-0-1444390989314 locally
15/10/09 06:43:32 INFO Executor: Finished task 0.0 in stage 4.0 (TID 4). 915 bytes result sent to driver
15/10/09 06:43:32 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 13 ms on localhost (1/1)
15/10/09 06:43:32 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
15/10/09 06:43:32 INFO DAGScheduler: ResultStage 4 (foreachRDD at Events.scala:18) finished in 0.014 s
15/10/09 06:43:32 INFO DAGScheduler: Job 24 finished: foreachRDD at Events.scala:18, took 0.029268 s
15/10/09 06:43:32 INFO JobScheduler: Finished job streaming job 1444391012000 ms.0 from job set of time 1444391012000 ms
15/10/09 06:43:32 INFO JobScheduler: Total delay: 0.035 s for time 1444391012000 ms (execution: 0.033 s)
15/10/09 06:43:32 INFO MapPartitionsRDD: Removing RDD 46 from persistence list
15/10/09 06:43:32 INFO BlockManager: Removing RDD 46
15/10/09 06:43:32 INFO WriteAheadLogBackedBlockRDD: Removing RDD 45 from persistence list
15/10/09 06:43:32 INFO BlockManager: Removing RDD 45
15/10/09 06:43:32 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[45] at createStream at Events.scala:13 of time 1444391012000 ms
15/10/09 06:43:32 INFO JobGenerator: Checkpointing graph for time 1444391012000 ms
15/10/09 06:43:32 INFO DStreamGraph: Updating checkpoint data for time 1444391012000 ms
15/10/09 06:43:32 INFO DStreamGraph: Updated checkpoint data for time 1444391012000 ms
15/10/09 06:43:32 INFO CheckpointWriter: Saving checkpoint for time 1444391012000 ms to file 'file:/tmp/checkpoint-1444391012000'
15/10/09 06:43:32 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391007000
15/10/09 06:43:32 INFO CheckpointWriter: Checkpoint for time 1444391012000 ms saved to file 'file:/tmp/checkpoint-1444391012000', took 3615 bytes and 7 ms
15/10/09 06:43:32 INFO DStreamGraph: Clearing checkpoint data for time 1444391012000 ms
15/10/09 06:43:32 INFO DStreamGraph: Cleared checkpoint data for time 1444391012000 ms
15/10/09 06:43:32 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391010000 ms)
15/10/09 06:43:32 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391011000: 
15/10/09 06:43:32 INFO ReceiverTracker: Cleanup old received batch data: 1444391011000 ms
15/10/09 06:43:32 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391011000
15/10/09 06:43:32 INFO InputInfoTracker: remove old batch metadata: 1444391010000 ms
15/10/09 06:43:32 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391011000: 
15/10/09 06:43:32 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391011000
15/10/09 06:43:33 INFO JobScheduler: Added jobs for time 1444391013000 ms
15/10/09 06:43:33 INFO JobGenerator: Checkpointing graph for time 1444391013000 ms
15/10/09 06:43:33 INFO DStreamGraph: Updating checkpoint data for time 1444391013000 ms
15/10/09 06:43:33 INFO DStreamGraph: Updated checkpoint data for time 1444391013000 ms
15/10/09 06:43:33 INFO JobScheduler: Starting job streaming job 1444391013000 ms.0 from job set of time 1444391013000 ms
15/10/09 06:43:33 INFO CheckpointWriter: Saving checkpoint for time 1444391013000 ms to file 'file:/tmp/checkpoint-1444391013000'
15/10/09 06:43:33 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:33 INFO DAGScheduler: Job 25 finished: foreachRDD at Events.scala:18, took 0.000024 s
15/10/09 06:43:33 INFO JobScheduler: Finished job streaming job 1444391013000 ms.0 from job set of time 1444391013000 ms
15/10/09 06:43:33 INFO JobScheduler: Total delay: 0.011 s for time 1444391013000 ms (execution: 0.004 s)
15/10/09 06:43:33 INFO MapPartitionsRDD: Removing RDD 48 from persistence list
15/10/09 06:43:33 INFO BlockManager: Removing RDD 48
15/10/09 06:43:33 INFO WriteAheadLogBackedBlockRDD: Removing RDD 47 from persistence list
15/10/09 06:43:33 INFO BlockManager: Removing RDD 47
15/10/09 06:43:33 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[47] at createStream at Events.scala:13 of time 1444391013000 ms
15/10/09 06:43:33 INFO JobGenerator: Checkpointing graph for time 1444391013000 ms
15/10/09 06:43:33 INFO DStreamGraph: Updating checkpoint data for time 1444391013000 ms
15/10/09 06:43:33 INFO DStreamGraph: Updated checkpoint data for time 1444391013000 ms
15/10/09 06:43:33 INFO BlockManagerInfo: Removed input-0-1444390989314 on localhost:55619 in memory (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:33 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391008000.bk
15/10/09 06:43:33 INFO CheckpointWriter: Checkpoint for time 1444391013000 ms saved to file 'file:/tmp/checkpoint-1444391013000', took 3619 bytes and 10 ms
15/10/09 06:43:33 INFO CheckpointWriter: Saving checkpoint for time 1444391013000 ms to file 'file:/tmp/checkpoint-1444391013000'
15/10/09 06:43:33 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391008000
15/10/09 06:43:33 INFO CheckpointWriter: Checkpoint for time 1444391013000 ms saved to file 'file:/tmp/checkpoint-1444391013000', took 3615 bytes and 6 ms
15/10/09 06:43:33 INFO DStreamGraph: Clearing checkpoint data for time 1444391013000 ms
15/10/09 06:43:33 INFO DStreamGraph: Cleared checkpoint data for time 1444391013000 ms
15/10/09 06:43:33 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391011000 ms)
15/10/09 06:43:33 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391012000: 
15/10/09 06:43:33 INFO ReceiverTracker: Cleanup old received batch data: 1444391012000 ms
15/10/09 06:43:33 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391012000
15/10/09 06:43:33 INFO InputInfoTracker: remove old batch metadata: 1444391011000 ms
15/10/09 06:43:33 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391012000: 
15/10/09 06:43:33 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391012000
15/10/09 06:43:33 INFO MemoryStore: ensureFreeSpace(74) called with curMem=417245, maxMem=555755765
15/10/09 06:43:33 INFO MemoryStore: Block input-0-1444390989315 stored as bytes in memory (estimated size 74.0 B, free 529.6 MB)
15/10/09 06:43:33 INFO BlockManagerInfo: Added input-0-1444390989315 in memory on localhost:55619 (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:33 INFO ReliableKafkaReceiver: Committed offset 425 for topic events, partition 0
15/10/09 06:43:33 INFO BlockGenerator: Pushed block input-0-1444391013000
15/10/09 06:43:34 INFO JobScheduler: Added jobs for time 1444391014000 ms
15/10/09 06:43:34 INFO JobGenerator: Checkpointing graph for time 1444391014000 ms
15/10/09 06:43:34 INFO DStreamGraph: Updating checkpoint data for time 1444391014000 ms
15/10/09 06:43:34 INFO DStreamGraph: Updated checkpoint data for time 1444391014000 ms
15/10/09 06:43:34 INFO JobScheduler: Starting job streaming job 1444391014000 ms.0 from job set of time 1444391014000 ms
15/10/09 06:43:34 INFO CheckpointWriter: Saving checkpoint for time 1444391014000 ms to file 'file:/tmp/checkpoint-1444391014000'
15/10/09 06:43:34 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:34 INFO DAGScheduler: Got job 26 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:34 INFO DAGScheduler: Final stage: ResultStage 5(foreachRDD at Events.scala:18)
15/10/09 06:43:34 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:34 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:34 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[52] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:34 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391009000.bk
15/10/09 06:43:34 INFO CheckpointWriter: Checkpoint for time 1444391014000 ms saved to file 'file:/tmp/checkpoint-1444391014000', took 3619 bytes and 9 ms
15/10/09 06:43:34 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=417319, maxMem=555755765
15/10/09 06:43:34 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 61.1 KB, free 529.6 MB)
15/10/09 06:43:34 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=479927, maxMem=555755765
15/10/09 06:43:34 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.5 MB)
15/10/09 06:43:34 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:43:34 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[52] at map at Events.scala:13)
15/10/09 06:43:34 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks
15/10/09 06:43:34 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:34 INFO Executor: Running task 0.0 in stage 5.0 (TID 5)
15/10/09 06:43:34 INFO BlockManager: Found block input-0-1444390989315 locally
15/10/09 06:43:34 ERROR Executor: Exception in task 0.0 in stage 5.0 (TID 5)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:43:34 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:43:34 ERROR TaskSetManager: Task 0 in stage 5.0 failed 1 times; aborting job
15/10/09 06:43:34 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
15/10/09 06:43:34 INFO TaskSchedulerImpl: Cancelling stage 5
15/10/09 06:43:34 INFO DAGScheduler: ResultStage 5 (foreachRDD at Events.scala:18) failed in 0.012 s
15/10/09 06:43:34 INFO DAGScheduler: Job 26 failed: foreachRDD at Events.scala:18, took 0.036221 s
15/10/09 06:43:34 INFO JobScheduler: Finished job streaming job 1444391014000 ms.0 from job set of time 1444391014000 ms
15/10/09 06:43:34 INFO JobScheduler: Total delay: 0.046 s for time 1444391014000 ms (execution: 0.040 s)
15/10/09 06:43:34 INFO MapPartitionsRDD: Removing RDD 50 from persistence list
15/10/09 06:43:34 ERROR JobScheduler: Error running job streaming job 1444391014000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 1 times, most recent failure: Lost task 0.0 in stage 5.0 (TID 5, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:43:34 INFO BlockManager: Removing RDD 50
15/10/09 06:43:34 INFO WriteAheadLogBackedBlockRDD: Removing RDD 49 from persistence list
15/10/09 06:43:34 INFO BlockManager: Removing RDD 49
15/10/09 06:43:34 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[49] at createStream at Events.scala:13 of time 1444391014000 ms
15/10/09 06:43:34 INFO JobGenerator: Checkpointing graph for time 1444391014000 ms
15/10/09 06:43:34 INFO DStreamGraph: Updating checkpoint data for time 1444391014000 ms
15/10/09 06:43:34 INFO DStreamGraph: Updated checkpoint data for time 1444391014000 ms
15/10/09 06:43:34 INFO CheckpointWriter: Saving checkpoint for time 1444391014000 ms to file 'file:/tmp/checkpoint-1444391014000'
15/10/09 06:43:34 INFO ContextCleaner: Cleaned accumulator 5
15/10/09 06:43:34 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391009000
15/10/09 06:43:34 INFO CheckpointWriter: Checkpoint for time 1444391014000 ms saved to file 'file:/tmp/checkpoint-1444391014000', took 3615 bytes and 8 ms
15/10/09 06:43:34 INFO DStreamGraph: Clearing checkpoint data for time 1444391014000 ms
15/10/09 06:43:34 INFO DStreamGraph: Cleared checkpoint data for time 1444391014000 ms
15/10/09 06:43:34 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391012000 ms)
15/10/09 06:43:34 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391013000: 
15/10/09 06:43:34 INFO ReceiverTracker: Cleanup old received batch data: 1444391013000 ms
15/10/09 06:43:34 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391013000
15/10/09 06:43:34 INFO InputInfoTracker: remove old batch metadata: 1444391012000 ms
15/10/09 06:43:34 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391013000: 
15/10/09 06:43:34 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391013000
15/10/09 06:43:34 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:43:34 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:43:34 INFO ContextCleaner: Cleaned accumulator 4
15/10/09 06:43:34 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 530.0 MB)
15/10/09 06:43:34 INFO ContextCleaner: Cleaned accumulator 3
15/10/09 06:43:34 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 530.0 MB)
15/10/09 06:43:34 INFO ContextCleaner: Cleaned accumulator 2
15/10/09 06:43:34 INFO MemoryStore: ensureFreeSpace(74) called with curMem=166617, maxMem=555755765
15/10/09 06:43:34 INFO MemoryStore: Block input-0-1444390989316 stored as bytes in memory (estimated size 74.0 B, free 529.9 MB)
15/10/09 06:43:34 INFO BlockManagerInfo: Added input-0-1444390989316 in memory on localhost:55619 (size: 74.0 B, free: 530.0 MB)
15/10/09 06:43:34 INFO ReliableKafkaReceiver: Committed offset 426 for topic events, partition 0
15/10/09 06:43:34 INFO BlockGenerator: Pushed block input-0-1444391014000
15/10/09 06:43:35 INFO JobScheduler: Added jobs for time 1444391015000 ms
15/10/09 06:43:35 INFO JobGenerator: Checkpointing graph for time 1444391015000 ms
15/10/09 06:43:35 INFO DStreamGraph: Updating checkpoint data for time 1444391015000 ms
15/10/09 06:43:35 INFO DStreamGraph: Updated checkpoint data for time 1444391015000 ms
15/10/09 06:43:35 INFO JobScheduler: Starting job streaming job 1444391015000 ms.0 from job set of time 1444391015000 ms
15/10/09 06:43:35 INFO CheckpointWriter: Saving checkpoint for time 1444391015000 ms to file 'file:/tmp/checkpoint-1444391015000'
15/10/09 06:43:35 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:35 INFO DAGScheduler: Got job 27 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:35 INFO DAGScheduler: Final stage: ResultStage 6(foreachRDD at Events.scala:18)
15/10/09 06:43:35 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:35 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:35 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[54] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:35 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391010000.bk
15/10/09 06:43:35 INFO CheckpointWriter: Checkpoint for time 1444391015000 ms saved to file 'file:/tmp/checkpoint-1444391015000', took 3619 bytes and 9 ms
15/10/09 06:43:35 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=166691, maxMem=555755765
15/10/09 06:43:35 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 61.1 KB, free 529.8 MB)
15/10/09 06:43:35 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=229299, maxMem=555755765
15/10/09 06:43:35 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.8 MB)
15/10/09 06:43:35 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 530.0 MB)
15/10/09 06:43:35 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[54] at map at Events.scala:13)
15/10/09 06:43:35 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
15/10/09 06:43:35 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:35 INFO Executor: Running task 0.0 in stage 6.0 (TID 6)
15/10/09 06:43:35 INFO BlockManager: Found block input-0-1444390989316 locally
15/10/09 06:43:35 INFO Executor: Finished task 0.0 in stage 6.0 (TID 6). 915 bytes result sent to driver
15/10/09 06:43:35 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 5 ms on localhost (1/1)
15/10/09 06:43:35 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
15/10/09 06:43:35 INFO DAGScheduler: ResultStage 6 (foreachRDD at Events.scala:18) finished in 0.005 s
15/10/09 06:43:35 INFO DAGScheduler: Job 27 finished: foreachRDD at Events.scala:18, took 0.017314 s
15/10/09 06:43:35 INFO JobScheduler: Finished job streaming job 1444391015000 ms.0 from job set of time 1444391015000 ms
15/10/09 06:43:35 INFO JobScheduler: Total delay: 0.025 s for time 1444391015000 ms (execution: 0.021 s)
15/10/09 06:43:35 INFO MapPartitionsRDD: Removing RDD 52 from persistence list
15/10/09 06:43:35 INFO BlockManager: Removing RDD 52
15/10/09 06:43:35 INFO WriteAheadLogBackedBlockRDD: Removing RDD 51 from persistence list
15/10/09 06:43:35 INFO BlockManager: Removing RDD 51
15/10/09 06:43:35 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[51] at createStream at Events.scala:13 of time 1444391015000 ms
15/10/09 06:43:35 INFO JobGenerator: Checkpointing graph for time 1444391015000 ms
15/10/09 06:43:35 INFO DStreamGraph: Updating checkpoint data for time 1444391015000 ms
15/10/09 06:43:35 INFO DStreamGraph: Updated checkpoint data for time 1444391015000 ms
15/10/09 06:43:35 INFO BlockManagerInfo: Removed input-0-1444390989315 on localhost:55619 in memory (size: 74.0 B, free: 530.0 MB)
15/10/09 06:43:35 INFO CheckpointWriter: Saving checkpoint for time 1444391015000 ms to file 'file:/tmp/checkpoint-1444391015000'
15/10/09 06:43:35 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391010000
15/10/09 06:43:35 INFO CheckpointWriter: Checkpoint for time 1444391015000 ms saved to file 'file:/tmp/checkpoint-1444391015000', took 3615 bytes and 7 ms
15/10/09 06:43:35 INFO DStreamGraph: Clearing checkpoint data for time 1444391015000 ms
15/10/09 06:43:35 INFO DStreamGraph: Cleared checkpoint data for time 1444391015000 ms
15/10/09 06:43:35 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391013000 ms)
15/10/09 06:43:35 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391014000: 
15/10/09 06:43:35 INFO ReceiverTracker: Cleanup old received batch data: 1444391014000 ms
15/10/09 06:43:35 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391014000
15/10/09 06:43:35 INFO InputInfoTracker: remove old batch metadata: 1444391013000 ms
15/10/09 06:43:35 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391014000: 
15/10/09 06:43:35 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391014000
15/10/09 06:43:36 INFO JobScheduler: Added jobs for time 1444391016000 ms
15/10/09 06:43:36 INFO JobGenerator: Checkpointing graph for time 1444391016000 ms
15/10/09 06:43:36 INFO DStreamGraph: Updating checkpoint data for time 1444391016000 ms
15/10/09 06:43:36 INFO DStreamGraph: Updated checkpoint data for time 1444391016000 ms
15/10/09 06:43:36 INFO JobScheduler: Starting job streaming job 1444391016000 ms.0 from job set of time 1444391016000 ms
15/10/09 06:43:36 INFO CheckpointWriter: Saving checkpoint for time 1444391016000 ms to file 'file:/tmp/checkpoint-1444391016000'
15/10/09 06:43:36 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:36 INFO DAGScheduler: Job 28 finished: foreachRDD at Events.scala:18, took 0.000023 s
15/10/09 06:43:36 INFO JobScheduler: Finished job streaming job 1444391016000 ms.0 from job set of time 1444391016000 ms
15/10/09 06:43:36 INFO JobScheduler: Total delay: 0.008 s for time 1444391016000 ms (execution: 0.003 s)
15/10/09 06:43:36 INFO MapPartitionsRDD: Removing RDD 54 from persistence list
15/10/09 06:43:36 INFO BlockManager: Removing RDD 54
15/10/09 06:43:36 INFO WriteAheadLogBackedBlockRDD: Removing RDD 53 from persistence list
15/10/09 06:43:36 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[53] at createStream at Events.scala:13 of time 1444391016000 ms
15/10/09 06:43:36 INFO BlockManager: Removing RDD 53
15/10/09 06:43:36 INFO JobGenerator: Checkpointing graph for time 1444391016000 ms
15/10/09 06:43:36 INFO DStreamGraph: Updating checkpoint data for time 1444391016000 ms
15/10/09 06:43:36 INFO DStreamGraph: Updated checkpoint data for time 1444391016000 ms
15/10/09 06:43:36 INFO BlockManagerInfo: Removed input-0-1444390989316 on localhost:55619 in memory (size: 74.0 B, free: 530.0 MB)
15/10/09 06:43:36 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391011000.bk
15/10/09 06:43:36 INFO CheckpointWriter: Checkpoint for time 1444391016000 ms saved to file 'file:/tmp/checkpoint-1444391016000', took 3619 bytes and 8 ms
15/10/09 06:43:36 INFO CheckpointWriter: Saving checkpoint for time 1444391016000 ms to file 'file:/tmp/checkpoint-1444391016000'
15/10/09 06:43:36 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391011000
15/10/09 06:43:36 INFO CheckpointWriter: Checkpoint for time 1444391016000 ms saved to file 'file:/tmp/checkpoint-1444391016000', took 3615 bytes and 7 ms
15/10/09 06:43:36 INFO DStreamGraph: Clearing checkpoint data for time 1444391016000 ms
15/10/09 06:43:36 INFO DStreamGraph: Cleared checkpoint data for time 1444391016000 ms
15/10/09 06:43:36 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391014000 ms)
15/10/09 06:43:36 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391015000: 
15/10/09 06:43:36 INFO ReceiverTracker: Cleanup old received batch data: 1444391015000 ms
15/10/09 06:43:36 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391015000
15/10/09 06:43:36 INFO InputInfoTracker: remove old batch metadata: 1444391014000 ms
15/10/09 06:43:36 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391015000: 
15/10/09 06:43:36 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391015000
15/10/09 06:43:36 INFO MemoryStore: ensureFreeSpace(74) called with curMem=250111, maxMem=555755765
15/10/09 06:43:36 INFO MemoryStore: Block input-0-1444390989317 stored as bytes in memory (estimated size 74.0 B, free 529.8 MB)
15/10/09 06:43:36 INFO BlockManagerInfo: Added input-0-1444390989317 in memory on localhost:55619 (size: 74.0 B, free: 530.0 MB)
15/10/09 06:43:36 INFO ReliableKafkaReceiver: Committed offset 427 for topic events, partition 0
15/10/09 06:43:36 INFO BlockGenerator: Pushed block input-0-1444391016000
15/10/09 06:43:37 INFO JobScheduler: Added jobs for time 1444391017000 ms
15/10/09 06:43:37 INFO JobGenerator: Checkpointing graph for time 1444391017000 ms
15/10/09 06:43:37 INFO DStreamGraph: Updating checkpoint data for time 1444391017000 ms
15/10/09 06:43:37 INFO DStreamGraph: Updated checkpoint data for time 1444391017000 ms
15/10/09 06:43:37 INFO JobScheduler: Starting job streaming job 1444391017000 ms.0 from job set of time 1444391017000 ms
15/10/09 06:43:37 INFO CheckpointWriter: Saving checkpoint for time 1444391017000 ms to file 'file:/tmp/checkpoint-1444391017000'
15/10/09 06:43:37 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:37 INFO DAGScheduler: Got job 29 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:37 INFO DAGScheduler: Final stage: ResultStage 7(foreachRDD at Events.scala:18)
15/10/09 06:43:37 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:37 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:37 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[58] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:37 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391012000.bk
15/10/09 06:43:37 INFO CheckpointWriter: Checkpoint for time 1444391017000 ms saved to file 'file:/tmp/checkpoint-1444391017000', took 3619 bytes and 8 ms
15/10/09 06:43:37 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=250185, maxMem=555755765
15/10/09 06:43:37 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 61.1 KB, free 529.7 MB)
15/10/09 06:43:37 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=312793, maxMem=555755765
15/10/09 06:43:37 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.7 MB)
15/10/09 06:43:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:43:37 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[58] at map at Events.scala:13)
15/10/09 06:43:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
15/10/09 06:43:37 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:37 INFO Executor: Running task 0.0 in stage 7.0 (TID 7)
15/10/09 06:43:37 INFO BlockManager: Found block input-0-1444390989317 locally
15/10/09 06:43:37 INFO Executor: Finished task 0.0 in stage 7.0 (TID 7). 915 bytes result sent to driver
15/10/09 06:43:37 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 5 ms on localhost (1/1)
15/10/09 06:43:37 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
15/10/09 06:43:37 INFO DAGScheduler: ResultStage 7 (foreachRDD at Events.scala:18) finished in 0.005 s
15/10/09 06:43:37 INFO DAGScheduler: Job 29 finished: foreachRDD at Events.scala:18, took 0.016595 s
15/10/09 06:43:37 INFO JobScheduler: Finished job streaming job 1444391017000 ms.0 from job set of time 1444391017000 ms
15/10/09 06:43:37 INFO JobScheduler: Total delay: 0.025 s for time 1444391017000 ms (execution: 0.020 s)
15/10/09 06:43:37 INFO MapPartitionsRDD: Removing RDD 56 from persistence list
15/10/09 06:43:37 INFO BlockManager: Removing RDD 56
15/10/09 06:43:37 INFO WriteAheadLogBackedBlockRDD: Removing RDD 55 from persistence list
15/10/09 06:43:37 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[55] at createStream at Events.scala:13 of time 1444391017000 ms
15/10/09 06:43:37 INFO BlockManager: Removing RDD 55
15/10/09 06:43:37 INFO JobGenerator: Checkpointing graph for time 1444391017000 ms
15/10/09 06:43:37 INFO DStreamGraph: Updating checkpoint data for time 1444391017000 ms
15/10/09 06:43:37 INFO DStreamGraph: Updated checkpoint data for time 1444391017000 ms
15/10/09 06:43:37 INFO CheckpointWriter: Saving checkpoint for time 1444391017000 ms to file 'file:/tmp/checkpoint-1444391017000'
15/10/09 06:43:37 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391012000
15/10/09 06:43:37 INFO CheckpointWriter: Checkpoint for time 1444391017000 ms saved to file 'file:/tmp/checkpoint-1444391017000', took 3615 bytes and 7 ms
15/10/09 06:43:37 INFO DStreamGraph: Clearing checkpoint data for time 1444391017000 ms
15/10/09 06:43:37 INFO DStreamGraph: Cleared checkpoint data for time 1444391017000 ms
15/10/09 06:43:37 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391015000 ms)
15/10/09 06:43:37 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391016000: 
15/10/09 06:43:37 INFO ReceiverTracker: Cleanup old received batch data: 1444391016000 ms
15/10/09 06:43:37 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391016000
15/10/09 06:43:37 INFO InputInfoTracker: remove old batch metadata: 1444391015000 ms
15/10/09 06:43:37 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391016000: 
15/10/09 06:43:37 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391016000
15/10/09 06:43:38 INFO JobScheduler: Added jobs for time 1444391018000 ms
15/10/09 06:43:38 INFO JobGenerator: Checkpointing graph for time 1444391018000 ms
15/10/09 06:43:38 INFO DStreamGraph: Updating checkpoint data for time 1444391018000 ms
15/10/09 06:43:38 INFO DStreamGraph: Updated checkpoint data for time 1444391018000 ms
15/10/09 06:43:38 INFO JobScheduler: Starting job streaming job 1444391018000 ms.0 from job set of time 1444391018000 ms
15/10/09 06:43:38 INFO CheckpointWriter: Saving checkpoint for time 1444391018000 ms to file 'file:/tmp/checkpoint-1444391018000'
15/10/09 06:43:38 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:38 INFO DAGScheduler: Job 30 finished: foreachRDD at Events.scala:18, took 0.000022 s
15/10/09 06:43:38 INFO JobScheduler: Finished job streaming job 1444391018000 ms.0 from job set of time 1444391018000 ms
15/10/09 06:43:38 INFO JobScheduler: Total delay: 0.007 s for time 1444391018000 ms (execution: 0.003 s)
15/10/09 06:43:38 INFO MapPartitionsRDD: Removing RDD 58 from persistence list
15/10/09 06:43:38 INFO WriteAheadLogBackedBlockRDD: Removing RDD 57 from persistence list
15/10/09 06:43:38 INFO BlockManager: Removing RDD 58
15/10/09 06:43:38 INFO BlockManager: Removing RDD 57
15/10/09 06:43:38 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[57] at createStream at Events.scala:13 of time 1444391018000 ms
15/10/09 06:43:38 INFO JobGenerator: Checkpointing graph for time 1444391018000 ms
15/10/09 06:43:38 INFO DStreamGraph: Updating checkpoint data for time 1444391018000 ms
15/10/09 06:43:38 INFO DStreamGraph: Updated checkpoint data for time 1444391018000 ms
15/10/09 06:43:38 INFO BlockManagerInfo: Removed input-0-1444390989317 on localhost:55619 in memory (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:38 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391013000.bk
15/10/09 06:43:38 INFO CheckpointWriter: Checkpoint for time 1444391018000 ms saved to file 'file:/tmp/checkpoint-1444391018000', took 3619 bytes and 9 ms
15/10/09 06:43:38 INFO CheckpointWriter: Saving checkpoint for time 1444391018000 ms to file 'file:/tmp/checkpoint-1444391018000'
15/10/09 06:43:38 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391013000
15/10/09 06:43:38 INFO CheckpointWriter: Checkpoint for time 1444391018000 ms saved to file 'file:/tmp/checkpoint-1444391018000', took 3615 bytes and 6 ms
15/10/09 06:43:38 INFO DStreamGraph: Clearing checkpoint data for time 1444391018000 ms
15/10/09 06:43:38 INFO DStreamGraph: Cleared checkpoint data for time 1444391018000 ms
15/10/09 06:43:38 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391016000 ms)
15/10/09 06:43:38 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391017000: 
15/10/09 06:43:38 INFO ReceiverTracker: Cleanup old received batch data: 1444391017000 ms
15/10/09 06:43:38 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391017000
15/10/09 06:43:38 INFO InputInfoTracker: remove old batch metadata: 1444391016000 ms
15/10/09 06:43:38 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391017000: 
15/10/09 06:43:38 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391017000
15/10/09 06:43:38 INFO MemoryStore: ensureFreeSpace(74) called with curMem=333679, maxMem=555755765
15/10/09 06:43:38 INFO MemoryStore: Block input-0-1444390989318 stored as bytes in memory (estimated size 74.0 B, free 529.7 MB)
15/10/09 06:43:38 INFO BlockManagerInfo: Added input-0-1444390989318 in memory on localhost:55619 (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:38 INFO ReliableKafkaReceiver: Committed offset 428 for topic events, partition 0
15/10/09 06:43:38 INFO BlockGenerator: Pushed block input-0-1444391018000
15/10/09 06:43:39 INFO JobScheduler: Added jobs for time 1444391019000 ms
15/10/09 06:43:39 INFO JobGenerator: Checkpointing graph for time 1444391019000 ms
15/10/09 06:43:39 INFO DStreamGraph: Updating checkpoint data for time 1444391019000 ms
15/10/09 06:43:39 INFO DStreamGraph: Updated checkpoint data for time 1444391019000 ms
15/10/09 06:43:39 INFO JobScheduler: Starting job streaming job 1444391019000 ms.0 from job set of time 1444391019000 ms
15/10/09 06:43:39 INFO CheckpointWriter: Saving checkpoint for time 1444391019000 ms to file 'file:/tmp/checkpoint-1444391019000'
15/10/09 06:43:39 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:39 INFO DAGScheduler: Got job 31 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:39 INFO DAGScheduler: Final stage: ResultStage 8(foreachRDD at Events.scala:18)
15/10/09 06:43:39 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:39 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:39 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[62] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:39 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391014000.bk
15/10/09 06:43:39 INFO CheckpointWriter: Checkpoint for time 1444391019000 ms saved to file 'file:/tmp/checkpoint-1444391019000', took 3619 bytes and 8 ms
15/10/09 06:43:39 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=333753, maxMem=555755765
15/10/09 06:43:39 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 61.1 KB, free 529.6 MB)
15/10/09 06:43:39 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=396361, maxMem=555755765
15/10/09 06:43:39 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.6 MB)
15/10/09 06:43:39 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:43:39 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[62] at map at Events.scala:13)
15/10/09 06:43:39 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
15/10/09 06:43:39 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:39 INFO Executor: Running task 0.0 in stage 8.0 (TID 8)
15/10/09 06:43:39 INFO BlockManager: Found block input-0-1444390989318 locally
15/10/09 06:43:39 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 8)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:43:39 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 8, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:43:39 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job
15/10/09 06:43:39 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
15/10/09 06:43:39 INFO TaskSchedulerImpl: Cancelling stage 8
15/10/09 06:43:39 INFO DAGScheduler: ResultStage 8 (foreachRDD at Events.scala:18) failed in 0.006 s
15/10/09 06:43:39 INFO DAGScheduler: Job 31 failed: foreachRDD at Events.scala:18, took 0.017074 s
15/10/09 06:43:39 INFO JobScheduler: Finished job streaming job 1444391019000 ms.0 from job set of time 1444391019000 ms
15/10/09 06:43:39 INFO JobScheduler: Total delay: 0.028 s for time 1444391019000 ms (execution: 0.021 s)
15/10/09 06:43:39 INFO MapPartitionsRDD: Removing RDD 60 from persistence list
15/10/09 06:43:39 ERROR JobScheduler: Error running job streaming job 1444391019000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 8, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:43:39 INFO WriteAheadLogBackedBlockRDD: Removing RDD 59 from persistence list
15/10/09 06:43:39 INFO BlockManager: Removing RDD 60
15/10/09 06:43:39 INFO BlockManager: Removing RDD 59
15/10/09 06:43:39 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[59] at createStream at Events.scala:13 of time 1444391019000 ms
15/10/09 06:43:39 INFO JobGenerator: Checkpointing graph for time 1444391019000 ms
15/10/09 06:43:39 INFO DStreamGraph: Updating checkpoint data for time 1444391019000 ms
15/10/09 06:43:39 INFO DStreamGraph: Updated checkpoint data for time 1444391019000 ms
15/10/09 06:43:39 INFO CheckpointWriter: Saving checkpoint for time 1444391019000 ms to file 'file:/tmp/checkpoint-1444391019000'
15/10/09 06:43:39 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391014000
15/10/09 06:43:39 INFO CheckpointWriter: Checkpoint for time 1444391019000 ms saved to file 'file:/tmp/checkpoint-1444391019000', took 3615 bytes and 7 ms
15/10/09 06:43:39 INFO DStreamGraph: Clearing checkpoint data for time 1444391019000 ms
15/10/09 06:43:39 INFO DStreamGraph: Cleared checkpoint data for time 1444391019000 ms
15/10/09 06:43:39 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391017000 ms)
15/10/09 06:43:39 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391018000: 
15/10/09 06:43:39 INFO ReceiverTracker: Cleanup old received batch data: 1444391018000 ms
15/10/09 06:43:39 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391018000
15/10/09 06:43:39 INFO InputInfoTracker: remove old batch metadata: 1444391017000 ms
15/10/09 06:43:39 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391018000: 
15/10/09 06:43:39 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391018000
15/10/09 06:43:40 INFO JobScheduler: Added jobs for time 1444391020000 ms
15/10/09 06:43:40 INFO JobGenerator: Checkpointing graph for time 1444391020000 ms
15/10/09 06:43:40 INFO DStreamGraph: Updating checkpoint data for time 1444391020000 ms
15/10/09 06:43:40 INFO DStreamGraph: Updated checkpoint data for time 1444391020000 ms
15/10/09 06:43:40 INFO JobScheduler: Starting job streaming job 1444391020000 ms.0 from job set of time 1444391020000 ms
15/10/09 06:43:40 INFO CheckpointWriter: Saving checkpoint for time 1444391020000 ms to file 'file:/tmp/checkpoint-1444391020000'
15/10/09 06:43:40 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:40 INFO DAGScheduler: Job 32 finished: foreachRDD at Events.scala:18, took 0.000021 s
15/10/09 06:43:40 INFO JobScheduler: Finished job streaming job 1444391020000 ms.0 from job set of time 1444391020000 ms
15/10/09 06:43:40 INFO JobScheduler: Total delay: 0.006 s for time 1444391020000 ms (execution: 0.003 s)
15/10/09 06:43:40 INFO MapPartitionsRDD: Removing RDD 62 from persistence list
15/10/09 06:43:40 INFO BlockManager: Removing RDD 62
15/10/09 06:43:40 INFO WriteAheadLogBackedBlockRDD: Removing RDD 61 from persistence list
15/10/09 06:43:40 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[61] at createStream at Events.scala:13 of time 1444391020000 ms
15/10/09 06:43:40 INFO BlockManager: Removing RDD 61
15/10/09 06:43:40 INFO JobGenerator: Checkpointing graph for time 1444391020000 ms
15/10/09 06:43:40 INFO DStreamGraph: Updating checkpoint data for time 1444391020000 ms
15/10/09 06:43:40 INFO DStreamGraph: Updated checkpoint data for time 1444391020000 ms
15/10/09 06:43:40 INFO BlockManagerInfo: Removed input-0-1444390989318 on localhost:55619 in memory (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:40 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391015000.bk
15/10/09 06:43:40 INFO CheckpointWriter: Checkpoint for time 1444391020000 ms saved to file 'file:/tmp/checkpoint-1444391020000', took 3619 bytes and 8 ms
15/10/09 06:43:40 INFO CheckpointWriter: Saving checkpoint for time 1444391020000 ms to file 'file:/tmp/checkpoint-1444391020000'
15/10/09 06:43:40 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391015000
15/10/09 06:43:40 INFO CheckpointWriter: Checkpoint for time 1444391020000 ms saved to file 'file:/tmp/checkpoint-1444391020000', took 3615 bytes and 6 ms
15/10/09 06:43:40 INFO DStreamGraph: Clearing checkpoint data for time 1444391020000 ms
15/10/09 06:43:40 INFO DStreamGraph: Cleared checkpoint data for time 1444391020000 ms
15/10/09 06:43:40 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391018000 ms)
15/10/09 06:43:40 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391019000: 
15/10/09 06:43:40 INFO ReceiverTracker: Cleanup old received batch data: 1444391019000 ms
15/10/09 06:43:40 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391019000
15/10/09 06:43:40 INFO InputInfoTracker: remove old batch metadata: 1444391018000 ms
15/10/09 06:43:40 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391019000: 
15/10/09 06:43:40 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391019000
15/10/09 06:43:40 INFO MemoryStore: ensureFreeSpace(74) called with curMem=417247, maxMem=555755765
15/10/09 06:43:40 INFO MemoryStore: Block input-0-1444390989319 stored as bytes in memory (estimated size 74.0 B, free 529.6 MB)
15/10/09 06:43:40 INFO BlockManagerInfo: Added input-0-1444390989319 in memory on localhost:55619 (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:40 INFO ReliableKafkaReceiver: Committed offset 429 for topic events, partition 0
15/10/09 06:43:40 INFO BlockGenerator: Pushed block input-0-1444391020000
15/10/09 06:43:41 INFO JobScheduler: Added jobs for time 1444391021000 ms
15/10/09 06:43:41 INFO JobGenerator: Checkpointing graph for time 1444391021000 ms
15/10/09 06:43:41 INFO DStreamGraph: Updating checkpoint data for time 1444391021000 ms
15/10/09 06:43:41 INFO DStreamGraph: Updated checkpoint data for time 1444391021000 ms
15/10/09 06:43:41 INFO JobScheduler: Starting job streaming job 1444391021000 ms.0 from job set of time 1444391021000 ms
15/10/09 06:43:41 INFO CheckpointWriter: Saving checkpoint for time 1444391021000 ms to file 'file:/tmp/checkpoint-1444391021000'
15/10/09 06:43:41 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:41 INFO DAGScheduler: Got job 33 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:41 INFO DAGScheduler: Final stage: ResultStage 9(foreachRDD at Events.scala:18)
15/10/09 06:43:41 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:41 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:41 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[66] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:41 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391016000.bk
15/10/09 06:43:41 INFO CheckpointWriter: Checkpoint for time 1444391021000 ms saved to file 'file:/tmp/checkpoint-1444391021000', took 3619 bytes and 8 ms
15/10/09 06:43:41 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=417321, maxMem=555755765
15/10/09 06:43:41 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 61.1 KB, free 529.6 MB)
15/10/09 06:43:41 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=479929, maxMem=555755765
15/10/09 06:43:41 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.5 MB)
15/10/09 06:43:41 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:43:41 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[66] at map at Events.scala:13)
15/10/09 06:43:41 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks
15/10/09 06:43:41 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:41 INFO Executor: Running task 0.0 in stage 9.0 (TID 9)
15/10/09 06:43:41 INFO BlockManager: Found block input-0-1444390989319 locally
15/10/09 06:43:41 INFO Executor: Finished task 0.0 in stage 9.0 (TID 9). 915 bytes result sent to driver
15/10/09 06:43:41 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 5 ms on localhost (1/1)
15/10/09 06:43:41 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
15/10/09 06:43:41 INFO DAGScheduler: ResultStage 9 (foreachRDD at Events.scala:18) finished in 0.005 s
15/10/09 06:43:41 INFO DAGScheduler: Job 33 finished: foreachRDD at Events.scala:18, took 0.015618 s
15/10/09 06:43:41 INFO JobScheduler: Finished job streaming job 1444391021000 ms.0 from job set of time 1444391021000 ms
15/10/09 06:43:41 INFO JobScheduler: Total delay: 0.024 s for time 1444391021000 ms (execution: 0.018 s)
15/10/09 06:43:41 INFO MapPartitionsRDD: Removing RDD 64 from persistence list
15/10/09 06:43:41 INFO WriteAheadLogBackedBlockRDD: Removing RDD 63 from persistence list
15/10/09 06:43:41 INFO BlockManager: Removing RDD 64
15/10/09 06:43:41 INFO BlockManager: Removing RDD 63
15/10/09 06:43:41 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[63] at createStream at Events.scala:13 of time 1444391021000 ms
15/10/09 06:43:41 INFO JobGenerator: Checkpointing graph for time 1444391021000 ms
15/10/09 06:43:41 INFO DStreamGraph: Updating checkpoint data for time 1444391021000 ms
15/10/09 06:43:41 INFO DStreamGraph: Updated checkpoint data for time 1444391021000 ms
15/10/09 06:43:41 INFO CheckpointWriter: Saving checkpoint for time 1444391021000 ms to file 'file:/tmp/checkpoint-1444391021000'
15/10/09 06:43:41 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391016000
15/10/09 06:43:41 INFO CheckpointWriter: Checkpoint for time 1444391021000 ms saved to file 'file:/tmp/checkpoint-1444391021000', took 3615 bytes and 6 ms
15/10/09 06:43:41 INFO DStreamGraph: Clearing checkpoint data for time 1444391021000 ms
15/10/09 06:43:41 INFO DStreamGraph: Cleared checkpoint data for time 1444391021000 ms
15/10/09 06:43:41 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391019000 ms)
15/10/09 06:43:41 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391020000: 
15/10/09 06:43:41 INFO ReceiverTracker: Cleanup old received batch data: 1444391020000 ms
15/10/09 06:43:41 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391020000
15/10/09 06:43:41 INFO InputInfoTracker: remove old batch metadata: 1444391019000 ms
15/10/09 06:43:41 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391020000: 
15/10/09 06:43:41 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391020000
15/10/09 06:43:42 INFO JobScheduler: Added jobs for time 1444391022000 ms
15/10/09 06:43:42 INFO JobGenerator: Checkpointing graph for time 1444391022000 ms
15/10/09 06:43:42 INFO DStreamGraph: Updating checkpoint data for time 1444391022000 ms
15/10/09 06:43:42 INFO DStreamGraph: Updated checkpoint data for time 1444391022000 ms
15/10/09 06:43:42 INFO JobScheduler: Starting job streaming job 1444391022000 ms.0 from job set of time 1444391022000 ms
15/10/09 06:43:42 INFO CheckpointWriter: Saving checkpoint for time 1444391022000 ms to file 'file:/tmp/checkpoint-1444391022000'
15/10/09 06:43:42 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:42 INFO DAGScheduler: Job 34 finished: foreachRDD at Events.scala:18, took 0.000021 s
15/10/09 06:43:42 INFO JobScheduler: Finished job streaming job 1444391022000 ms.0 from job set of time 1444391022000 ms
15/10/09 06:43:42 INFO JobScheduler: Total delay: 0.007 s for time 1444391022000 ms (execution: 0.004 s)
15/10/09 06:43:42 INFO MapPartitionsRDD: Removing RDD 66 from persistence list
15/10/09 06:43:42 INFO BlockManager: Removing RDD 66
15/10/09 06:43:42 INFO WriteAheadLogBackedBlockRDD: Removing RDD 65 from persistence list
15/10/09 06:43:42 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[65] at createStream at Events.scala:13 of time 1444391022000 ms
15/10/09 06:43:42 INFO BlockManager: Removing RDD 65
15/10/09 06:43:42 INFO JobGenerator: Checkpointing graph for time 1444391022000 ms
15/10/09 06:43:42 INFO DStreamGraph: Updating checkpoint data for time 1444391022000 ms
15/10/09 06:43:42 INFO DStreamGraph: Updated checkpoint data for time 1444391022000 ms
15/10/09 06:43:42 INFO BlockManagerInfo: Removed input-0-1444390989319 on localhost:55619 in memory (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:42 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391017000.bk
15/10/09 06:43:42 INFO CheckpointWriter: Checkpoint for time 1444391022000 ms saved to file 'file:/tmp/checkpoint-1444391022000', took 3619 bytes and 8 ms
15/10/09 06:43:42 INFO CheckpointWriter: Saving checkpoint for time 1444391022000 ms to file 'file:/tmp/checkpoint-1444391022000'
15/10/09 06:43:42 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391017000
15/10/09 06:43:42 INFO CheckpointWriter: Checkpoint for time 1444391022000 ms saved to file 'file:/tmp/checkpoint-1444391022000', took 3615 bytes and 6 ms
15/10/09 06:43:42 INFO DStreamGraph: Clearing checkpoint data for time 1444391022000 ms
15/10/09 06:43:42 INFO DStreamGraph: Cleared checkpoint data for time 1444391022000 ms
15/10/09 06:43:42 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391020000 ms)
15/10/09 06:43:42 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391021000: 
15/10/09 06:43:42 INFO ReceiverTracker: Cleanup old received batch data: 1444391021000 ms
15/10/09 06:43:42 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391021000
15/10/09 06:43:42 INFO InputInfoTracker: remove old batch metadata: 1444391020000 ms
15/10/09 06:43:42 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391021000: 
15/10/09 06:43:42 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391021000
15/10/09 06:43:42 INFO MemoryStore: ensureFreeSpace(74) called with curMem=500815, maxMem=555755765
15/10/09 06:43:42 INFO MemoryStore: Block input-0-1444390989320 stored as bytes in memory (estimated size 74.0 B, free 529.5 MB)
15/10/09 06:43:42 INFO BlockManagerInfo: Added input-0-1444390989320 in memory on localhost:55619 (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:42 INFO ReliableKafkaReceiver: Committed offset 430 for topic events, partition 0
15/10/09 06:43:42 INFO BlockGenerator: Pushed block input-0-1444391022200
15/10/09 06:43:43 INFO JobScheduler: Added jobs for time 1444391023000 ms
15/10/09 06:43:43 INFO JobGenerator: Checkpointing graph for time 1444391023000 ms
15/10/09 06:43:43 INFO DStreamGraph: Updating checkpoint data for time 1444391023000 ms
15/10/09 06:43:43 INFO DStreamGraph: Updated checkpoint data for time 1444391023000 ms
15/10/09 06:43:43 INFO JobScheduler: Starting job streaming job 1444391023000 ms.0 from job set of time 1444391023000 ms
15/10/09 06:43:43 INFO CheckpointWriter: Saving checkpoint for time 1444391023000 ms to file 'file:/tmp/checkpoint-1444391023000'
15/10/09 06:43:43 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:43 INFO DAGScheduler: Got job 35 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:43 INFO DAGScheduler: Final stage: ResultStage 10(foreachRDD at Events.scala:18)
15/10/09 06:43:43 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:43 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:43 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[70] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:43 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391018000.bk
15/10/09 06:43:43 INFO CheckpointWriter: Checkpoint for time 1444391023000 ms saved to file 'file:/tmp/checkpoint-1444391023000', took 3619 bytes and 8 ms
15/10/09 06:43:43 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=500889, maxMem=555755765
15/10/09 06:43:43 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 61.1 KB, free 529.5 MB)
15/10/09 06:43:43 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=563497, maxMem=555755765
15/10/09 06:43:43 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.5 MB)
15/10/09 06:43:43 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:43:43 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[70] at map at Events.scala:13)
15/10/09 06:43:43 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
15/10/09 06:43:43 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 10, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:43 INFO Executor: Running task 0.0 in stage 10.0 (TID 10)
15/10/09 06:43:43 INFO BlockManager: Found block input-0-1444390989320 locally
15/10/09 06:43:43 INFO Executor: Finished task 0.0 in stage 10.0 (TID 10). 915 bytes result sent to driver
15/10/09 06:43:43 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 10) in 4 ms on localhost (1/1)
15/10/09 06:43:43 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
15/10/09 06:43:43 INFO DAGScheduler: ResultStage 10 (foreachRDD at Events.scala:18) finished in 0.005 s
15/10/09 06:43:43 INFO DAGScheduler: Job 35 finished: foreachRDD at Events.scala:18, took 0.015852 s
15/10/09 06:43:43 INFO JobScheduler: Finished job streaming job 1444391023000 ms.0 from job set of time 1444391023000 ms
15/10/09 06:43:43 INFO JobScheduler: Total delay: 0.026 s for time 1444391023000 ms (execution: 0.019 s)
15/10/09 06:43:43 INFO MapPartitionsRDD: Removing RDD 68 from persistence list
15/10/09 06:43:43 INFO BlockManager: Removing RDD 68
15/10/09 06:43:43 INFO WriteAheadLogBackedBlockRDD: Removing RDD 67 from persistence list
15/10/09 06:43:43 INFO BlockManager: Removing RDD 67
15/10/09 06:43:43 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[67] at createStream at Events.scala:13 of time 1444391023000 ms
15/10/09 06:43:43 INFO JobGenerator: Checkpointing graph for time 1444391023000 ms
15/10/09 06:43:43 INFO DStreamGraph: Updating checkpoint data for time 1444391023000 ms
15/10/09 06:43:43 INFO DStreamGraph: Updated checkpoint data for time 1444391023000 ms
15/10/09 06:43:43 INFO CheckpointWriter: Saving checkpoint for time 1444391023000 ms to file 'file:/tmp/checkpoint-1444391023000'
15/10/09 06:43:43 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391018000
15/10/09 06:43:43 INFO CheckpointWriter: Checkpoint for time 1444391023000 ms saved to file 'file:/tmp/checkpoint-1444391023000', took 3615 bytes and 7 ms
15/10/09 06:43:43 INFO DStreamGraph: Clearing checkpoint data for time 1444391023000 ms
15/10/09 06:43:43 INFO DStreamGraph: Cleared checkpoint data for time 1444391023000 ms
15/10/09 06:43:43 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391021000 ms)
15/10/09 06:43:43 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391022000: 
15/10/09 06:43:43 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391022000
15/10/09 06:43:43 INFO ReceiverTracker: Cleanup old received batch data: 1444391022000 ms
15/10/09 06:43:43 INFO InputInfoTracker: remove old batch metadata: 1444391021000 ms
15/10/09 06:43:43 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391022000: 
15/10/09 06:43:43 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391022000
15/10/09 06:43:43 INFO MemoryStore: ensureFreeSpace(74) called with curMem=584457, maxMem=555755765
15/10/09 06:43:43 INFO MemoryStore: Block input-0-1444390989321 stored as bytes in memory (estimated size 74.0 B, free 529.5 MB)
15/10/09 06:43:43 INFO BlockManagerInfo: Added input-0-1444390989321 in memory on localhost:55619 (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:43 INFO ReliableKafkaReceiver: Committed offset 431 for topic events, partition 0
15/10/09 06:43:43 INFO BlockGenerator: Pushed block input-0-1444391023200
15/10/09 06:43:44 INFO JobScheduler: Added jobs for time 1444391024000 ms
15/10/09 06:43:44 INFO JobGenerator: Checkpointing graph for time 1444391024000 ms
15/10/09 06:43:44 INFO DStreamGraph: Updating checkpoint data for time 1444391024000 ms
15/10/09 06:43:44 INFO JobScheduler: Starting job streaming job 1444391024000 ms.0 from job set of time 1444391024000 ms
15/10/09 06:43:44 INFO DStreamGraph: Updated checkpoint data for time 1444391024000 ms
15/10/09 06:43:44 INFO CheckpointWriter: Saving checkpoint for time 1444391024000 ms to file 'file:/tmp/checkpoint-1444391024000'
15/10/09 06:43:44 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:44 INFO DAGScheduler: Got job 36 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:44 INFO DAGScheduler: Final stage: ResultStage 11(foreachRDD at Events.scala:18)
15/10/09 06:43:44 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:44 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:44 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[72] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:44 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391019000.bk
15/10/09 06:43:44 INFO CheckpointWriter: Checkpoint for time 1444391024000 ms saved to file 'file:/tmp/checkpoint-1444391024000', took 3619 bytes and 9 ms
15/10/09 06:43:44 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=584531, maxMem=555755765
15/10/09 06:43:44 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 61.1 KB, free 529.4 MB)
15/10/09 06:43:44 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=647139, maxMem=555755765
15/10/09 06:43:44 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.4 MB)
15/10/09 06:43:44 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:43:44 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:44 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[72] at map at Events.scala:13)
15/10/09 06:43:44 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks
15/10/09 06:43:44 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 11, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:44 INFO Executor: Running task 0.0 in stage 11.0 (TID 11)
15/10/09 06:43:44 INFO BlockManager: Found block input-0-1444390989321 locally
15/10/09 06:43:44 INFO Executor: Finished task 0.0 in stage 11.0 (TID 11). 915 bytes result sent to driver
15/10/09 06:43:44 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 11) in 4 ms on localhost (1/1)
15/10/09 06:43:44 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
15/10/09 06:43:44 INFO DAGScheduler: ResultStage 11 (foreachRDD at Events.scala:18) finished in 0.004 s
15/10/09 06:43:44 INFO DAGScheduler: Job 36 finished: foreachRDD at Events.scala:18, took 0.016089 s
15/10/09 06:43:44 INFO JobScheduler: Finished job streaming job 1444391024000 ms.0 from job set of time 1444391024000 ms
15/10/09 06:43:44 INFO JobScheduler: Total delay: 0.026 s for time 1444391024000 ms (execution: 0.019 s)
15/10/09 06:43:44 INFO MapPartitionsRDD: Removing RDD 70 from persistence list
15/10/09 06:43:44 INFO BlockManager: Removing RDD 70
15/10/09 06:43:44 INFO WriteAheadLogBackedBlockRDD: Removing RDD 69 from persistence list
15/10/09 06:43:44 INFO BlockManager: Removing RDD 69
15/10/09 06:43:44 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[69] at createStream at Events.scala:13 of time 1444391024000 ms
15/10/09 06:43:44 INFO JobGenerator: Checkpointing graph for time 1444391024000 ms
15/10/09 06:43:44 INFO DStreamGraph: Updating checkpoint data for time 1444391024000 ms
15/10/09 06:43:44 INFO DStreamGraph: Updated checkpoint data for time 1444391024000 ms
15/10/09 06:43:44 INFO BlockManagerInfo: Removed input-0-1444390989320 on localhost:55619 in memory (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:44 INFO CheckpointWriter: Saving checkpoint for time 1444391024000 ms to file 'file:/tmp/checkpoint-1444391024000'
15/10/09 06:43:44 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391019000
15/10/09 06:43:44 INFO CheckpointWriter: Checkpoint for time 1444391024000 ms saved to file 'file:/tmp/checkpoint-1444391024000', took 3615 bytes and 6 ms
15/10/09 06:43:44 INFO DStreamGraph: Clearing checkpoint data for time 1444391024000 ms
15/10/09 06:43:44 INFO DStreamGraph: Cleared checkpoint data for time 1444391024000 ms
15/10/09 06:43:44 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391022000 ms)
15/10/09 06:43:44 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391023000: 
15/10/09 06:43:44 INFO ReceiverTracker: Cleanup old received batch data: 1444391023000 ms
15/10/09 06:43:44 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391023000
15/10/09 06:43:44 INFO InputInfoTracker: remove old batch metadata: 1444391022000 ms
15/10/09 06:43:44 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391023000: 
15/10/09 06:43:44 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391023000
15/10/09 06:43:45 INFO JobScheduler: Added jobs for time 1444391025000 ms
15/10/09 06:43:45 INFO JobGenerator: Checkpointing graph for time 1444391025000 ms
15/10/09 06:43:45 INFO DStreamGraph: Updating checkpoint data for time 1444391025000 ms
15/10/09 06:43:45 INFO DStreamGraph: Updated checkpoint data for time 1444391025000 ms
15/10/09 06:43:45 INFO JobScheduler: Starting job streaming job 1444391025000 ms.0 from job set of time 1444391025000 ms
15/10/09 06:43:45 INFO CheckpointWriter: Saving checkpoint for time 1444391025000 ms to file 'file:/tmp/checkpoint-1444391025000'
15/10/09 06:43:45 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:45 INFO DAGScheduler: Job 37 finished: foreachRDD at Events.scala:18, took 0.000023 s
15/10/09 06:43:45 INFO JobScheduler: Finished job streaming job 1444391025000 ms.0 from job set of time 1444391025000 ms
15/10/09 06:43:45 INFO JobScheduler: Total delay: 0.007 s for time 1444391025000 ms (execution: 0.003 s)
15/10/09 06:43:45 INFO MapPartitionsRDD: Removing RDD 72 from persistence list
15/10/09 06:43:45 INFO BlockManager: Removing RDD 72
15/10/09 06:43:45 INFO WriteAheadLogBackedBlockRDD: Removing RDD 71 from persistence list
15/10/09 06:43:45 INFO BlockManager: Removing RDD 71
15/10/09 06:43:45 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[71] at createStream at Events.scala:13 of time 1444391025000 ms
15/10/09 06:43:45 INFO JobGenerator: Checkpointing graph for time 1444391025000 ms
15/10/09 06:43:45 INFO DStreamGraph: Updating checkpoint data for time 1444391025000 ms
15/10/09 06:43:45 INFO DStreamGraph: Updated checkpoint data for time 1444391025000 ms
15/10/09 06:43:45 INFO BlockManagerInfo: Removed input-0-1444390989321 on localhost:55619 in memory (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:45 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391020000.bk
15/10/09 06:43:45 INFO CheckpointWriter: Checkpoint for time 1444391025000 ms saved to file 'file:/tmp/checkpoint-1444391025000', took 3619 bytes and 10 ms
15/10/09 06:43:45 INFO CheckpointWriter: Saving checkpoint for time 1444391025000 ms to file 'file:/tmp/checkpoint-1444391025000'
15/10/09 06:43:45 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391020000
15/10/09 06:43:45 INFO CheckpointWriter: Checkpoint for time 1444391025000 ms saved to file 'file:/tmp/checkpoint-1444391025000', took 3615 bytes and 5 ms
15/10/09 06:43:45 INFO DStreamGraph: Clearing checkpoint data for time 1444391025000 ms
15/10/09 06:43:45 INFO DStreamGraph: Cleared checkpoint data for time 1444391025000 ms
15/10/09 06:43:45 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391023000 ms)
15/10/09 06:43:45 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391024000: 
15/10/09 06:43:45 INFO ReceiverTracker: Cleanup old received batch data: 1444391024000 ms
15/10/09 06:43:45 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391024000
15/10/09 06:43:45 INFO InputInfoTracker: remove old batch metadata: 1444391023000 ms
15/10/09 06:43:45 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391024000: 
15/10/09 06:43:45 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391024000
15/10/09 06:43:45 INFO MemoryStore: ensureFreeSpace(74) called with curMem=667951, maxMem=555755765
15/10/09 06:43:45 INFO MemoryStore: Block input-0-1444390989322 stored as bytes in memory (estimated size 74.0 B, free 529.4 MB)
15/10/09 06:43:45 INFO BlockManagerInfo: Added input-0-1444390989322 in memory on localhost:55619 (size: 74.0 B, free: 529.9 MB)
15/10/09 06:43:45 INFO ReliableKafkaReceiver: Committed offset 432 for topic events, partition 0
15/10/09 06:43:45 INFO BlockGenerator: Pushed block input-0-1444391025200
15/10/09 06:43:46 INFO JobScheduler: Added jobs for time 1444391026000 ms
15/10/09 06:43:46 INFO JobGenerator: Checkpointing graph for time 1444391026000 ms
15/10/09 06:43:46 INFO DStreamGraph: Updating checkpoint data for time 1444391026000 ms
15/10/09 06:43:46 INFO DStreamGraph: Updated checkpoint data for time 1444391026000 ms
15/10/09 06:43:46 INFO JobScheduler: Starting job streaming job 1444391026000 ms.0 from job set of time 1444391026000 ms
15/10/09 06:43:46 INFO CheckpointWriter: Saving checkpoint for time 1444391026000 ms to file 'file:/tmp/checkpoint-1444391026000'
15/10/09 06:43:46 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:46 INFO DAGScheduler: Got job 38 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:46 INFO DAGScheduler: Final stage: ResultStage 12(foreachRDD at Events.scala:18)
15/10/09 06:43:46 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:46 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:46 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[76] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:46 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391021000.bk
15/10/09 06:43:46 INFO CheckpointWriter: Checkpoint for time 1444391026000 ms saved to file 'file:/tmp/checkpoint-1444391026000', took 3619 bytes and 10 ms
15/10/09 06:43:46 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=668025, maxMem=555755765
15/10/09 06:43:46 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 61.1 KB, free 529.3 MB)
15/10/09 06:43:46 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=730633, maxMem=555755765
15/10/09 06:43:46 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.3 MB)
15/10/09 06:43:46 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.8 MB)
15/10/09 06:43:46 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[76] at map at Events.scala:13)
15/10/09 06:43:46 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
15/10/09 06:43:46 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 12, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:46 INFO Executor: Running task 0.0 in stage 12.0 (TID 12)
15/10/09 06:43:46 INFO BlockManager: Found block input-0-1444390989322 locally
15/10/09 06:43:46 INFO Executor: Finished task 0.0 in stage 12.0 (TID 12). 915 bytes result sent to driver
15/10/09 06:43:46 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 12) in 4 ms on localhost (1/1)
15/10/09 06:43:46 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
15/10/09 06:43:46 INFO DAGScheduler: ResultStage 12 (foreachRDD at Events.scala:18) finished in 0.005 s
15/10/09 06:43:46 INFO DAGScheduler: Job 38 finished: foreachRDD at Events.scala:18, took 0.016703 s
15/10/09 06:43:46 INFO JobScheduler: Finished job streaming job 1444391026000 ms.0 from job set of time 1444391026000 ms
15/10/09 06:43:46 INFO JobScheduler: Total delay: 0.024 s for time 1444391026000 ms (execution: 0.020 s)
15/10/09 06:43:46 INFO MapPartitionsRDD: Removing RDD 74 from persistence list
15/10/09 06:43:46 INFO WriteAheadLogBackedBlockRDD: Removing RDD 73 from persistence list
15/10/09 06:43:46 INFO BlockManager: Removing RDD 74
15/10/09 06:43:46 INFO BlockManager: Removing RDD 73
15/10/09 06:43:46 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[73] at createStream at Events.scala:13 of time 1444391026000 ms
15/10/09 06:43:46 INFO JobGenerator: Checkpointing graph for time 1444391026000 ms
15/10/09 06:43:46 INFO DStreamGraph: Updating checkpoint data for time 1444391026000 ms
15/10/09 06:43:46 INFO DStreamGraph: Updated checkpoint data for time 1444391026000 ms
15/10/09 06:43:46 INFO CheckpointWriter: Saving checkpoint for time 1444391026000 ms to file 'file:/tmp/checkpoint-1444391026000'
15/10/09 06:43:46 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391021000
15/10/09 06:43:46 INFO CheckpointWriter: Checkpoint for time 1444391026000 ms saved to file 'file:/tmp/checkpoint-1444391026000', took 3615 bytes and 6 ms
15/10/09 06:43:46 INFO DStreamGraph: Clearing checkpoint data for time 1444391026000 ms
15/10/09 06:43:46 INFO DStreamGraph: Cleared checkpoint data for time 1444391026000 ms
15/10/09 06:43:46 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391024000 ms)
15/10/09 06:43:46 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391025000: 
15/10/09 06:43:46 INFO ReceiverTracker: Cleanup old received batch data: 1444391025000 ms
15/10/09 06:43:46 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391025000
15/10/09 06:43:46 INFO InputInfoTracker: remove old batch metadata: 1444391024000 ms
15/10/09 06:43:46 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391025000: 
15/10/09 06:43:46 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391025000
15/10/09 06:43:47 INFO JobScheduler: Added jobs for time 1444391027000 ms
15/10/09 06:43:47 INFO JobGenerator: Checkpointing graph for time 1444391027000 ms
15/10/09 06:43:47 INFO DStreamGraph: Updating checkpoint data for time 1444391027000 ms
15/10/09 06:43:47 INFO DStreamGraph: Updated checkpoint data for time 1444391027000 ms
15/10/09 06:43:47 INFO JobScheduler: Starting job streaming job 1444391027000 ms.0 from job set of time 1444391027000 ms
15/10/09 06:43:47 INFO CheckpointWriter: Saving checkpoint for time 1444391027000 ms to file 'file:/tmp/checkpoint-1444391027000'
15/10/09 06:43:47 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:47 INFO DAGScheduler: Job 39 finished: foreachRDD at Events.scala:18, took 0.000023 s
15/10/09 06:43:47 INFO JobScheduler: Finished job streaming job 1444391027000 ms.0 from job set of time 1444391027000 ms
15/10/09 06:43:47 INFO JobScheduler: Total delay: 0.007 s for time 1444391027000 ms (execution: 0.002 s)
15/10/09 06:43:47 INFO MapPartitionsRDD: Removing RDD 76 from persistence list
15/10/09 06:43:47 INFO BlockManager: Removing RDD 76
15/10/09 06:43:47 INFO WriteAheadLogBackedBlockRDD: Removing RDD 75 from persistence list
15/10/09 06:43:47 INFO BlockManager: Removing RDD 75
15/10/09 06:43:47 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[75] at createStream at Events.scala:13 of time 1444391027000 ms
15/10/09 06:43:47 INFO JobGenerator: Checkpointing graph for time 1444391027000 ms
15/10/09 06:43:47 INFO DStreamGraph: Updating checkpoint data for time 1444391027000 ms
15/10/09 06:43:47 INFO DStreamGraph: Updated checkpoint data for time 1444391027000 ms
15/10/09 06:43:47 INFO BlockManagerInfo: Removed input-0-1444390989322 on localhost:55619 in memory (size: 74.0 B, free: 529.8 MB)
15/10/09 06:43:47 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391022000.bk
15/10/09 06:43:47 INFO CheckpointWriter: Checkpoint for time 1444391027000 ms saved to file 'file:/tmp/checkpoint-1444391027000', took 3619 bytes and 8 ms
15/10/09 06:43:47 INFO CheckpointWriter: Saving checkpoint for time 1444391027000 ms to file 'file:/tmp/checkpoint-1444391027000'
15/10/09 06:43:47 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391022000
15/10/09 06:43:47 INFO CheckpointWriter: Checkpoint for time 1444391027000 ms saved to file 'file:/tmp/checkpoint-1444391027000', took 3615 bytes and 5 ms
15/10/09 06:43:47 INFO DStreamGraph: Clearing checkpoint data for time 1444391027000 ms
15/10/09 06:43:47 INFO DStreamGraph: Cleared checkpoint data for time 1444391027000 ms
15/10/09 06:43:47 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391025000 ms)
15/10/09 06:43:47 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391026000: 
15/10/09 06:43:47 INFO ReceiverTracker: Cleanup old received batch data: 1444391026000 ms
15/10/09 06:43:47 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391026000
15/10/09 06:43:47 INFO InputInfoTracker: remove old batch metadata: 1444391025000 ms
15/10/09 06:43:47 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391026000: 
15/10/09 06:43:47 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391026000
15/10/09 06:43:47 INFO MemoryStore: ensureFreeSpace(74) called with curMem=751519, maxMem=555755765
15/10/09 06:43:47 INFO MemoryStore: Block input-0-1444390989323 stored as bytes in memory (estimated size 74.0 B, free 529.3 MB)
15/10/09 06:43:47 INFO BlockManagerInfo: Added input-0-1444390989323 in memory on localhost:55619 (size: 74.0 B, free: 529.8 MB)
15/10/09 06:43:47 INFO ReliableKafkaReceiver: Committed offset 433 for topic events, partition 0
15/10/09 06:43:47 INFO BlockGenerator: Pushed block input-0-1444391027200
15/10/09 06:43:48 INFO JobScheduler: Added jobs for time 1444391028000 ms
15/10/09 06:43:48 INFO JobGenerator: Checkpointing graph for time 1444391028000 ms
15/10/09 06:43:48 INFO DStreamGraph: Updating checkpoint data for time 1444391028000 ms
15/10/09 06:43:48 INFO DStreamGraph: Updated checkpoint data for time 1444391028000 ms
15/10/09 06:43:48 INFO JobScheduler: Starting job streaming job 1444391028000 ms.0 from job set of time 1444391028000 ms
15/10/09 06:43:48 INFO CheckpointWriter: Saving checkpoint for time 1444391028000 ms to file 'file:/tmp/checkpoint-1444391028000'
15/10/09 06:43:48 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:48 INFO DAGScheduler: Got job 40 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:48 INFO DAGScheduler: Final stage: ResultStage 13(foreachRDD at Events.scala:18)
15/10/09 06:43:48 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:48 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:48 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[80] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:48 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391023000.bk
15/10/09 06:43:48 INFO CheckpointWriter: Checkpoint for time 1444391028000 ms saved to file 'file:/tmp/checkpoint-1444391028000', took 3619 bytes and 8 ms
15/10/09 06:43:48 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=751593, maxMem=555755765
15/10/09 06:43:48 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 61.1 KB, free 529.2 MB)
15/10/09 06:43:48 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=814201, maxMem=555755765
15/10/09 06:43:48 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.2 MB)
15/10/09 06:43:48 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.8 MB)
15/10/09 06:43:48 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[80] at map at Events.scala:13)
15/10/09 06:43:48 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks
15/10/09 06:43:48 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 13, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:48 INFO Executor: Running task 0.0 in stage 13.0 (TID 13)
15/10/09 06:43:48 INFO BlockManager: Found block input-0-1444390989323 locally
15/10/09 06:43:48 ERROR Executor: Exception in task 0.0 in stage 13.0 (TID 13)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:43:48 WARN TaskSetManager: Lost task 0.0 in stage 13.0 (TID 13, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:43:48 ERROR TaskSetManager: Task 0 in stage 13.0 failed 1 times; aborting job
15/10/09 06:43:48 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
15/10/09 06:43:48 INFO TaskSchedulerImpl: Cancelling stage 13
15/10/09 06:43:48 INFO DAGScheduler: ResultStage 13 (foreachRDD at Events.scala:18) failed in 0.005 s
15/10/09 06:43:48 INFO DAGScheduler: Job 40 failed: foreachRDD at Events.scala:18, took 0.015822 s
15/10/09 06:43:48 INFO JobScheduler: Finished job streaming job 1444391028000 ms.0 from job set of time 1444391028000 ms
15/10/09 06:43:48 INFO JobScheduler: Total delay: 0.023 s for time 1444391028000 ms (execution: 0.019 s)
15/10/09 06:43:48 INFO MapPartitionsRDD: Removing RDD 78 from persistence list
15/10/09 06:43:48 ERROR JobScheduler: Error running job streaming job 1444391028000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 13.0 failed 1 times, most recent failure: Lost task 0.0 in stage 13.0 (TID 13, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:43:48 INFO WriteAheadLogBackedBlockRDD: Removing RDD 77 from persistence list
15/10/09 06:43:48 INFO BlockManager: Removing RDD 78
15/10/09 06:43:48 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[77] at createStream at Events.scala:13 of time 1444391028000 ms
15/10/09 06:43:48 INFO BlockManager: Removing RDD 77
15/10/09 06:43:48 INFO JobGenerator: Checkpointing graph for time 1444391028000 ms
15/10/09 06:43:48 INFO DStreamGraph: Updating checkpoint data for time 1444391028000 ms
15/10/09 06:43:48 INFO DStreamGraph: Updated checkpoint data for time 1444391028000 ms
15/10/09 06:43:48 INFO CheckpointWriter: Saving checkpoint for time 1444391028000 ms to file 'file:/tmp/checkpoint-1444391028000'
15/10/09 06:43:48 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391023000
15/10/09 06:43:48 INFO CheckpointWriter: Checkpoint for time 1444391028000 ms saved to file 'file:/tmp/checkpoint-1444391028000', took 3615 bytes and 6 ms
15/10/09 06:43:48 INFO DStreamGraph: Clearing checkpoint data for time 1444391028000 ms
15/10/09 06:43:48 INFO DStreamGraph: Cleared checkpoint data for time 1444391028000 ms
15/10/09 06:43:48 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391026000 ms)
15/10/09 06:43:48 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391027000: 
15/10/09 06:43:48 INFO ReceiverTracker: Cleanup old received batch data: 1444391027000 ms
15/10/09 06:43:48 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391027000
15/10/09 06:43:48 INFO InputInfoTracker: remove old batch metadata: 1444391026000 ms
15/10/09 06:43:48 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391027000: 
15/10/09 06:43:48 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391027000
15/10/09 06:43:48 INFO MemoryStore: ensureFreeSpace(74) called with curMem=835161, maxMem=555755765
15/10/09 06:43:48 INFO MemoryStore: Block input-0-1444390989324 stored as bytes in memory (estimated size 74.0 B, free 529.2 MB)
15/10/09 06:43:48 INFO BlockManagerInfo: Added input-0-1444390989324 in memory on localhost:55619 (size: 74.0 B, free: 529.8 MB)
15/10/09 06:43:48 INFO ReliableKafkaReceiver: Committed offset 434 for topic events, partition 0
15/10/09 06:43:48 INFO BlockGenerator: Pushed block input-0-1444391028200
15/10/09 06:43:49 INFO JobScheduler: Added jobs for time 1444391029000 ms
15/10/09 06:43:49 INFO JobGenerator: Checkpointing graph for time 1444391029000 ms
15/10/09 06:43:49 INFO DStreamGraph: Updating checkpoint data for time 1444391029000 ms
15/10/09 06:43:49 INFO DStreamGraph: Updated checkpoint data for time 1444391029000 ms
15/10/09 06:43:49 INFO JobScheduler: Starting job streaming job 1444391029000 ms.0 from job set of time 1444391029000 ms
15/10/09 06:43:49 INFO CheckpointWriter: Saving checkpoint for time 1444391029000 ms to file 'file:/tmp/checkpoint-1444391029000'
15/10/09 06:43:49 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:49 INFO DAGScheduler: Got job 41 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:49 INFO DAGScheduler: Final stage: ResultStage 14(foreachRDD at Events.scala:18)
15/10/09 06:43:49 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:49 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:49 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[82] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:49 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391024000.bk
15/10/09 06:43:49 INFO CheckpointWriter: Checkpoint for time 1444391029000 ms saved to file 'file:/tmp/checkpoint-1444391029000', took 3619 bytes and 8 ms
15/10/09 06:43:49 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=835235, maxMem=555755765
15/10/09 06:43:49 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 61.1 KB, free 529.2 MB)
15/10/09 06:43:49 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=897843, maxMem=555755765
15/10/09 06:43:49 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.1 MB)
15/10/09 06:43:49 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.8 MB)
15/10/09 06:43:49 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[82] at map at Events.scala:13)
15/10/09 06:43:49 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
15/10/09 06:43:49 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 14, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:49 INFO Executor: Running task 0.0 in stage 14.0 (TID 14)
15/10/09 06:43:49 INFO BlockManager: Found block input-0-1444390989324 locally
15/10/09 06:43:49 INFO Executor: Finished task 0.0 in stage 14.0 (TID 14). 915 bytes result sent to driver
15/10/09 06:43:49 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 14) in 4 ms on localhost (1/1)
15/10/09 06:43:49 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
15/10/09 06:43:49 INFO DAGScheduler: ResultStage 14 (foreachRDD at Events.scala:18) finished in 0.004 s
15/10/09 06:43:49 INFO DAGScheduler: Job 41 finished: foreachRDD at Events.scala:18, took 0.016555 s
15/10/09 06:43:49 INFO JobScheduler: Finished job streaming job 1444391029000 ms.0 from job set of time 1444391029000 ms
15/10/09 06:43:49 INFO JobScheduler: Total delay: 0.023 s for time 1444391029000 ms (execution: 0.019 s)
15/10/09 06:43:49 INFO MapPartitionsRDD: Removing RDD 80 from persistence list
15/10/09 06:43:49 INFO WriteAheadLogBackedBlockRDD: Removing RDD 79 from persistence list
15/10/09 06:43:49 INFO BlockManager: Removing RDD 80
15/10/09 06:43:49 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[79] at createStream at Events.scala:13 of time 1444391029000 ms
15/10/09 06:43:49 INFO BlockManager: Removing RDD 79
15/10/09 06:43:49 INFO JobGenerator: Checkpointing graph for time 1444391029000 ms
15/10/09 06:43:49 INFO DStreamGraph: Updating checkpoint data for time 1444391029000 ms
15/10/09 06:43:49 INFO DStreamGraph: Updated checkpoint data for time 1444391029000 ms
15/10/09 06:43:49 INFO BlockManagerInfo: Removed input-0-1444390989323 on localhost:55619 in memory (size: 74.0 B, free: 529.8 MB)
15/10/09 06:43:49 INFO CheckpointWriter: Saving checkpoint for time 1444391029000 ms to file 'file:/tmp/checkpoint-1444391029000'
15/10/09 06:43:49 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391024000
15/10/09 06:43:49 INFO CheckpointWriter: Checkpoint for time 1444391029000 ms saved to file 'file:/tmp/checkpoint-1444391029000', took 3615 bytes and 6 ms
15/10/09 06:43:49 INFO DStreamGraph: Clearing checkpoint data for time 1444391029000 ms
15/10/09 06:43:49 INFO DStreamGraph: Cleared checkpoint data for time 1444391029000 ms
15/10/09 06:43:49 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391027000 ms)
15/10/09 06:43:49 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391028000: 
15/10/09 06:43:49 INFO ReceiverTracker: Cleanup old received batch data: 1444391028000 ms
15/10/09 06:43:49 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391028000
15/10/09 06:43:49 INFO InputInfoTracker: remove old batch metadata: 1444391027000 ms
15/10/09 06:43:49 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391028000: 
15/10/09 06:43:49 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391028000
15/10/09 06:43:50 INFO JobScheduler: Added jobs for time 1444391030000 ms
15/10/09 06:43:50 INFO JobGenerator: Checkpointing graph for time 1444391030000 ms
15/10/09 06:43:50 INFO DStreamGraph: Updating checkpoint data for time 1444391030000 ms
15/10/09 06:43:50 INFO DStreamGraph: Updated checkpoint data for time 1444391030000 ms
15/10/09 06:43:50 INFO JobScheduler: Starting job streaming job 1444391030000 ms.0 from job set of time 1444391030000 ms
15/10/09 06:43:50 INFO CheckpointWriter: Saving checkpoint for time 1444391030000 ms to file 'file:/tmp/checkpoint-1444391030000'
15/10/09 06:43:50 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:50 INFO DAGScheduler: Job 42 finished: foreachRDD at Events.scala:18, took 0.000022 s
15/10/09 06:43:50 INFO JobScheduler: Finished job streaming job 1444391030000 ms.0 from job set of time 1444391030000 ms
15/10/09 06:43:50 INFO JobScheduler: Total delay: 0.008 s for time 1444391030000 ms (execution: 0.003 s)
15/10/09 06:43:50 INFO MapPartitionsRDD: Removing RDD 82 from persistence list
15/10/09 06:43:50 INFO BlockManager: Removing RDD 82
15/10/09 06:43:50 INFO WriteAheadLogBackedBlockRDD: Removing RDD 81 from persistence list
15/10/09 06:43:50 INFO BlockManager: Removing RDD 81
15/10/09 06:43:50 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[81] at createStream at Events.scala:13 of time 1444391030000 ms
15/10/09 06:43:50 INFO JobGenerator: Checkpointing graph for time 1444391030000 ms
15/10/09 06:43:50 INFO DStreamGraph: Updating checkpoint data for time 1444391030000 ms
15/10/09 06:43:50 INFO DStreamGraph: Updated checkpoint data for time 1444391030000 ms
15/10/09 06:43:50 INFO BlockManagerInfo: Removed input-0-1444390989324 on localhost:55619 in memory (size: 74.0 B, free: 529.8 MB)
15/10/09 06:43:50 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391025000.bk
15/10/09 06:43:50 INFO CheckpointWriter: Checkpoint for time 1444391030000 ms saved to file 'file:/tmp/checkpoint-1444391030000', took 3619 bytes and 9 ms
15/10/09 06:43:50 INFO CheckpointWriter: Saving checkpoint for time 1444391030000 ms to file 'file:/tmp/checkpoint-1444391030000'
15/10/09 06:43:50 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391025000
15/10/09 06:43:50 INFO CheckpointWriter: Checkpoint for time 1444391030000 ms saved to file 'file:/tmp/checkpoint-1444391030000', took 3615 bytes and 6 ms
15/10/09 06:43:50 INFO DStreamGraph: Clearing checkpoint data for time 1444391030000 ms
15/10/09 06:43:50 INFO DStreamGraph: Cleared checkpoint data for time 1444391030000 ms
15/10/09 06:43:50 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391028000 ms)
15/10/09 06:43:50 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391029000: 
15/10/09 06:43:50 INFO ReceiverTracker: Cleanup old received batch data: 1444391029000 ms
15/10/09 06:43:50 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391029000
15/10/09 06:43:50 INFO InputInfoTracker: remove old batch metadata: 1444391028000 ms
15/10/09 06:43:50 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391029000: 
15/10/09 06:43:50 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391029000
15/10/09 06:43:50 INFO MemoryStore: ensureFreeSpace(74) called with curMem=918655, maxMem=555755765
15/10/09 06:43:50 INFO MemoryStore: Block input-0-1444390989325 stored as bytes in memory (estimated size 74.0 B, free 529.1 MB)
15/10/09 06:43:50 INFO BlockManagerInfo: Added input-0-1444390989325 in memory on localhost:55619 (size: 74.0 B, free: 529.8 MB)
15/10/09 06:43:50 INFO ReliableKafkaReceiver: Committed offset 435 for topic events, partition 0
15/10/09 06:43:50 INFO BlockGenerator: Pushed block input-0-1444391030200
15/10/09 06:43:51 INFO JobScheduler: Added jobs for time 1444391031000 ms
15/10/09 06:43:51 INFO JobGenerator: Checkpointing graph for time 1444391031000 ms
15/10/09 06:43:51 INFO DStreamGraph: Updating checkpoint data for time 1444391031000 ms
15/10/09 06:43:51 INFO DStreamGraph: Updated checkpoint data for time 1444391031000 ms
15/10/09 06:43:51 INFO JobScheduler: Starting job streaming job 1444391031000 ms.0 from job set of time 1444391031000 ms
15/10/09 06:43:51 INFO CheckpointWriter: Saving checkpoint for time 1444391031000 ms to file 'file:/tmp/checkpoint-1444391031000'
15/10/09 06:43:51 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:51 INFO DAGScheduler: Got job 43 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:51 INFO DAGScheduler: Final stage: ResultStage 15(foreachRDD at Events.scala:18)
15/10/09 06:43:51 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:51 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:51 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[86] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:51 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391026000.bk
15/10/09 06:43:51 INFO CheckpointWriter: Checkpoint for time 1444391031000 ms saved to file 'file:/tmp/checkpoint-1444391031000', took 3619 bytes and 9 ms
15/10/09 06:43:51 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=918729, maxMem=555755765
15/10/09 06:43:51 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 61.1 KB, free 529.1 MB)
15/10/09 06:43:51 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=981337, maxMem=555755765
15/10/09 06:43:51 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.1 MB)
15/10/09 06:43:51 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.8 MB)
15/10/09 06:43:51 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[86] at map at Events.scala:13)
15/10/09 06:43:51 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
15/10/09 06:43:51 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:51 INFO Executor: Running task 0.0 in stage 15.0 (TID 15)
15/10/09 06:43:51 INFO BlockManager: Found block input-0-1444390989325 locally
15/10/09 06:43:51 ERROR Executor: Exception in task 0.0 in stage 15.0 (TID 15)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:43:51 WARN TaskSetManager: Lost task 0.0 in stage 15.0 (TID 15, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:43:51 ERROR TaskSetManager: Task 0 in stage 15.0 failed 1 times; aborting job
15/10/09 06:43:51 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
15/10/09 06:43:51 INFO TaskSchedulerImpl: Cancelling stage 15
15/10/09 06:43:51 INFO DAGScheduler: ResultStage 15 (foreachRDD at Events.scala:18) failed in 0.006 s
15/10/09 06:43:51 INFO DAGScheduler: Job 43 failed: foreachRDD at Events.scala:18, took 0.017793 s
15/10/09 06:43:51 INFO JobScheduler: Finished job streaming job 1444391031000 ms.0 from job set of time 1444391031000 ms
15/10/09 06:43:51 INFO JobScheduler: Total delay: 0.026 s for time 1444391031000 ms (execution: 0.021 s)
15/10/09 06:43:51 INFO MapPartitionsRDD: Removing RDD 84 from persistence list
15/10/09 06:43:51 ERROR JobScheduler: Error running job streaming job 1444391031000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 15.0 failed 1 times, most recent failure: Lost task 0.0 in stage 15.0 (TID 15, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:43:51 INFO BlockManager: Removing RDD 84
15/10/09 06:43:51 INFO WriteAheadLogBackedBlockRDD: Removing RDD 83 from persistence list
15/10/09 06:43:51 INFO BlockManager: Removing RDD 83
15/10/09 06:43:51 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[83] at createStream at Events.scala:13 of time 1444391031000 ms
15/10/09 06:43:51 INFO JobGenerator: Checkpointing graph for time 1444391031000 ms
15/10/09 06:43:51 INFO DStreamGraph: Updating checkpoint data for time 1444391031000 ms
15/10/09 06:43:51 INFO DStreamGraph: Updated checkpoint data for time 1444391031000 ms
15/10/09 06:43:51 INFO CheckpointWriter: Saving checkpoint for time 1444391031000 ms to file 'file:/tmp/checkpoint-1444391031000'
15/10/09 06:43:51 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391026000
15/10/09 06:43:51 INFO CheckpointWriter: Checkpoint for time 1444391031000 ms saved to file 'file:/tmp/checkpoint-1444391031000', took 3615 bytes and 6 ms
15/10/09 06:43:51 INFO DStreamGraph: Clearing checkpoint data for time 1444391031000 ms
15/10/09 06:43:51 INFO DStreamGraph: Cleared checkpoint data for time 1444391031000 ms
15/10/09 06:43:51 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391029000 ms)
15/10/09 06:43:51 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391030000: 
15/10/09 06:43:51 INFO ReceiverTracker: Cleanup old received batch data: 1444391030000 ms
15/10/09 06:43:51 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391030000
15/10/09 06:43:51 INFO InputInfoTracker: remove old batch metadata: 1444391029000 ms
15/10/09 06:43:51 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391030000: 
15/10/09 06:43:51 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391030000
15/10/09 06:43:51 INFO MemoryStore: ensureFreeSpace(74) called with curMem=1002297, maxMem=555755765
15/10/09 06:43:51 INFO MemoryStore: Block input-0-1444390989326 stored as bytes in memory (estimated size 74.0 B, free 529.1 MB)
15/10/09 06:43:51 INFO BlockManagerInfo: Added input-0-1444390989326 in memory on localhost:55619 (size: 74.0 B, free: 529.8 MB)
15/10/09 06:43:51 INFO ReliableKafkaReceiver: Committed offset 436 for topic events, partition 0
15/10/09 06:43:51 INFO BlockGenerator: Pushed block input-0-1444391031200
15/10/09 06:43:52 INFO JobScheduler: Added jobs for time 1444391032000 ms
15/10/09 06:43:52 INFO JobGenerator: Checkpointing graph for time 1444391032000 ms
15/10/09 06:43:52 INFO DStreamGraph: Updating checkpoint data for time 1444391032000 ms
15/10/09 06:43:52 INFO DStreamGraph: Updated checkpoint data for time 1444391032000 ms
15/10/09 06:43:52 INFO JobScheduler: Starting job streaming job 1444391032000 ms.0 from job set of time 1444391032000 ms
15/10/09 06:43:52 INFO CheckpointWriter: Saving checkpoint for time 1444391032000 ms to file 'file:/tmp/checkpoint-1444391032000'
15/10/09 06:43:52 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:52 INFO DAGScheduler: Got job 44 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:52 INFO DAGScheduler: Final stage: ResultStage 16(foreachRDD at Events.scala:18)
15/10/09 06:43:52 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:52 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:52 INFO DAGScheduler: Submitting ResultStage 16 (MapPartitionsRDD[88] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:52 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391027000.bk
15/10/09 06:43:52 INFO CheckpointWriter: Checkpoint for time 1444391032000 ms saved to file 'file:/tmp/checkpoint-1444391032000', took 3619 bytes and 9 ms
15/10/09 06:43:52 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=1002371, maxMem=555755765
15/10/09 06:43:52 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 61.1 KB, free 529.0 MB)
15/10/09 06:43:52 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=1064979, maxMem=555755765
15/10/09 06:43:52 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.0 MB)
15/10/09 06:43:52 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.8 MB)
15/10/09 06:43:52 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 16 (MapPartitionsRDD[88] at map at Events.scala:13)
15/10/09 06:43:52 INFO TaskSchedulerImpl: Adding task set 16.0 with 1 tasks
15/10/09 06:43:52 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 16, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:52 INFO Executor: Running task 0.0 in stage 16.0 (TID 16)
15/10/09 06:43:52 INFO BlockManager: Found block input-0-1444390989326 locally
15/10/09 06:43:52 INFO Executor: Finished task 0.0 in stage 16.0 (TID 16). 915 bytes result sent to driver
15/10/09 06:43:52 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 16) in 4 ms on localhost (1/1)
15/10/09 06:43:52 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
15/10/09 06:43:52 INFO DAGScheduler: ResultStage 16 (foreachRDD at Events.scala:18) finished in 0.005 s
15/10/09 06:43:52 INFO DAGScheduler: Job 44 finished: foreachRDD at Events.scala:18, took 0.015573 s
15/10/09 06:43:52 INFO JobScheduler: Finished job streaming job 1444391032000 ms.0 from job set of time 1444391032000 ms
15/10/09 06:43:52 INFO JobScheduler: Total delay: 0.022 s for time 1444391032000 ms (execution: 0.018 s)
15/10/09 06:43:52 INFO MapPartitionsRDD: Removing RDD 86 from persistence list
15/10/09 06:43:52 INFO BlockManager: Removing RDD 86
15/10/09 06:43:52 INFO WriteAheadLogBackedBlockRDD: Removing RDD 85 from persistence list
15/10/09 06:43:52 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[85] at createStream at Events.scala:13 of time 1444391032000 ms
15/10/09 06:43:52 INFO BlockManager: Removing RDD 85
15/10/09 06:43:52 INFO JobGenerator: Checkpointing graph for time 1444391032000 ms
15/10/09 06:43:52 INFO DStreamGraph: Updating checkpoint data for time 1444391032000 ms
15/10/09 06:43:52 INFO DStreamGraph: Updated checkpoint data for time 1444391032000 ms
15/10/09 06:43:52 INFO BlockManagerInfo: Removed input-0-1444390989325 on localhost:55619 in memory (size: 74.0 B, free: 529.8 MB)
15/10/09 06:43:52 INFO CheckpointWriter: Saving checkpoint for time 1444391032000 ms to file 'file:/tmp/checkpoint-1444391032000'
15/10/09 06:43:52 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391027000
15/10/09 06:43:52 INFO CheckpointWriter: Checkpoint for time 1444391032000 ms saved to file 'file:/tmp/checkpoint-1444391032000', took 3615 bytes and 6 ms
15/10/09 06:43:52 INFO DStreamGraph: Clearing checkpoint data for time 1444391032000 ms
15/10/09 06:43:52 INFO DStreamGraph: Cleared checkpoint data for time 1444391032000 ms
15/10/09 06:43:52 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391030000 ms)
15/10/09 06:43:52 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391031000: 
15/10/09 06:43:52 INFO ReceiverTracker: Cleanup old received batch data: 1444391031000 ms
15/10/09 06:43:52 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391031000
15/10/09 06:43:52 INFO InputInfoTracker: remove old batch metadata: 1444391030000 ms
15/10/09 06:43:52 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391031000: 
15/10/09 06:43:52 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391031000
15/10/09 06:43:53 INFO JobScheduler: Added jobs for time 1444391033000 ms
15/10/09 06:43:53 INFO JobGenerator: Checkpointing graph for time 1444391033000 ms
15/10/09 06:43:53 INFO DStreamGraph: Updating checkpoint data for time 1444391033000 ms
15/10/09 06:43:53 INFO DStreamGraph: Updated checkpoint data for time 1444391033000 ms
15/10/09 06:43:53 INFO JobScheduler: Starting job streaming job 1444391033000 ms.0 from job set of time 1444391033000 ms
15/10/09 06:43:53 INFO CheckpointWriter: Saving checkpoint for time 1444391033000 ms to file 'file:/tmp/checkpoint-1444391033000'
15/10/09 06:43:53 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:53 INFO DAGScheduler: Job 45 finished: foreachRDD at Events.scala:18, took 0.000021 s
15/10/09 06:43:53 INFO JobScheduler: Finished job streaming job 1444391033000 ms.0 from job set of time 1444391033000 ms
15/10/09 06:43:53 INFO JobScheduler: Total delay: 0.009 s for time 1444391033000 ms (execution: 0.003 s)
15/10/09 06:43:53 INFO MapPartitionsRDD: Removing RDD 88 from persistence list
15/10/09 06:43:53 INFO WriteAheadLogBackedBlockRDD: Removing RDD 87 from persistence list
15/10/09 06:43:53 INFO BlockManager: Removing RDD 88
15/10/09 06:43:53 INFO BlockManager: Removing RDD 87
15/10/09 06:43:53 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[87] at createStream at Events.scala:13 of time 1444391033000 ms
15/10/09 06:43:53 INFO JobGenerator: Checkpointing graph for time 1444391033000 ms
15/10/09 06:43:53 INFO DStreamGraph: Updating checkpoint data for time 1444391033000 ms
15/10/09 06:43:53 INFO DStreamGraph: Updated checkpoint data for time 1444391033000 ms
15/10/09 06:43:53 INFO BlockManagerInfo: Removed input-0-1444390989326 on localhost:55619 in memory (size: 74.0 B, free: 529.8 MB)
15/10/09 06:43:53 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391028000.bk
15/10/09 06:43:53 INFO CheckpointWriter: Checkpoint for time 1444391033000 ms saved to file 'file:/tmp/checkpoint-1444391033000', took 3619 bytes and 8 ms
15/10/09 06:43:53 INFO CheckpointWriter: Saving checkpoint for time 1444391033000 ms to file 'file:/tmp/checkpoint-1444391033000'
15/10/09 06:43:53 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391028000
15/10/09 06:43:53 INFO CheckpointWriter: Checkpoint for time 1444391033000 ms saved to file 'file:/tmp/checkpoint-1444391033000', took 3615 bytes and 6 ms
15/10/09 06:43:53 INFO DStreamGraph: Clearing checkpoint data for time 1444391033000 ms
15/10/09 06:43:53 INFO DStreamGraph: Cleared checkpoint data for time 1444391033000 ms
15/10/09 06:43:53 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391031000 ms)
15/10/09 06:43:53 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391032000: 
15/10/09 06:43:53 INFO ReceiverTracker: Cleanup old received batch data: 1444391032000 ms
15/10/09 06:43:53 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391032000
15/10/09 06:43:53 INFO InputInfoTracker: remove old batch metadata: 1444391031000 ms
15/10/09 06:43:53 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391032000: 
15/10/09 06:43:53 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391032000
15/10/09 06:43:53 INFO MemoryStore: ensureFreeSpace(74) called with curMem=1085791, maxMem=555755765
15/10/09 06:43:53 INFO MemoryStore: Block input-0-1444390989327 stored as bytes in memory (estimated size 74.0 B, free 529.0 MB)
15/10/09 06:43:53 INFO BlockManagerInfo: Added input-0-1444390989327 in memory on localhost:55619 (size: 74.0 B, free: 529.8 MB)
15/10/09 06:43:53 INFO ReliableKafkaReceiver: Committed offset 437 for topic events, partition 0
15/10/09 06:43:53 INFO BlockGenerator: Pushed block input-0-1444391033200
15/10/09 06:43:54 INFO JobScheduler: Added jobs for time 1444391034000 ms
15/10/09 06:43:54 INFO JobGenerator: Checkpointing graph for time 1444391034000 ms
15/10/09 06:43:54 INFO DStreamGraph: Updating checkpoint data for time 1444391034000 ms
15/10/09 06:43:54 INFO JobScheduler: Starting job streaming job 1444391034000 ms.0 from job set of time 1444391034000 ms
15/10/09 06:43:54 INFO DStreamGraph: Updated checkpoint data for time 1444391034000 ms
15/10/09 06:43:54 INFO CheckpointWriter: Saving checkpoint for time 1444391034000 ms to file 'file:/tmp/checkpoint-1444391034000'
15/10/09 06:43:54 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:54 INFO DAGScheduler: Got job 46 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:54 INFO DAGScheduler: Final stage: ResultStage 17(foreachRDD at Events.scala:18)
15/10/09 06:43:54 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:54 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:54 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[92] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:54 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391029000.bk
15/10/09 06:43:54 INFO CheckpointWriter: Checkpoint for time 1444391034000 ms saved to file 'file:/tmp/checkpoint-1444391034000', took 3619 bytes and 9 ms
15/10/09 06:43:54 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=1085865, maxMem=555755765
15/10/09 06:43:54 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 61.1 KB, free 528.9 MB)
15/10/09 06:43:54 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=1148473, maxMem=555755765
15/10/09 06:43:54 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 20.5 KB, free 528.9 MB)
15/10/09 06:43:54 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.7 MB)
15/10/09 06:43:54 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:54 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[92] at map at Events.scala:13)
15/10/09 06:43:54 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
15/10/09 06:43:54 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 17, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:54 INFO Executor: Running task 0.0 in stage 17.0 (TID 17)
15/10/09 06:43:54 INFO BlockManager: Found block input-0-1444390989327 locally
15/10/09 06:43:54 INFO Executor: Finished task 0.0 in stage 17.0 (TID 17). 915 bytes result sent to driver
15/10/09 06:43:54 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 17) in 4 ms on localhost (1/1)
15/10/09 06:43:54 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
15/10/09 06:43:54 INFO DAGScheduler: ResultStage 17 (foreachRDD at Events.scala:18) finished in 0.004 s
15/10/09 06:43:54 INFO DAGScheduler: Job 46 finished: foreachRDD at Events.scala:18, took 0.015373 s
15/10/09 06:43:54 INFO JobScheduler: Finished job streaming job 1444391034000 ms.0 from job set of time 1444391034000 ms
15/10/09 06:43:54 INFO JobScheduler: Total delay: 0.024 s for time 1444391034000 ms (execution: 0.019 s)
15/10/09 06:43:54 INFO MapPartitionsRDD: Removing RDD 90 from persistence list
15/10/09 06:43:54 INFO BlockManager: Removing RDD 90
15/10/09 06:43:54 INFO WriteAheadLogBackedBlockRDD: Removing RDD 89 from persistence list
15/10/09 06:43:54 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[89] at createStream at Events.scala:13 of time 1444391034000 ms
15/10/09 06:43:54 INFO BlockManager: Removing RDD 89
15/10/09 06:43:54 INFO JobGenerator: Checkpointing graph for time 1444391034000 ms
15/10/09 06:43:54 INFO DStreamGraph: Updating checkpoint data for time 1444391034000 ms
15/10/09 06:43:54 INFO DStreamGraph: Updated checkpoint data for time 1444391034000 ms
15/10/09 06:43:54 INFO CheckpointWriter: Saving checkpoint for time 1444391034000 ms to file 'file:/tmp/checkpoint-1444391034000'
15/10/09 06:43:54 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391029000
15/10/09 06:43:54 INFO CheckpointWriter: Checkpoint for time 1444391034000 ms saved to file 'file:/tmp/checkpoint-1444391034000', took 3615 bytes and 6 ms
15/10/09 06:43:54 INFO DStreamGraph: Clearing checkpoint data for time 1444391034000 ms
15/10/09 06:43:54 INFO DStreamGraph: Cleared checkpoint data for time 1444391034000 ms
15/10/09 06:43:54 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391032000 ms)
15/10/09 06:43:54 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391033000: 
15/10/09 06:43:54 INFO ReceiverTracker: Cleanup old received batch data: 1444391033000 ms
15/10/09 06:43:54 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391033000
15/10/09 06:43:54 INFO InputInfoTracker: remove old batch metadata: 1444391032000 ms
15/10/09 06:43:54 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391033000: 
15/10/09 06:43:54 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391033000
15/10/09 06:43:55 INFO JobScheduler: Added jobs for time 1444391035000 ms
15/10/09 06:43:55 INFO JobGenerator: Checkpointing graph for time 1444391035000 ms
15/10/09 06:43:55 INFO DStreamGraph: Updating checkpoint data for time 1444391035000 ms
15/10/09 06:43:55 INFO DStreamGraph: Updated checkpoint data for time 1444391035000 ms
15/10/09 06:43:55 INFO JobScheduler: Starting job streaming job 1444391035000 ms.0 from job set of time 1444391035000 ms
15/10/09 06:43:55 INFO CheckpointWriter: Saving checkpoint for time 1444391035000 ms to file 'file:/tmp/checkpoint-1444391035000'
15/10/09 06:43:55 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:55 INFO DAGScheduler: Job 47 finished: foreachRDD at Events.scala:18, took 0.000021 s
15/10/09 06:43:55 INFO JobScheduler: Finished job streaming job 1444391035000 ms.0 from job set of time 1444391035000 ms
15/10/09 06:43:55 INFO JobScheduler: Total delay: 0.006 s for time 1444391035000 ms (execution: 0.003 s)
15/10/09 06:43:55 INFO MapPartitionsRDD: Removing RDD 92 from persistence list
15/10/09 06:43:55 INFO WriteAheadLogBackedBlockRDD: Removing RDD 91 from persistence list
15/10/09 06:43:55 INFO BlockManager: Removing RDD 92
15/10/09 06:43:55 INFO BlockManager: Removing RDD 91
15/10/09 06:43:55 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[91] at createStream at Events.scala:13 of time 1444391035000 ms
15/10/09 06:43:55 INFO JobGenerator: Checkpointing graph for time 1444391035000 ms
15/10/09 06:43:55 INFO DStreamGraph: Updating checkpoint data for time 1444391035000 ms
15/10/09 06:43:55 INFO DStreamGraph: Updated checkpoint data for time 1444391035000 ms
15/10/09 06:43:55 INFO BlockManagerInfo: Removed input-0-1444390989327 on localhost:55619 in memory (size: 74.0 B, free: 529.7 MB)
15/10/09 06:43:55 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391030000.bk
15/10/09 06:43:55 INFO CheckpointWriter: Checkpoint for time 1444391035000 ms saved to file 'file:/tmp/checkpoint-1444391035000', took 3619 bytes and 8 ms
15/10/09 06:43:55 INFO CheckpointWriter: Saving checkpoint for time 1444391035000 ms to file 'file:/tmp/checkpoint-1444391035000'
15/10/09 06:43:55 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391030000
15/10/09 06:43:55 INFO CheckpointWriter: Checkpoint for time 1444391035000 ms saved to file 'file:/tmp/checkpoint-1444391035000', took 3615 bytes and 6 ms
15/10/09 06:43:55 INFO DStreamGraph: Clearing checkpoint data for time 1444391035000 ms
15/10/09 06:43:55 INFO DStreamGraph: Cleared checkpoint data for time 1444391035000 ms
15/10/09 06:43:55 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391033000 ms)
15/10/09 06:43:55 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391034000: 
15/10/09 06:43:55 INFO ReceiverTracker: Cleanup old received batch data: 1444391034000 ms
15/10/09 06:43:55 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391034000
15/10/09 06:43:55 INFO InputInfoTracker: remove old batch metadata: 1444391033000 ms
15/10/09 06:43:55 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391034000: 
15/10/09 06:43:55 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391034000
15/10/09 06:43:55 INFO MemoryStore: ensureFreeSpace(74) called with curMem=1169359, maxMem=555755765
15/10/09 06:43:55 INFO MemoryStore: Block input-0-1444390989328 stored as bytes in memory (estimated size 74.0 B, free 528.9 MB)
15/10/09 06:43:55 INFO BlockManagerInfo: Added input-0-1444390989328 in memory on localhost:55619 (size: 74.0 B, free: 529.7 MB)
15/10/09 06:43:55 INFO ReliableKafkaReceiver: Committed offset 438 for topic events, partition 0
15/10/09 06:43:55 INFO BlockGenerator: Pushed block input-0-1444391035200
15/10/09 06:43:56 INFO JobScheduler: Added jobs for time 1444391036000 ms
15/10/09 06:43:56 INFO JobGenerator: Checkpointing graph for time 1444391036000 ms
15/10/09 06:43:56 INFO DStreamGraph: Updating checkpoint data for time 1444391036000 ms
15/10/09 06:43:56 INFO DStreamGraph: Updated checkpoint data for time 1444391036000 ms
15/10/09 06:43:56 INFO JobScheduler: Starting job streaming job 1444391036000 ms.0 from job set of time 1444391036000 ms
15/10/09 06:43:56 INFO CheckpointWriter: Saving checkpoint for time 1444391036000 ms to file 'file:/tmp/checkpoint-1444391036000'
15/10/09 06:43:56 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:56 INFO DAGScheduler: Got job 48 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:56 INFO DAGScheduler: Final stage: ResultStage 18(foreachRDD at Events.scala:18)
15/10/09 06:43:56 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:56 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:56 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[96] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:56 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391031000.bk
15/10/09 06:43:56 INFO CheckpointWriter: Checkpoint for time 1444391036000 ms saved to file 'file:/tmp/checkpoint-1444391036000', took 3619 bytes and 9 ms
15/10/09 06:43:56 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=1169433, maxMem=555755765
15/10/09 06:43:56 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 61.1 KB, free 528.8 MB)
15/10/09 06:43:56 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=1232041, maxMem=555755765
15/10/09 06:43:56 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 20.5 KB, free 528.8 MB)
15/10/09 06:43:56 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.7 MB)
15/10/09 06:43:56 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:56 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[96] at map at Events.scala:13)
15/10/09 06:43:56 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks
15/10/09 06:43:56 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 18, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:56 INFO Executor: Running task 0.0 in stage 18.0 (TID 18)
15/10/09 06:43:56 INFO BlockManager: Found block input-0-1444390989328 locally
15/10/09 06:43:56 INFO Executor: Finished task 0.0 in stage 18.0 (TID 18). 915 bytes result sent to driver
15/10/09 06:43:56 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 18) in 4 ms on localhost (1/1)
15/10/09 06:43:56 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
15/10/09 06:43:56 INFO DAGScheduler: ResultStage 18 (foreachRDD at Events.scala:18) finished in 0.004 s
15/10/09 06:43:56 INFO DAGScheduler: Job 48 finished: foreachRDD at Events.scala:18, took 0.015222 s
15/10/09 06:43:56 INFO JobScheduler: Finished job streaming job 1444391036000 ms.0 from job set of time 1444391036000 ms
15/10/09 06:43:56 INFO JobScheduler: Total delay: 0.024 s for time 1444391036000 ms (execution: 0.018 s)
15/10/09 06:43:56 INFO MapPartitionsRDD: Removing RDD 94 from persistence list
15/10/09 06:43:56 INFO WriteAheadLogBackedBlockRDD: Removing RDD 93 from persistence list
15/10/09 06:43:56 INFO BlockManager: Removing RDD 94
15/10/09 06:43:56 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[93] at createStream at Events.scala:13 of time 1444391036000 ms
15/10/09 06:43:56 INFO BlockManager: Removing RDD 93
15/10/09 06:43:56 INFO JobGenerator: Checkpointing graph for time 1444391036000 ms
15/10/09 06:43:56 INFO DStreamGraph: Updating checkpoint data for time 1444391036000 ms
15/10/09 06:43:56 INFO DStreamGraph: Updated checkpoint data for time 1444391036000 ms
15/10/09 06:43:56 INFO CheckpointWriter: Saving checkpoint for time 1444391036000 ms to file 'file:/tmp/checkpoint-1444391036000'
15/10/09 06:43:56 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391031000
15/10/09 06:43:56 INFO CheckpointWriter: Checkpoint for time 1444391036000 ms saved to file 'file:/tmp/checkpoint-1444391036000', took 3615 bytes and 6 ms
15/10/09 06:43:56 INFO DStreamGraph: Clearing checkpoint data for time 1444391036000 ms
15/10/09 06:43:56 INFO DStreamGraph: Cleared checkpoint data for time 1444391036000 ms
15/10/09 06:43:56 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391034000 ms)
15/10/09 06:43:56 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391035000: 
15/10/09 06:43:56 INFO ReceiverTracker: Cleanup old received batch data: 1444391035000 ms
15/10/09 06:43:56 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391035000
15/10/09 06:43:56 INFO InputInfoTracker: remove old batch metadata: 1444391034000 ms
15/10/09 06:43:56 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391035000: 
15/10/09 06:43:56 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391035000
15/10/09 06:43:56 INFO MemoryStore: ensureFreeSpace(74) called with curMem=1253001, maxMem=555755765
15/10/09 06:43:56 INFO MemoryStore: Block input-0-1444390989329 stored as bytes in memory (estimated size 74.0 B, free 528.8 MB)
15/10/09 06:43:56 INFO BlockManagerInfo: Added input-0-1444390989329 in memory on localhost:55619 (size: 74.0 B, free: 529.7 MB)
15/10/09 06:43:56 INFO ReliableKafkaReceiver: Committed offset 439 for topic events, partition 0
15/10/09 06:43:56 INFO BlockGenerator: Pushed block input-0-1444391036200
15/10/09 06:43:57 INFO JobScheduler: Added jobs for time 1444391037000 ms
15/10/09 06:43:57 INFO JobGenerator: Checkpointing graph for time 1444391037000 ms
15/10/09 06:43:57 INFO DStreamGraph: Updating checkpoint data for time 1444391037000 ms
15/10/09 06:43:57 INFO JobScheduler: Starting job streaming job 1444391037000 ms.0 from job set of time 1444391037000 ms
15/10/09 06:43:57 INFO DStreamGraph: Updated checkpoint data for time 1444391037000 ms
15/10/09 06:43:57 INFO CheckpointWriter: Saving checkpoint for time 1444391037000 ms to file 'file:/tmp/checkpoint-1444391037000'
15/10/09 06:43:57 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:57 INFO DAGScheduler: Got job 49 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:57 INFO DAGScheduler: Final stage: ResultStage 19(foreachRDD at Events.scala:18)
15/10/09 06:43:57 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:57 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:57 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[98] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:57 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391032000.bk
15/10/09 06:43:57 INFO CheckpointWriter: Checkpoint for time 1444391037000 ms saved to file 'file:/tmp/checkpoint-1444391037000', took 3619 bytes and 8 ms
15/10/09 06:43:57 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=1253075, maxMem=555755765
15/10/09 06:43:57 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 61.1 KB, free 528.8 MB)
15/10/09 06:43:57 INFO MemoryStore: ensureFreeSpace(20959) called with curMem=1315683, maxMem=555755765
15/10/09 06:43:57 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 20.5 KB, free 528.7 MB)
15/10/09 06:43:57 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.7 MB)
15/10/09 06:43:57 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[98] at map at Events.scala:13)
15/10/09 06:43:57 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
15/10/09 06:43:57 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 19, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:57 INFO Executor: Running task 0.0 in stage 19.0 (TID 19)
15/10/09 06:43:57 INFO BlockManager: Found block input-0-1444390989329 locally
15/10/09 06:43:57 INFO Executor: Finished task 0.0 in stage 19.0 (TID 19). 915 bytes result sent to driver
15/10/09 06:43:57 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 19) in 4 ms on localhost (1/1)
15/10/09 06:43:57 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
15/10/09 06:43:57 INFO DAGScheduler: ResultStage 19 (foreachRDD at Events.scala:18) finished in 0.004 s
15/10/09 06:43:57 INFO DAGScheduler: Job 49 finished: foreachRDD at Events.scala:18, took 0.015393 s
15/10/09 06:43:57 INFO JobScheduler: Finished job streaming job 1444391037000 ms.0 from job set of time 1444391037000 ms
15/10/09 06:43:57 INFO JobScheduler: Total delay: 0.025 s for time 1444391037000 ms (execution: 0.019 s)
15/10/09 06:43:57 INFO MapPartitionsRDD: Removing RDD 96 from persistence list
15/10/09 06:43:57 INFO BlockManager: Removing RDD 96
15/10/09 06:43:57 INFO WriteAheadLogBackedBlockRDD: Removing RDD 95 from persistence list
15/10/09 06:43:57 INFO BlockManager: Removing RDD 95
15/10/09 06:43:57 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[95] at createStream at Events.scala:13 of time 1444391037000 ms
15/10/09 06:43:57 INFO JobGenerator: Checkpointing graph for time 1444391037000 ms
15/10/09 06:43:57 INFO DStreamGraph: Updating checkpoint data for time 1444391037000 ms
15/10/09 06:43:57 INFO DStreamGraph: Updated checkpoint data for time 1444391037000 ms
15/10/09 06:43:57 INFO BlockManagerInfo: Removed input-0-1444390989328 on localhost:55619 in memory (size: 74.0 B, free: 529.7 MB)
15/10/09 06:43:57 INFO CheckpointWriter: Saving checkpoint for time 1444391037000 ms to file 'file:/tmp/checkpoint-1444391037000'
15/10/09 06:43:57 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391032000
15/10/09 06:43:57 INFO CheckpointWriter: Checkpoint for time 1444391037000 ms saved to file 'file:/tmp/checkpoint-1444391037000', took 3615 bytes and 6 ms
15/10/09 06:43:57 INFO DStreamGraph: Clearing checkpoint data for time 1444391037000 ms
15/10/09 06:43:57 INFO DStreamGraph: Cleared checkpoint data for time 1444391037000 ms
15/10/09 06:43:57 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391035000 ms)
15/10/09 06:43:57 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391036000: 
15/10/09 06:43:57 INFO ReceiverTracker: Cleanup old received batch data: 1444391036000 ms
15/10/09 06:43:57 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391036000
15/10/09 06:43:57 INFO InputInfoTracker: remove old batch metadata: 1444391035000 ms
15/10/09 06:43:57 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391036000: 
15/10/09 06:43:57 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391036000
15/10/09 06:43:57 INFO MemoryStore: ensureFreeSpace(74) called with curMem=1336568, maxMem=555755765
15/10/09 06:43:57 INFO MemoryStore: Block input-0-1444390989330 stored as bytes in memory (estimated size 74.0 B, free 528.7 MB)
15/10/09 06:43:57 INFO BlockManagerInfo: Added input-0-1444390989330 in memory on localhost:55619 (size: 74.0 B, free: 529.7 MB)
15/10/09 06:43:57 INFO ReliableKafkaReceiver: Committed offset 440 for topic events, partition 0
15/10/09 06:43:57 INFO BlockGenerator: Pushed block input-0-1444391037200
15/10/09 06:43:58 INFO JobScheduler: Added jobs for time 1444391038000 ms
15/10/09 06:43:58 INFO JobGenerator: Checkpointing graph for time 1444391038000 ms
15/10/09 06:43:58 INFO DStreamGraph: Updating checkpoint data for time 1444391038000 ms
15/10/09 06:43:58 INFO DStreamGraph: Updated checkpoint data for time 1444391038000 ms
15/10/09 06:43:58 INFO JobScheduler: Starting job streaming job 1444391038000 ms.0 from job set of time 1444391038000 ms
15/10/09 06:43:58 INFO CheckpointWriter: Saving checkpoint for time 1444391038000 ms to file 'file:/tmp/checkpoint-1444391038000'
15/10/09 06:43:58 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:58 INFO DAGScheduler: Got job 50 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:43:58 INFO DAGScheduler: Final stage: ResultStage 20(foreachRDD at Events.scala:18)
15/10/09 06:43:58 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:43:58 INFO DAGScheduler: Missing parents: List()
15/10/09 06:43:58 INFO DAGScheduler: Submitting ResultStage 20 (MapPartitionsRDD[100] at map at Events.scala:13), which has no missing parents
15/10/09 06:43:58 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391033000.bk
15/10/09 06:43:58 INFO CheckpointWriter: Checkpoint for time 1444391038000 ms saved to file 'file:/tmp/checkpoint-1444391038000', took 3619 bytes and 8 ms
15/10/09 06:43:58 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=1336642, maxMem=555755765
15/10/09 06:43:58 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 61.1 KB, free 528.7 MB)
15/10/09 06:43:58 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=1399250, maxMem=555755765
15/10/09 06:43:58 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 20.5 KB, free 528.7 MB)
15/10/09 06:43:58 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.7 MB)
15/10/09 06:43:58 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:861
15/10/09 06:43:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 20 (MapPartitionsRDD[100] at map at Events.scala:13)
15/10/09 06:43:58 INFO TaskSchedulerImpl: Adding task set 20.0 with 1 tasks
15/10/09 06:43:58 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 20, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:43:58 INFO Executor: Running task 0.0 in stage 20.0 (TID 20)
15/10/09 06:43:58 INFO BlockManager: Found block input-0-1444390989330 locally
15/10/09 06:43:58 INFO Executor: Finished task 0.0 in stage 20.0 (TID 20). 915 bytes result sent to driver
15/10/09 06:43:58 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 20) in 4 ms on localhost (1/1)
15/10/09 06:43:58 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
15/10/09 06:43:58 INFO DAGScheduler: ResultStage 20 (foreachRDD at Events.scala:18) finished in 0.005 s
15/10/09 06:43:58 INFO DAGScheduler: Job 50 finished: foreachRDD at Events.scala:18, took 0.016021 s
15/10/09 06:43:58 INFO JobScheduler: Finished job streaming job 1444391038000 ms.0 from job set of time 1444391038000 ms
15/10/09 06:43:58 INFO JobScheduler: Total delay: 0.024 s for time 1444391038000 ms (execution: 0.018 s)
15/10/09 06:43:58 INFO MapPartitionsRDD: Removing RDD 98 from persistence list
15/10/09 06:43:58 INFO WriteAheadLogBackedBlockRDD: Removing RDD 97 from persistence list
15/10/09 06:43:58 INFO BlockManager: Removing RDD 98
15/10/09 06:43:58 INFO BlockManager: Removing RDD 97
15/10/09 06:43:58 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[97] at createStream at Events.scala:13 of time 1444391038000 ms
15/10/09 06:43:58 INFO JobGenerator: Checkpointing graph for time 1444391038000 ms
15/10/09 06:43:58 INFO DStreamGraph: Updating checkpoint data for time 1444391038000 ms
15/10/09 06:43:58 INFO DStreamGraph: Updated checkpoint data for time 1444391038000 ms
15/10/09 06:43:58 INFO BlockManagerInfo: Removed input-0-1444390989329 on localhost:55619 in memory (size: 74.0 B, free: 529.7 MB)
15/10/09 06:43:58 INFO CheckpointWriter: Saving checkpoint for time 1444391038000 ms to file 'file:/tmp/checkpoint-1444391038000'
15/10/09 06:43:58 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391033000
15/10/09 06:43:58 INFO CheckpointWriter: Checkpoint for time 1444391038000 ms saved to file 'file:/tmp/checkpoint-1444391038000', took 3615 bytes and 7 ms
15/10/09 06:43:58 INFO DStreamGraph: Clearing checkpoint data for time 1444391038000 ms
15/10/09 06:43:58 INFO DStreamGraph: Cleared checkpoint data for time 1444391038000 ms
15/10/09 06:43:58 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391036000 ms)
15/10/09 06:43:58 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391037000: 
15/10/09 06:43:58 INFO ReceiverTracker: Cleanup old received batch data: 1444391037000 ms
15/10/09 06:43:58 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391037000
15/10/09 06:43:58 INFO InputInfoTracker: remove old batch metadata: 1444391036000 ms
15/10/09 06:43:58 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391037000: 
15/10/09 06:43:58 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391037000
15/10/09 06:43:59 INFO JobScheduler: Added jobs for time 1444391039000 ms
15/10/09 06:43:59 INFO JobGenerator: Checkpointing graph for time 1444391039000 ms
15/10/09 06:43:59 INFO DStreamGraph: Updating checkpoint data for time 1444391039000 ms
15/10/09 06:43:59 INFO DStreamGraph: Updated checkpoint data for time 1444391039000 ms
15/10/09 06:43:59 INFO JobScheduler: Starting job streaming job 1444391039000 ms.0 from job set of time 1444391039000 ms
15/10/09 06:43:59 INFO CheckpointWriter: Saving checkpoint for time 1444391039000 ms to file 'file:/tmp/checkpoint-1444391039000'
15/10/09 06:43:59 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:43:59 INFO DAGScheduler: Job 51 finished: foreachRDD at Events.scala:18, took 0.000019 s
15/10/09 06:43:59 INFO JobScheduler: Finished job streaming job 1444391039000 ms.0 from job set of time 1444391039000 ms
15/10/09 06:43:59 INFO JobScheduler: Total delay: 0.010 s for time 1444391039000 ms (execution: 0.003 s)
15/10/09 06:43:59 INFO MapPartitionsRDD: Removing RDD 100 from persistence list
15/10/09 06:43:59 INFO BlockManager: Removing RDD 100
15/10/09 06:43:59 INFO WriteAheadLogBackedBlockRDD: Removing RDD 99 from persistence list
15/10/09 06:43:59 INFO BlockManager: Removing RDD 99
15/10/09 06:43:59 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[99] at createStream at Events.scala:13 of time 1444391039000 ms
15/10/09 06:43:59 INFO JobGenerator: Checkpointing graph for time 1444391039000 ms
15/10/09 06:43:59 INFO DStreamGraph: Updating checkpoint data for time 1444391039000 ms
15/10/09 06:43:59 INFO DStreamGraph: Updated checkpoint data for time 1444391039000 ms
15/10/09 06:43:59 INFO BlockManagerInfo: Removed input-0-1444390989330 on localhost:55619 in memory (size: 74.0 B, free: 529.7 MB)
15/10/09 06:43:59 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391034000.bk
15/10/09 06:43:59 INFO CheckpointWriter: Checkpoint for time 1444391039000 ms saved to file 'file:/tmp/checkpoint-1444391039000', took 3619 bytes and 9 ms
15/10/09 06:43:59 INFO CheckpointWriter: Saving checkpoint for time 1444391039000 ms to file 'file:/tmp/checkpoint-1444391039000'
15/10/09 06:43:59 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391034000
15/10/09 06:43:59 INFO CheckpointWriter: Checkpoint for time 1444391039000 ms saved to file 'file:/tmp/checkpoint-1444391039000', took 3615 bytes and 6 ms
15/10/09 06:43:59 INFO DStreamGraph: Clearing checkpoint data for time 1444391039000 ms
15/10/09 06:43:59 INFO DStreamGraph: Cleared checkpoint data for time 1444391039000 ms
15/10/09 06:43:59 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391037000 ms)
15/10/09 06:43:59 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391038000: 
15/10/09 06:43:59 INFO ReceiverTracker: Cleanup old received batch data: 1444391038000 ms
15/10/09 06:43:59 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391038000
15/10/09 06:43:59 INFO InputInfoTracker: remove old batch metadata: 1444391037000 ms
15/10/09 06:43:59 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391038000: 
15/10/09 06:43:59 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391038000
15/10/09 06:43:59 INFO MemoryStore: ensureFreeSpace(74) called with curMem=1420062, maxMem=555755765
15/10/09 06:43:59 INFO MemoryStore: Block input-0-1444390989331 stored as bytes in memory (estimated size 74.0 B, free 528.7 MB)
15/10/09 06:43:59 INFO BlockManagerInfo: Added input-0-1444390989331 in memory on localhost:55619 (size: 74.0 B, free: 529.7 MB)
15/10/09 06:43:59 INFO ReliableKafkaReceiver: Committed offset 441 for topic events, partition 0
15/10/09 06:43:59 INFO BlockGenerator: Pushed block input-0-1444391039200
15/10/09 06:44:00 INFO JobScheduler: Added jobs for time 1444391040000 ms
15/10/09 06:44:00 INFO JobGenerator: Checkpointing graph for time 1444391040000 ms
15/10/09 06:44:00 INFO DStreamGraph: Updating checkpoint data for time 1444391040000 ms
15/10/09 06:44:00 INFO DStreamGraph: Updated checkpoint data for time 1444391040000 ms
15/10/09 06:44:00 INFO JobScheduler: Starting job streaming job 1444391040000 ms.0 from job set of time 1444391040000 ms
15/10/09 06:44:00 INFO CheckpointWriter: Saving checkpoint for time 1444391040000 ms to file 'file:/tmp/checkpoint-1444391040000'
15/10/09 06:44:00 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:00 INFO DAGScheduler: Got job 52 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:44:00 INFO DAGScheduler: Final stage: ResultStage 21(foreachRDD at Events.scala:18)
15/10/09 06:44:00 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:44:00 INFO DAGScheduler: Missing parents: List()
15/10/09 06:44:00 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[104] at map at Events.scala:13), which has no missing parents
15/10/09 06:44:00 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391035000.bk
15/10/09 06:44:00 INFO CheckpointWriter: Checkpoint for time 1444391040000 ms saved to file 'file:/tmp/checkpoint-1444391040000', took 3619 bytes and 8 ms
15/10/09 06:44:00 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=1420136, maxMem=555755765
15/10/09 06:44:00 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 61.1 KB, free 528.6 MB)
15/10/09 06:44:00 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=1482744, maxMem=555755765
15/10/09 06:44:00 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 20.5 KB, free 528.6 MB)
15/10/09 06:44:00 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.7 MB)
15/10/09 06:44:00 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:861
15/10/09 06:44:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[104] at map at Events.scala:13)
15/10/09 06:44:00 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
15/10/09 06:44:00 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 21, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:44:00 INFO Executor: Running task 0.0 in stage 21.0 (TID 21)
15/10/09 06:44:00 INFO BlockManager: Found block input-0-1444390989331 locally
15/10/09 06:44:00 ERROR Executor: Exception in task 0.0 in stage 21.0 (TID 21)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:44:00 WARN TaskSetManager: Lost task 0.0 in stage 21.0 (TID 21, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:44:00 ERROR TaskSetManager: Task 0 in stage 21.0 failed 1 times; aborting job
15/10/09 06:44:00 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
15/10/09 06:44:00 INFO TaskSchedulerImpl: Cancelling stage 21
15/10/09 06:44:00 INFO DAGScheduler: ResultStage 21 (foreachRDD at Events.scala:18) failed in 0.005 s
15/10/09 06:44:00 INFO DAGScheduler: Job 52 failed: foreachRDD at Events.scala:18, took 0.015991 s
15/10/09 06:44:00 INFO JobScheduler: Finished job streaming job 1444391040000 ms.0 from job set of time 1444391040000 ms
15/10/09 06:44:00 INFO JobScheduler: Total delay: 0.022 s for time 1444391040000 ms (execution: 0.019 s)
15/10/09 06:44:00 INFO MapPartitionsRDD: Removing RDD 102 from persistence list
15/10/09 06:44:00 ERROR JobScheduler: Error running job streaming job 1444391040000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 1 times, most recent failure: Lost task 0.0 in stage 21.0 (TID 21, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:44:00 INFO WriteAheadLogBackedBlockRDD: Removing RDD 101 from persistence list
15/10/09 06:44:00 INFO BlockManager: Removing RDD 102
15/10/09 06:44:00 INFO BlockManager: Removing RDD 101
15/10/09 06:44:00 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[101] at createStream at Events.scala:13 of time 1444391040000 ms
15/10/09 06:44:00 INFO JobGenerator: Checkpointing graph for time 1444391040000 ms
15/10/09 06:44:00 INFO DStreamGraph: Updating checkpoint data for time 1444391040000 ms
15/10/09 06:44:00 INFO DStreamGraph: Updated checkpoint data for time 1444391040000 ms
15/10/09 06:44:00 INFO CheckpointWriter: Saving checkpoint for time 1444391040000 ms to file 'file:/tmp/checkpoint-1444391040000'
15/10/09 06:44:00 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391035000
15/10/09 06:44:00 INFO CheckpointWriter: Checkpoint for time 1444391040000 ms saved to file 'file:/tmp/checkpoint-1444391040000', took 3615 bytes and 6 ms
15/10/09 06:44:00 INFO DStreamGraph: Clearing checkpoint data for time 1444391040000 ms
15/10/09 06:44:00 INFO DStreamGraph: Cleared checkpoint data for time 1444391040000 ms
15/10/09 06:44:00 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391038000 ms)
15/10/09 06:44:00 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391039000: 
15/10/09 06:44:00 INFO ReceiverTracker: Cleanup old received batch data: 1444391039000 ms
15/10/09 06:44:00 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391039000
15/10/09 06:44:00 INFO InputInfoTracker: remove old batch metadata: 1444391038000 ms
15/10/09 06:44:00 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391039000: 
15/10/09 06:44:00 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391039000
15/10/09 06:44:00 INFO MemoryStore: ensureFreeSpace(74) called with curMem=1503704, maxMem=555755765
15/10/09 06:44:00 INFO MemoryStore: Block input-0-1444390989332 stored as bytes in memory (estimated size 74.0 B, free 528.6 MB)
15/10/09 06:44:00 INFO BlockManagerInfo: Added input-0-1444390989332 in memory on localhost:55619 (size: 74.0 B, free: 529.7 MB)
15/10/09 06:44:00 INFO ReliableKafkaReceiver: Committed offset 442 for topic events, partition 0
15/10/09 06:44:00 INFO BlockGenerator: Pushed block input-0-1444391040200
15/10/09 06:44:01 INFO JobScheduler: Added jobs for time 1444391041000 ms
15/10/09 06:44:01 INFO JobGenerator: Checkpointing graph for time 1444391041000 ms
15/10/09 06:44:01 INFO DStreamGraph: Updating checkpoint data for time 1444391041000 ms
15/10/09 06:44:01 INFO DStreamGraph: Updated checkpoint data for time 1444391041000 ms
15/10/09 06:44:01 INFO JobScheduler: Starting job streaming job 1444391041000 ms.0 from job set of time 1444391041000 ms
15/10/09 06:44:01 INFO CheckpointWriter: Saving checkpoint for time 1444391041000 ms to file 'file:/tmp/checkpoint-1444391041000'
15/10/09 06:44:01 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:01 INFO DAGScheduler: Got job 53 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:44:01 INFO DAGScheduler: Final stage: ResultStage 22(foreachRDD at Events.scala:18)
15/10/09 06:44:01 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:44:01 INFO DAGScheduler: Missing parents: List()
15/10/09 06:44:01 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[106] at map at Events.scala:13), which has no missing parents
15/10/09 06:44:01 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391036000.bk
15/10/09 06:44:01 INFO CheckpointWriter: Checkpoint for time 1444391041000 ms saved to file 'file:/tmp/checkpoint-1444391041000', took 3619 bytes and 7 ms
15/10/09 06:44:01 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=1503778, maxMem=555755765
15/10/09 06:44:01 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 61.1 KB, free 528.5 MB)
15/10/09 06:44:01 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=1566386, maxMem=555755765
15/10/09 06:44:01 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 20.5 KB, free 528.5 MB)
15/10/09 06:44:01 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.6 MB)
15/10/09 06:44:01 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:861
15/10/09 06:44:01 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[106] at map at Events.scala:13)
15/10/09 06:44:01 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
15/10/09 06:44:01 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 22, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:44:01 INFO Executor: Running task 0.0 in stage 22.0 (TID 22)
15/10/09 06:44:01 INFO BlockManager: Found block input-0-1444390989332 locally
15/10/09 06:44:01 INFO Executor: Finished task 0.0 in stage 22.0 (TID 22). 915 bytes result sent to driver
15/10/09 06:44:01 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 22) in 4 ms on localhost (1/1)
15/10/09 06:44:01 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
15/10/09 06:44:01 INFO DAGScheduler: ResultStage 22 (foreachRDD at Events.scala:18) finished in 0.004 s
15/10/09 06:44:01 INFO DAGScheduler: Job 53 finished: foreachRDD at Events.scala:18, took 0.012981 s
15/10/09 06:44:01 INFO JobScheduler: Finished job streaming job 1444391041000 ms.0 from job set of time 1444391041000 ms
15/10/09 06:44:01 INFO JobScheduler: Total delay: 0.019 s for time 1444391041000 ms (execution: 0.015 s)
15/10/09 06:44:01 INFO MapPartitionsRDD: Removing RDD 104 from persistence list
15/10/09 06:44:01 INFO WriteAheadLogBackedBlockRDD: Removing RDD 103 from persistence list
15/10/09 06:44:01 INFO BlockManager: Removing RDD 104
15/10/09 06:44:01 INFO BlockManager: Removing RDD 103
15/10/09 06:44:01 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[103] at createStream at Events.scala:13 of time 1444391041000 ms
15/10/09 06:44:01 INFO JobGenerator: Checkpointing graph for time 1444391041000 ms
15/10/09 06:44:01 INFO DStreamGraph: Updating checkpoint data for time 1444391041000 ms
15/10/09 06:44:01 INFO DStreamGraph: Updated checkpoint data for time 1444391041000 ms
15/10/09 06:44:01 INFO BlockManagerInfo: Removed input-0-1444390989331 on localhost:55619 in memory (size: 74.0 B, free: 529.6 MB)
15/10/09 06:44:01 INFO CheckpointWriter: Saving checkpoint for time 1444391041000 ms to file 'file:/tmp/checkpoint-1444391041000'
15/10/09 06:44:01 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391036000
15/10/09 06:44:01 INFO CheckpointWriter: Checkpoint for time 1444391041000 ms saved to file 'file:/tmp/checkpoint-1444391041000', took 3615 bytes and 6 ms
15/10/09 06:44:01 INFO DStreamGraph: Clearing checkpoint data for time 1444391041000 ms
15/10/09 06:44:01 INFO DStreamGraph: Cleared checkpoint data for time 1444391041000 ms
15/10/09 06:44:01 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391039000 ms)
15/10/09 06:44:01 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391040000: 
15/10/09 06:44:01 INFO ReceiverTracker: Cleanup old received batch data: 1444391040000 ms
15/10/09 06:44:01 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391040000
15/10/09 06:44:01 INFO InputInfoTracker: remove old batch metadata: 1444391039000 ms
15/10/09 06:44:01 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391040000: 
15/10/09 06:44:01 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391040000
15/10/09 06:44:01 INFO MemoryStore: ensureFreeSpace(74) called with curMem=1587272, maxMem=555755765
15/10/09 06:44:01 INFO MemoryStore: Block input-0-1444390989333 stored as bytes in memory (estimated size 74.0 B, free 528.5 MB)
15/10/09 06:44:01 INFO BlockManagerInfo: Added input-0-1444390989333 in memory on localhost:55619 (size: 74.0 B, free: 529.6 MB)
15/10/09 06:44:01 INFO ReliableKafkaReceiver: Committed offset 443 for topic events, partition 0
15/10/09 06:44:01 INFO BlockGenerator: Pushed block input-0-1444391041200
15/10/09 06:44:02 INFO JobScheduler: Added jobs for time 1444391042000 ms
15/10/09 06:44:02 INFO JobGenerator: Checkpointing graph for time 1444391042000 ms
15/10/09 06:44:02 INFO DStreamGraph: Updating checkpoint data for time 1444391042000 ms
15/10/09 06:44:02 INFO DStreamGraph: Updated checkpoint data for time 1444391042000 ms
15/10/09 06:44:02 INFO JobScheduler: Starting job streaming job 1444391042000 ms.0 from job set of time 1444391042000 ms
15/10/09 06:44:02 INFO CheckpointWriter: Saving checkpoint for time 1444391042000 ms to file 'file:/tmp/checkpoint-1444391042000'
15/10/09 06:44:02 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:02 INFO DAGScheduler: Got job 54 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:44:02 INFO DAGScheduler: Final stage: ResultStage 23(foreachRDD at Events.scala:18)
15/10/09 06:44:02 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:44:02 INFO DAGScheduler: Missing parents: List()
15/10/09 06:44:02 INFO DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[108] at map at Events.scala:13), which has no missing parents
15/10/09 06:44:02 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391037000.bk
15/10/09 06:44:02 INFO CheckpointWriter: Checkpoint for time 1444391042000 ms saved to file 'file:/tmp/checkpoint-1444391042000', took 3619 bytes and 8 ms
15/10/09 06:44:02 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=1587346, maxMem=555755765
15/10/09 06:44:02 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 61.1 KB, free 528.4 MB)
15/10/09 06:44:02 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=1649954, maxMem=555755765
15/10/09 06:44:02 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 20.5 KB, free 528.4 MB)
15/10/09 06:44:02 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.6 MB)
15/10/09 06:44:02 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:861
15/10/09 06:44:02 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[108] at map at Events.scala:13)
15/10/09 06:44:02 INFO TaskSchedulerImpl: Adding task set 23.0 with 1 tasks
15/10/09 06:44:02 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 23, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:44:02 INFO Executor: Running task 0.0 in stage 23.0 (TID 23)
15/10/09 06:44:02 INFO BlockManager: Found block input-0-1444390989333 locally
15/10/09 06:44:02 INFO Executor: Finished task 0.0 in stage 23.0 (TID 23). 915 bytes result sent to driver
15/10/09 06:44:02 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 23) in 4 ms on localhost (1/1)
15/10/09 06:44:02 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
15/10/09 06:44:02 INFO DAGScheduler: ResultStage 23 (foreachRDD at Events.scala:18) finished in 0.004 s
15/10/09 06:44:02 INFO DAGScheduler: Job 54 finished: foreachRDD at Events.scala:18, took 0.014183 s
15/10/09 06:44:02 INFO JobScheduler: Finished job streaming job 1444391042000 ms.0 from job set of time 1444391042000 ms
15/10/09 06:44:02 INFO JobScheduler: Total delay: 0.021 s for time 1444391042000 ms (execution: 0.017 s)
15/10/09 06:44:02 INFO MapPartitionsRDD: Removing RDD 106 from persistence list
15/10/09 06:44:02 INFO BlockManager: Removing RDD 106
15/10/09 06:44:02 INFO WriteAheadLogBackedBlockRDD: Removing RDD 105 from persistence list
15/10/09 06:44:02 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[105] at createStream at Events.scala:13 of time 1444391042000 ms
15/10/09 06:44:02 INFO BlockManager: Removing RDD 105
15/10/09 06:44:02 INFO JobGenerator: Checkpointing graph for time 1444391042000 ms
15/10/09 06:44:02 INFO DStreamGraph: Updating checkpoint data for time 1444391042000 ms
15/10/09 06:44:02 INFO DStreamGraph: Updated checkpoint data for time 1444391042000 ms
15/10/09 06:44:02 INFO BlockManagerInfo: Removed input-0-1444390989332 on localhost:55619 in memory (size: 74.0 B, free: 529.6 MB)
15/10/09 06:44:02 INFO CheckpointWriter: Saving checkpoint for time 1444391042000 ms to file 'file:/tmp/checkpoint-1444391042000'
15/10/09 06:44:02 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391037000
15/10/09 06:44:02 INFO CheckpointWriter: Checkpoint for time 1444391042000 ms saved to file 'file:/tmp/checkpoint-1444391042000', took 3615 bytes and 6 ms
15/10/09 06:44:02 INFO DStreamGraph: Clearing checkpoint data for time 1444391042000 ms
15/10/09 06:44:02 INFO DStreamGraph: Cleared checkpoint data for time 1444391042000 ms
15/10/09 06:44:02 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391040000 ms)
15/10/09 06:44:02 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391041000: 
15/10/09 06:44:02 INFO ReceiverTracker: Cleanup old received batch data: 1444391041000 ms
15/10/09 06:44:02 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391041000
15/10/09 06:44:02 INFO InputInfoTracker: remove old batch metadata: 1444391040000 ms
15/10/09 06:44:02 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391041000: 
15/10/09 06:44:02 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391041000
15/10/09 06:44:03 INFO JobScheduler: Added jobs for time 1444391043000 ms
15/10/09 06:44:03 INFO JobGenerator: Checkpointing graph for time 1444391043000 ms
15/10/09 06:44:03 INFO DStreamGraph: Updating checkpoint data for time 1444391043000 ms
15/10/09 06:44:03 INFO DStreamGraph: Updated checkpoint data for time 1444391043000 ms
15/10/09 06:44:03 INFO JobScheduler: Starting job streaming job 1444391043000 ms.0 from job set of time 1444391043000 ms
15/10/09 06:44:03 INFO CheckpointWriter: Saving checkpoint for time 1444391043000 ms to file 'file:/tmp/checkpoint-1444391043000'
15/10/09 06:44:03 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:03 INFO DAGScheduler: Job 55 finished: foreachRDD at Events.scala:18, took 0.000024 s
15/10/09 06:44:03 INFO JobScheduler: Finished job streaming job 1444391043000 ms.0 from job set of time 1444391043000 ms
15/10/09 06:44:03 INFO JobScheduler: Total delay: 0.009 s for time 1444391043000 ms (execution: 0.003 s)
15/10/09 06:44:03 INFO MapPartitionsRDD: Removing RDD 108 from persistence list
15/10/09 06:44:03 INFO BlockManager: Removing RDD 108
15/10/09 06:44:03 INFO WriteAheadLogBackedBlockRDD: Removing RDD 107 from persistence list
15/10/09 06:44:03 INFO BlockManager: Removing RDD 107
15/10/09 06:44:03 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[107] at createStream at Events.scala:13 of time 1444391043000 ms
15/10/09 06:44:03 INFO JobGenerator: Checkpointing graph for time 1444391043000 ms
15/10/09 06:44:03 INFO DStreamGraph: Updating checkpoint data for time 1444391043000 ms
15/10/09 06:44:03 INFO DStreamGraph: Updated checkpoint data for time 1444391043000 ms
15/10/09 06:44:03 INFO BlockManagerInfo: Removed input-0-1444390989333 on localhost:55619 in memory (size: 74.0 B, free: 529.6 MB)
15/10/09 06:44:03 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391038000.bk
15/10/09 06:44:03 INFO CheckpointWriter: Checkpoint for time 1444391043000 ms saved to file 'file:/tmp/checkpoint-1444391043000', took 3619 bytes and 8 ms
15/10/09 06:44:03 INFO CheckpointWriter: Saving checkpoint for time 1444391043000 ms to file 'file:/tmp/checkpoint-1444391043000'
15/10/09 06:44:03 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391038000
15/10/09 06:44:03 INFO CheckpointWriter: Checkpoint for time 1444391043000 ms saved to file 'file:/tmp/checkpoint-1444391043000', took 3615 bytes and 6 ms
15/10/09 06:44:03 INFO DStreamGraph: Clearing checkpoint data for time 1444391043000 ms
15/10/09 06:44:03 INFO DStreamGraph: Cleared checkpoint data for time 1444391043000 ms
15/10/09 06:44:03 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391041000 ms)
15/10/09 06:44:03 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391042000: 
15/10/09 06:44:03 INFO ReceiverTracker: Cleanup old received batch data: 1444391042000 ms
15/10/09 06:44:03 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391042000
15/10/09 06:44:03 INFO InputInfoTracker: remove old batch metadata: 1444391041000 ms
15/10/09 06:44:03 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391042000: 
15/10/09 06:44:03 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391042000
15/10/09 06:44:03 INFO MemoryStore: ensureFreeSpace(74) called with curMem=1670766, maxMem=555755765
15/10/09 06:44:03 INFO MemoryStore: Block input-0-1444390989334 stored as bytes in memory (estimated size 74.0 B, free 528.4 MB)
15/10/09 06:44:03 INFO BlockManagerInfo: Added input-0-1444390989334 in memory on localhost:55619 (size: 74.0 B, free: 529.6 MB)
15/10/09 06:44:03 INFO ReliableKafkaReceiver: Committed offset 444 for topic events, partition 0
15/10/09 06:44:03 INFO BlockGenerator: Pushed block input-0-1444391043200
15/10/09 06:44:04 INFO JobScheduler: Added jobs for time 1444391044000 ms
15/10/09 06:44:04 INFO JobGenerator: Checkpointing graph for time 1444391044000 ms
15/10/09 06:44:04 INFO DStreamGraph: Updating checkpoint data for time 1444391044000 ms
15/10/09 06:44:04 INFO DStreamGraph: Updated checkpoint data for time 1444391044000 ms
15/10/09 06:44:04 INFO JobScheduler: Starting job streaming job 1444391044000 ms.0 from job set of time 1444391044000 ms
15/10/09 06:44:04 INFO CheckpointWriter: Saving checkpoint for time 1444391044000 ms to file 'file:/tmp/checkpoint-1444391044000'
15/10/09 06:44:04 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:04 INFO DAGScheduler: Got job 56 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:44:04 INFO DAGScheduler: Final stage: ResultStage 24(foreachRDD at Events.scala:18)
15/10/09 06:44:04 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:44:04 INFO DAGScheduler: Missing parents: List()
15/10/09 06:44:04 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[112] at map at Events.scala:13), which has no missing parents
15/10/09 06:44:04 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391039000.bk
15/10/09 06:44:04 INFO CheckpointWriter: Checkpoint for time 1444391044000 ms saved to file 'file:/tmp/checkpoint-1444391044000', took 3619 bytes and 8 ms
15/10/09 06:44:04 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=1670840, maxMem=555755765
15/10/09 06:44:04 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 61.1 KB, free 528.4 MB)
15/10/09 06:44:04 INFO MemoryStore: ensureFreeSpace(20959) called with curMem=1733448, maxMem=555755765
15/10/09 06:44:04 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 20.5 KB, free 528.3 MB)
15/10/09 06:44:04 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.6 MB)
15/10/09 06:44:04 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:861
15/10/09 06:44:04 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[112] at map at Events.scala:13)
15/10/09 06:44:04 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
15/10/09 06:44:04 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 24, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:44:04 INFO Executor: Running task 0.0 in stage 24.0 (TID 24)
15/10/09 06:44:04 INFO BlockManager: Found block input-0-1444390989334 locally
15/10/09 06:44:04 ERROR Executor: Exception in task 0.0 in stage 24.0 (TID 24)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:44:04 WARN TaskSetManager: Lost task 0.0 in stage 24.0 (TID 24, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:44:04 ERROR TaskSetManager: Task 0 in stage 24.0 failed 1 times; aborting job
15/10/09 06:44:04 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
15/10/09 06:44:04 INFO TaskSchedulerImpl: Cancelling stage 24
15/10/09 06:44:04 INFO DAGScheduler: ResultStage 24 (foreachRDD at Events.scala:18) failed in 0.005 s
15/10/09 06:44:04 INFO DAGScheduler: Job 56 failed: foreachRDD at Events.scala:18, took 0.016802 s
15/10/09 06:44:04 INFO JobScheduler: Finished job streaming job 1444391044000 ms.0 from job set of time 1444391044000 ms
15/10/09 06:44:04 INFO MapPartitionsRDD: Removing RDD 110 from persistence list
15/10/09 06:44:04 INFO JobScheduler: Total delay: 0.025 s for time 1444391044000 ms (execution: 0.019 s)
15/10/09 06:44:04 ERROR JobScheduler: Error running job streaming job 1444391044000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 24.0 failed 1 times, most recent failure: Lost task 0.0 in stage 24.0 (TID 24, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:44:04 INFO WriteAheadLogBackedBlockRDD: Removing RDD 109 from persistence list
15/10/09 06:44:04 INFO BlockManager: Removing RDD 110
15/10/09 06:44:04 INFO BlockManager: Removing RDD 109
15/10/09 06:44:04 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[109] at createStream at Events.scala:13 of time 1444391044000 ms
15/10/09 06:44:04 INFO JobGenerator: Checkpointing graph for time 1444391044000 ms
15/10/09 06:44:04 INFO DStreamGraph: Updating checkpoint data for time 1444391044000 ms
15/10/09 06:44:04 INFO DStreamGraph: Updated checkpoint data for time 1444391044000 ms
15/10/09 06:44:04 INFO CheckpointWriter: Saving checkpoint for time 1444391044000 ms to file 'file:/tmp/checkpoint-1444391044000'
15/10/09 06:44:04 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391039000
15/10/09 06:44:04 INFO CheckpointWriter: Checkpoint for time 1444391044000 ms saved to file 'file:/tmp/checkpoint-1444391044000', took 3615 bytes and 8 ms
15/10/09 06:44:04 INFO DStreamGraph: Clearing checkpoint data for time 1444391044000 ms
15/10/09 06:44:04 INFO DStreamGraph: Cleared checkpoint data for time 1444391044000 ms
15/10/09 06:44:04 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391042000 ms)
15/10/09 06:44:04 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391043000: 
15/10/09 06:44:04 INFO ReceiverTracker: Cleanup old received batch data: 1444391043000 ms
15/10/09 06:44:04 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391043000
15/10/09 06:44:04 INFO InputInfoTracker: remove old batch metadata: 1444391042000 ms
15/10/09 06:44:04 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391043000: 
15/10/09 06:44:04 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391043000
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 15
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.6 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 22
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.6 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 21
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.7 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 20
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.7 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 19
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.7 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 18
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.7 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 17
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_15_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.7 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 16
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.8 MB)
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.8 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 14
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.8 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 13
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.8 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 12
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.8 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 11
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 10
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 9
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 8
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 7
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_5_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 6
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 530.0 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 25
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 530.0 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 24
15/10/09 06:44:04 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:55619 in memory (size: 20.5 KB, free: 530.0 MB)
15/10/09 06:44:04 INFO ContextCleaner: Cleaned accumulator 23
15/10/09 06:44:04 INFO MemoryStore: ensureFreeSpace(74) called with curMem=83049, maxMem=555755765
15/10/09 06:44:04 INFO MemoryStore: Block input-0-1444390989335 stored as bytes in memory (estimated size 74.0 B, free 529.9 MB)
15/10/09 06:44:04 INFO BlockManagerInfo: Added input-0-1444390989335 in memory on localhost:55619 (size: 74.0 B, free: 530.0 MB)
15/10/09 06:44:04 INFO ReliableKafkaReceiver: Committed offset 445 for topic events, partition 0
15/10/09 06:44:04 INFO BlockGenerator: Pushed block input-0-1444391044200
15/10/09 06:44:05 INFO JobScheduler: Added jobs for time 1444391045000 ms
15/10/09 06:44:05 INFO JobGenerator: Checkpointing graph for time 1444391045000 ms
15/10/09 06:44:05 INFO DStreamGraph: Updating checkpoint data for time 1444391045000 ms
15/10/09 06:44:05 INFO DStreamGraph: Updated checkpoint data for time 1444391045000 ms
15/10/09 06:44:05 INFO JobScheduler: Starting job streaming job 1444391045000 ms.0 from job set of time 1444391045000 ms
15/10/09 06:44:05 INFO CheckpointWriter: Saving checkpoint for time 1444391045000 ms to file 'file:/tmp/checkpoint-1444391045000'
15/10/09 06:44:05 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:05 INFO DAGScheduler: Got job 57 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:44:05 INFO DAGScheduler: Final stage: ResultStage 25(foreachRDD at Events.scala:18)
15/10/09 06:44:05 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:44:05 INFO DAGScheduler: Missing parents: List()
15/10/09 06:44:05 INFO DAGScheduler: Submitting ResultStage 25 (MapPartitionsRDD[114] at map at Events.scala:13), which has no missing parents
15/10/09 06:44:05 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391040000.bk
15/10/09 06:44:05 INFO CheckpointWriter: Checkpoint for time 1444391045000 ms saved to file 'file:/tmp/checkpoint-1444391045000', took 3619 bytes and 10 ms
15/10/09 06:44:05 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=83123, maxMem=555755765
15/10/09 06:44:05 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 61.1 KB, free 529.9 MB)
15/10/09 06:44:05 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=145731, maxMem=555755765
15/10/09 06:44:05 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.9 MB)
15/10/09 06:44:05 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 530.0 MB)
15/10/09 06:44:05 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:861
15/10/09 06:44:05 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 25 (MapPartitionsRDD[114] at map at Events.scala:13)
15/10/09 06:44:05 INFO TaskSchedulerImpl: Adding task set 25.0 with 1 tasks
15/10/09 06:44:05 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 25, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:44:05 INFO Executor: Running task 0.0 in stage 25.0 (TID 25)
15/10/09 06:44:05 INFO BlockManager: Found block input-0-1444390989335 locally
15/10/09 06:44:05 ERROR Executor: Exception in task 0.0 in stage 25.0 (TID 25)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:44:05 WARN TaskSetManager: Lost task 0.0 in stage 25.0 (TID 25, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:44:05 ERROR TaskSetManager: Task 0 in stage 25.0 failed 1 times; aborting job
15/10/09 06:44:05 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
15/10/09 06:44:05 INFO TaskSchedulerImpl: Cancelling stage 25
15/10/09 06:44:05 INFO DAGScheduler: ResultStage 25 (foreachRDD at Events.scala:18) failed in 0.005 s
15/10/09 06:44:05 INFO DAGScheduler: Job 57 failed: foreachRDD at Events.scala:18, took 0.014828 s
15/10/09 06:44:05 INFO JobScheduler: Finished job streaming job 1444391045000 ms.0 from job set of time 1444391045000 ms
15/10/09 06:44:05 INFO JobScheduler: Total delay: 0.021 s for time 1444391045000 ms (execution: 0.017 s)
15/10/09 06:44:05 INFO MapPartitionsRDD: Removing RDD 112 from persistence list
15/10/09 06:44:05 ERROR JobScheduler: Error running job streaming job 1444391045000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 25, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:44:05 INFO BlockManager: Removing RDD 112
15/10/09 06:44:05 INFO WriteAheadLogBackedBlockRDD: Removing RDD 111 from persistence list
15/10/09 06:44:05 INFO BlockManager: Removing RDD 111
15/10/09 06:44:05 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[111] at createStream at Events.scala:13 of time 1444391045000 ms
15/10/09 06:44:05 INFO JobGenerator: Checkpointing graph for time 1444391045000 ms
15/10/09 06:44:05 INFO DStreamGraph: Updating checkpoint data for time 1444391045000 ms
15/10/09 06:44:05 INFO DStreamGraph: Updated checkpoint data for time 1444391045000 ms
15/10/09 06:44:05 INFO BlockManagerInfo: Removed input-0-1444390989334 on localhost:55619 in memory (size: 74.0 B, free: 530.0 MB)
15/10/09 06:44:05 INFO CheckpointWriter: Saving checkpoint for time 1444391045000 ms to file 'file:/tmp/checkpoint-1444391045000'
15/10/09 06:44:05 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391040000
15/10/09 06:44:05 INFO CheckpointWriter: Checkpoint for time 1444391045000 ms saved to file 'file:/tmp/checkpoint-1444391045000', took 3615 bytes and 5 ms
15/10/09 06:44:05 INFO DStreamGraph: Clearing checkpoint data for time 1444391045000 ms
15/10/09 06:44:05 INFO DStreamGraph: Cleared checkpoint data for time 1444391045000 ms
15/10/09 06:44:05 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391043000 ms)
15/10/09 06:44:05 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391044000: 
15/10/09 06:44:05 INFO ReceiverTracker: Cleanup old received batch data: 1444391044000 ms
15/10/09 06:44:05 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391044000
15/10/09 06:44:05 INFO InputInfoTracker: remove old batch metadata: 1444391043000 ms
15/10/09 06:44:05 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391044000: 
15/10/09 06:44:05 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391044000
15/10/09 06:44:05 INFO MemoryStore: ensureFreeSpace(74) called with curMem=166617, maxMem=555755765
15/10/09 06:44:05 INFO MemoryStore: Block input-0-1444390989336 stored as bytes in memory (estimated size 74.0 B, free 529.9 MB)
15/10/09 06:44:05 INFO BlockManagerInfo: Added input-0-1444390989336 in memory on localhost:55619 (size: 74.0 B, free: 530.0 MB)
15/10/09 06:44:05 INFO ReliableKafkaReceiver: Committed offset 446 for topic events, partition 0
15/10/09 06:44:05 INFO BlockGenerator: Pushed block input-0-1444391045200
15/10/09 06:44:06 INFO JobScheduler: Added jobs for time 1444391046000 ms
15/10/09 06:44:06 INFO JobGenerator: Checkpointing graph for time 1444391046000 ms
15/10/09 06:44:06 INFO DStreamGraph: Updating checkpoint data for time 1444391046000 ms
15/10/09 06:44:06 INFO DStreamGraph: Updated checkpoint data for time 1444391046000 ms
15/10/09 06:44:06 INFO JobScheduler: Starting job streaming job 1444391046000 ms.0 from job set of time 1444391046000 ms
15/10/09 06:44:06 INFO CheckpointWriter: Saving checkpoint for time 1444391046000 ms to file 'file:/tmp/checkpoint-1444391046000'
15/10/09 06:44:06 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:06 INFO DAGScheduler: Got job 58 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:44:06 INFO DAGScheduler: Final stage: ResultStage 26(foreachRDD at Events.scala:18)
15/10/09 06:44:06 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:44:06 INFO DAGScheduler: Missing parents: List()
15/10/09 06:44:06 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[116] at map at Events.scala:13), which has no missing parents
15/10/09 06:44:06 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391041000.bk
15/10/09 06:44:06 INFO CheckpointWriter: Checkpoint for time 1444391046000 ms saved to file 'file:/tmp/checkpoint-1444391046000', took 3619 bytes and 7 ms
15/10/09 06:44:06 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=166691, maxMem=555755765
15/10/09 06:44:06 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 61.1 KB, free 529.8 MB)
15/10/09 06:44:06 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=229299, maxMem=555755765
15/10/09 06:44:06 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.8 MB)
15/10/09 06:44:06 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 530.0 MB)
15/10/09 06:44:06 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:861
15/10/09 06:44:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[116] at map at Events.scala:13)
15/10/09 06:44:06 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
15/10/09 06:44:06 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 26, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:44:06 INFO Executor: Running task 0.0 in stage 26.0 (TID 26)
15/10/09 06:44:06 INFO BlockManager: Found block input-0-1444390989336 locally
15/10/09 06:44:06 ERROR Executor: Exception in task 0.0 in stage 26.0 (TID 26)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:44:06 WARN TaskSetManager: Lost task 0.0 in stage 26.0 (TID 26, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:44:06 ERROR TaskSetManager: Task 0 in stage 26.0 failed 1 times; aborting job
15/10/09 06:44:06 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
15/10/09 06:44:06 INFO TaskSchedulerImpl: Cancelling stage 26
15/10/09 06:44:06 INFO DAGScheduler: ResultStage 26 (foreachRDD at Events.scala:18) failed in 0.005 s
15/10/09 06:44:06 INFO DAGScheduler: Job 58 failed: foreachRDD at Events.scala:18, took 0.014260 s
15/10/09 06:44:06 INFO JobScheduler: Finished job streaming job 1444391046000 ms.0 from job set of time 1444391046000 ms
15/10/09 06:44:06 INFO JobScheduler: Total delay: 0.019 s for time 1444391046000 ms (execution: 0.016 s)
15/10/09 06:44:06 INFO MapPartitionsRDD: Removing RDD 114 from persistence list
15/10/09 06:44:06 ERROR JobScheduler: Error running job streaming job 1444391046000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 26, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:44:06 INFO WriteAheadLogBackedBlockRDD: Removing RDD 113 from persistence list
15/10/09 06:44:06 INFO BlockManager: Removing RDD 114
15/10/09 06:44:06 INFO BlockManager: Removing RDD 113
15/10/09 06:44:06 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[113] at createStream at Events.scala:13 of time 1444391046000 ms
15/10/09 06:44:06 INFO JobGenerator: Checkpointing graph for time 1444391046000 ms
15/10/09 06:44:06 INFO DStreamGraph: Updating checkpoint data for time 1444391046000 ms
15/10/09 06:44:06 INFO DStreamGraph: Updated checkpoint data for time 1444391046000 ms
15/10/09 06:44:06 INFO BlockManagerInfo: Removed input-0-1444390989335 on localhost:55619 in memory (size: 74.0 B, free: 530.0 MB)
15/10/09 06:44:06 INFO CheckpointWriter: Saving checkpoint for time 1444391046000 ms to file 'file:/tmp/checkpoint-1444391046000'
15/10/09 06:44:06 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391041000
15/10/09 06:44:06 INFO CheckpointWriter: Checkpoint for time 1444391046000 ms saved to file 'file:/tmp/checkpoint-1444391046000', took 3615 bytes and 5 ms
15/10/09 06:44:06 INFO DStreamGraph: Clearing checkpoint data for time 1444391046000 ms
15/10/09 06:44:06 INFO DStreamGraph: Cleared checkpoint data for time 1444391046000 ms
15/10/09 06:44:06 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391044000 ms)
15/10/09 06:44:06 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391045000: 
15/10/09 06:44:06 INFO ReceiverTracker: Cleanup old received batch data: 1444391045000 ms
15/10/09 06:44:06 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391045000
15/10/09 06:44:06 INFO InputInfoTracker: remove old batch metadata: 1444391044000 ms
15/10/09 06:44:06 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391045000: 
15/10/09 06:44:06 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391045000
15/10/09 06:44:06 INFO MemoryStore: ensureFreeSpace(74) called with curMem=250185, maxMem=555755765
15/10/09 06:44:06 INFO MemoryStore: Block input-0-1444390989337 stored as bytes in memory (estimated size 74.0 B, free 529.8 MB)
15/10/09 06:44:06 INFO BlockManagerInfo: Added input-0-1444390989337 in memory on localhost:55619 (size: 74.0 B, free: 530.0 MB)
15/10/09 06:44:06 INFO ReliableKafkaReceiver: Committed offset 447 for topic events, partition 0
15/10/09 06:44:06 INFO BlockGenerator: Pushed block input-0-1444391046200
15/10/09 06:44:07 INFO JobScheduler: Added jobs for time 1444391047000 ms
15/10/09 06:44:07 INFO JobGenerator: Checkpointing graph for time 1444391047000 ms
15/10/09 06:44:07 INFO DStreamGraph: Updating checkpoint data for time 1444391047000 ms
15/10/09 06:44:07 INFO DStreamGraph: Updated checkpoint data for time 1444391047000 ms
15/10/09 06:44:07 INFO JobScheduler: Starting job streaming job 1444391047000 ms.0 from job set of time 1444391047000 ms
15/10/09 06:44:07 INFO CheckpointWriter: Saving checkpoint for time 1444391047000 ms to file 'file:/tmp/checkpoint-1444391047000'
15/10/09 06:44:07 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:07 INFO DAGScheduler: Got job 59 (foreachRDD at Events.scala:18) with 1 output partitions
15/10/09 06:44:07 INFO DAGScheduler: Final stage: ResultStage 27(foreachRDD at Events.scala:18)
15/10/09 06:44:07 INFO DAGScheduler: Parents of final stage: List()
15/10/09 06:44:07 INFO DAGScheduler: Missing parents: List()
15/10/09 06:44:07 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[118] at map at Events.scala:13), which has no missing parents
15/10/09 06:44:07 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391042000.bk
15/10/09 06:44:07 INFO CheckpointWriter: Checkpoint for time 1444391047000 ms saved to file 'file:/tmp/checkpoint-1444391047000', took 3619 bytes and 8 ms
15/10/09 06:44:07 INFO MemoryStore: ensureFreeSpace(62608) called with curMem=250259, maxMem=555755765
15/10/09 06:44:07 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 61.1 KB, free 529.7 MB)
15/10/09 06:44:07 INFO MemoryStore: ensureFreeSpace(20960) called with curMem=312867, maxMem=555755765
15/10/09 06:44:07 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 20.5 KB, free 529.7 MB)
15/10/09 06:44:07 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:55619 (size: 20.5 KB, free: 529.9 MB)
15/10/09 06:44:07 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:861
15/10/09 06:44:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[118] at map at Events.scala:13)
15/10/09 06:44:07 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
15/10/09 06:44:07 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 27, localhost, NODE_LOCAL, 2467 bytes)
15/10/09 06:44:07 INFO Executor: Running task 0.0 in stage 27.0 (TID 27)
15/10/09 06:44:07 INFO BlockManager: Found block input-0-1444390989337 locally
15/10/09 06:44:07 ERROR Executor: Exception in task 0.0 in stage 27.0 (TID 27)
java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/10/09 06:44:07 WARN TaskSetManager: Lost task 0.0 in stage 27.0 (TID 27, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

15/10/09 06:44:07 ERROR TaskSetManager: Task 0 in stage 27.0 failed 1 times; aborting job
15/10/09 06:44:07 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
15/10/09 06:44:07 INFO TaskSchedulerImpl: Cancelling stage 27
15/10/09 06:44:07 INFO DAGScheduler: ResultStage 27 (foreachRDD at Events.scala:18) failed in 0.005 s
15/10/09 06:44:07 INFO DAGScheduler: Job 59 failed: foreachRDD at Events.scala:18, took 0.014703 s
15/10/09 06:44:07 INFO JobScheduler: Finished job streaming job 1444391047000 ms.0 from job set of time 1444391047000 ms
15/10/09 06:44:07 INFO JobScheduler: Total delay: 0.021 s for time 1444391047000 ms (execution: 0.017 s)
15/10/09 06:44:07 INFO MapPartitionsRDD: Removing RDD 116 from persistence list
15/10/09 06:44:07 ERROR JobScheduler: Error running job streaming job 1444391047000 ms.0
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 27.0 failed 1 times, most recent failure: Lost task 0.0 in stage 27.0 (TID 27, localhost): java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1283)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1271)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1270)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1270)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:697)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:697)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1496)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1458)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:567)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1822)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1835)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1848)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:896)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:896)
	at Events$$anonfun$createContext$1.apply(Events.scala:20)
	at Events$$anonfun$createContext$1.apply(Events.scala:18)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.DStream$$anonfun$foreachRDD$1$$anonfun$apply$mcV$sp$3.apply(DStream.scala:631)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:399)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply$mcV$sp(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1.apply(ForEachDStream.scala:40)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.streaming.scheduler.Job.run(Job.scala:34)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply$mcV$sp(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler$$anonfun$run$1.apply(JobScheduler.scala:218)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
	at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:217)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.Exception
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:23)
	at Events$$anonfun$createContext$1$$anonfun$apply$1.apply(Events.scala:20)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1$$anonfun$apply$29.apply(RDD.scala:898)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1848)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	... 3 more
15/10/09 06:44:07 INFO WriteAheadLogBackedBlockRDD: Removing RDD 115 from persistence list
15/10/09 06:44:07 INFO BlockManager: Removing RDD 116
15/10/09 06:44:07 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[115] at createStream at Events.scala:13 of time 1444391047000 ms
15/10/09 06:44:07 INFO BlockManager: Removing RDD 115
15/10/09 06:44:07 INFO JobGenerator: Checkpointing graph for time 1444391047000 ms
15/10/09 06:44:07 INFO DStreamGraph: Updating checkpoint data for time 1444391047000 ms
15/10/09 06:44:07 INFO DStreamGraph: Updated checkpoint data for time 1444391047000 ms
15/10/09 06:44:07 INFO BlockManagerInfo: Removed input-0-1444390989336 on localhost:55619 in memory (size: 74.0 B, free: 529.9 MB)
15/10/09 06:44:07 INFO CheckpointWriter: Saving checkpoint for time 1444391047000 ms to file 'file:/tmp/checkpoint-1444391047000'
15/10/09 06:44:07 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391042000
15/10/09 06:44:07 INFO CheckpointWriter: Checkpoint for time 1444391047000 ms saved to file 'file:/tmp/checkpoint-1444391047000', took 3615 bytes and 5 ms
15/10/09 06:44:07 INFO DStreamGraph: Clearing checkpoint data for time 1444391047000 ms
15/10/09 06:44:07 INFO DStreamGraph: Cleared checkpoint data for time 1444391047000 ms
15/10/09 06:44:07 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391045000 ms)
15/10/09 06:44:07 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391046000: 
15/10/09 06:44:07 INFO ReceiverTracker: Cleanup old received batch data: 1444391046000 ms
15/10/09 06:44:07 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391046000
15/10/09 06:44:07 INFO InputInfoTracker: remove old batch metadata: 1444391045000 ms
15/10/09 06:44:07 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391046000: 
15/10/09 06:44:07 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391046000
15/10/09 06:44:08 INFO JobScheduler: Added jobs for time 1444391048000 ms
15/10/09 06:44:08 INFO JobGenerator: Checkpointing graph for time 1444391048000 ms
15/10/09 06:44:08 INFO DStreamGraph: Updating checkpoint data for time 1444391048000 ms
15/10/09 06:44:08 INFO DStreamGraph: Updated checkpoint data for time 1444391048000 ms
15/10/09 06:44:08 INFO JobScheduler: Starting job streaming job 1444391048000 ms.0 from job set of time 1444391048000 ms
15/10/09 06:44:08 INFO CheckpointWriter: Saving checkpoint for time 1444391048000 ms to file 'file:/tmp/checkpoint-1444391048000'
15/10/09 06:44:08 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:08 INFO DAGScheduler: Job 60 finished: foreachRDD at Events.scala:18, took 0.000020 s
15/10/09 06:44:08 INFO JobScheduler: Finished job streaming job 1444391048000 ms.0 from job set of time 1444391048000 ms
15/10/09 06:44:08 INFO JobScheduler: Total delay: 0.009 s for time 1444391048000 ms (execution: 0.002 s)
15/10/09 06:44:08 INFO MapPartitionsRDD: Removing RDD 118 from persistence list
15/10/09 06:44:08 INFO BlockManager: Removing RDD 118
15/10/09 06:44:08 INFO WriteAheadLogBackedBlockRDD: Removing RDD 117 from persistence list
15/10/09 06:44:08 INFO BlockManager: Removing RDD 117
15/10/09 06:44:08 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[117] at createStream at Events.scala:13 of time 1444391048000 ms
15/10/09 06:44:08 INFO JobGenerator: Checkpointing graph for time 1444391048000 ms
15/10/09 06:44:08 INFO DStreamGraph: Updating checkpoint data for time 1444391048000 ms
15/10/09 06:44:08 INFO DStreamGraph: Updated checkpoint data for time 1444391048000 ms
15/10/09 06:44:08 INFO BlockManagerInfo: Removed input-0-1444390989337 on localhost:55619 in memory (size: 74.0 B, free: 529.9 MB)
15/10/09 06:44:08 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391043000.bk
15/10/09 06:44:08 INFO CheckpointWriter: Checkpoint for time 1444391048000 ms saved to file 'file:/tmp/checkpoint-1444391048000', took 3619 bytes and 8 ms
15/10/09 06:44:08 INFO CheckpointWriter: Saving checkpoint for time 1444391048000 ms to file 'file:/tmp/checkpoint-1444391048000'
15/10/09 06:44:08 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391043000
15/10/09 06:44:08 INFO CheckpointWriter: Checkpoint for time 1444391048000 ms saved to file 'file:/tmp/checkpoint-1444391048000', took 3615 bytes and 6 ms
15/10/09 06:44:08 INFO DStreamGraph: Clearing checkpoint data for time 1444391048000 ms
15/10/09 06:44:08 INFO DStreamGraph: Cleared checkpoint data for time 1444391048000 ms
15/10/09 06:44:08 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391046000 ms)
15/10/09 06:44:08 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391047000: 
15/10/09 06:44:08 INFO ReceiverTracker: Cleanup old received batch data: 1444391047000 ms
15/10/09 06:44:08 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391047000
15/10/09 06:44:08 INFO InputInfoTracker: remove old batch metadata: 1444391046000 ms
15/10/09 06:44:08 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391047000: 
15/10/09 06:44:08 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391047000
15/10/09 06:44:09 INFO JobScheduler: Added jobs for time 1444391049000 ms
15/10/09 06:44:09 INFO JobGenerator: Checkpointing graph for time 1444391049000 ms
15/10/09 06:44:09 INFO DStreamGraph: Updating checkpoint data for time 1444391049000 ms
15/10/09 06:44:09 INFO DStreamGraph: Updated checkpoint data for time 1444391049000 ms
15/10/09 06:44:09 INFO JobScheduler: Starting job streaming job 1444391049000 ms.0 from job set of time 1444391049000 ms
15/10/09 06:44:09 INFO CheckpointWriter: Saving checkpoint for time 1444391049000 ms to file 'file:/tmp/checkpoint-1444391049000'
15/10/09 06:44:09 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:09 INFO DAGScheduler: Job 61 finished: foreachRDD at Events.scala:18, took 0.000036 s
15/10/09 06:44:09 INFO JobScheduler: Finished job streaming job 1444391049000 ms.0 from job set of time 1444391049000 ms
15/10/09 06:44:09 INFO JobScheduler: Total delay: 0.005 s for time 1444391049000 ms (execution: 0.003 s)
15/10/09 06:44:09 INFO MapPartitionsRDD: Removing RDD 120 from persistence list
15/10/09 06:44:09 INFO BlockManager: Removing RDD 120
15/10/09 06:44:09 INFO WriteAheadLogBackedBlockRDD: Removing RDD 119 from persistence list
15/10/09 06:44:09 INFO BlockManager: Removing RDD 119
15/10/09 06:44:09 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[119] at createStream at Events.scala:13 of time 1444391049000 ms
15/10/09 06:44:09 INFO JobGenerator: Checkpointing graph for time 1444391049000 ms
15/10/09 06:44:09 INFO DStreamGraph: Updating checkpoint data for time 1444391049000 ms
15/10/09 06:44:09 INFO DStreamGraph: Updated checkpoint data for time 1444391049000 ms
15/10/09 06:44:09 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391044000.bk
15/10/09 06:44:09 INFO CheckpointWriter: Checkpoint for time 1444391049000 ms saved to file 'file:/tmp/checkpoint-1444391049000', took 3619 bytes and 8 ms
15/10/09 06:44:09 INFO CheckpointWriter: Saving checkpoint for time 1444391049000 ms to file 'file:/tmp/checkpoint-1444391049000'
15/10/09 06:44:09 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391044000
15/10/09 06:44:09 INFO CheckpointWriter: Checkpoint for time 1444391049000 ms saved to file 'file:/tmp/checkpoint-1444391049000', took 3615 bytes and 7 ms
15/10/09 06:44:09 INFO DStreamGraph: Clearing checkpoint data for time 1444391049000 ms
15/10/09 06:44:09 INFO DStreamGraph: Cleared checkpoint data for time 1444391049000 ms
15/10/09 06:44:09 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391047000 ms)
15/10/09 06:44:09 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391048000: 
15/10/09 06:44:09 INFO ReceiverTracker: Cleanup old received batch data: 1444391048000 ms
15/10/09 06:44:09 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391048000
15/10/09 06:44:09 INFO InputInfoTracker: remove old batch metadata: 1444391047000 ms
15/10/09 06:44:09 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391048000: 
15/10/09 06:44:09 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391048000
15/10/09 06:44:10 INFO JobScheduler: Added jobs for time 1444391050000 ms
15/10/09 06:44:10 INFO JobGenerator: Checkpointing graph for time 1444391050000 ms
15/10/09 06:44:10 INFO DStreamGraph: Updating checkpoint data for time 1444391050000 ms
15/10/09 06:44:10 INFO DStreamGraph: Updated checkpoint data for time 1444391050000 ms
15/10/09 06:44:10 INFO JobScheduler: Starting job streaming job 1444391050000 ms.0 from job set of time 1444391050000 ms
15/10/09 06:44:10 INFO CheckpointWriter: Saving checkpoint for time 1444391050000 ms to file 'file:/tmp/checkpoint-1444391050000'
15/10/09 06:44:10 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:10 INFO DAGScheduler: Job 62 finished: foreachRDD at Events.scala:18, took 0.000052 s
15/10/09 06:44:10 INFO JobScheduler: Finished job streaming job 1444391050000 ms.0 from job set of time 1444391050000 ms
15/10/09 06:44:10 INFO JobScheduler: Total delay: 0.009 s for time 1444391050000 ms (execution: 0.002 s)
15/10/09 06:44:10 INFO MapPartitionsRDD: Removing RDD 122 from persistence list
15/10/09 06:44:10 INFO WriteAheadLogBackedBlockRDD: Removing RDD 121 from persistence list
15/10/09 06:44:10 INFO BlockManager: Removing RDD 122
15/10/09 06:44:10 INFO BlockManager: Removing RDD 121
15/10/09 06:44:10 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[121] at createStream at Events.scala:13 of time 1444391050000 ms
15/10/09 06:44:10 INFO JobGenerator: Checkpointing graph for time 1444391050000 ms
15/10/09 06:44:10 INFO DStreamGraph: Updating checkpoint data for time 1444391050000 ms
15/10/09 06:44:10 INFO DStreamGraph: Updated checkpoint data for time 1444391050000 ms
15/10/09 06:44:10 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391045000.bk
15/10/09 06:44:10 INFO CheckpointWriter: Checkpoint for time 1444391050000 ms saved to file 'file:/tmp/checkpoint-1444391050000', took 3619 bytes and 8 ms
15/10/09 06:44:10 INFO CheckpointWriter: Saving checkpoint for time 1444391050000 ms to file 'file:/tmp/checkpoint-1444391050000'
15/10/09 06:44:10 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391045000
15/10/09 06:44:10 INFO CheckpointWriter: Checkpoint for time 1444391050000 ms saved to file 'file:/tmp/checkpoint-1444391050000', took 3615 bytes and 6 ms
15/10/09 06:44:10 INFO DStreamGraph: Clearing checkpoint data for time 1444391050000 ms
15/10/09 06:44:10 INFO DStreamGraph: Cleared checkpoint data for time 1444391050000 ms
15/10/09 06:44:10 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391048000 ms)
15/10/09 06:44:10 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391049000: 
15/10/09 06:44:10 INFO ReceiverTracker: Cleanup old received batch data: 1444391049000 ms
15/10/09 06:44:10 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391049000
15/10/09 06:44:10 INFO InputInfoTracker: remove old batch metadata: 1444391048000 ms
15/10/09 06:44:10 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391049000: 
15/10/09 06:44:10 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391049000
15/10/09 06:44:11 INFO JobScheduler: Added jobs for time 1444391051000 ms
15/10/09 06:44:11 INFO JobGenerator: Checkpointing graph for time 1444391051000 ms
15/10/09 06:44:11 INFO DStreamGraph: Updating checkpoint data for time 1444391051000 ms
15/10/09 06:44:11 INFO JobScheduler: Starting job streaming job 1444391051000 ms.0 from job set of time 1444391051000 ms
15/10/09 06:44:11 INFO DStreamGraph: Updated checkpoint data for time 1444391051000 ms
15/10/09 06:44:11 INFO CheckpointWriter: Saving checkpoint for time 1444391051000 ms to file 'file:/tmp/checkpoint-1444391051000'
15/10/09 06:44:11 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:11 INFO DAGScheduler: Job 63 finished: foreachRDD at Events.scala:18, took 0.000019 s
15/10/09 06:44:11 INFO JobScheduler: Finished job streaming job 1444391051000 ms.0 from job set of time 1444391051000 ms
15/10/09 06:44:11 INFO JobScheduler: Total delay: 0.009 s for time 1444391051000 ms (execution: 0.002 s)
15/10/09 06:44:11 INFO MapPartitionsRDD: Removing RDD 124 from persistence list
15/10/09 06:44:11 INFO BlockManager: Removing RDD 124
15/10/09 06:44:11 INFO WriteAheadLogBackedBlockRDD: Removing RDD 123 from persistence list
15/10/09 06:44:11 INFO BlockManager: Removing RDD 123
15/10/09 06:44:11 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[123] at createStream at Events.scala:13 of time 1444391051000 ms
15/10/09 06:44:11 INFO JobGenerator: Checkpointing graph for time 1444391051000 ms
15/10/09 06:44:11 INFO DStreamGraph: Updating checkpoint data for time 1444391051000 ms
15/10/09 06:44:11 INFO DStreamGraph: Updated checkpoint data for time 1444391051000 ms
15/10/09 06:44:11 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391046000.bk
15/10/09 06:44:11 INFO CheckpointWriter: Checkpoint for time 1444391051000 ms saved to file 'file:/tmp/checkpoint-1444391051000', took 3619 bytes and 8 ms
15/10/09 06:44:11 INFO CheckpointWriter: Saving checkpoint for time 1444391051000 ms to file 'file:/tmp/checkpoint-1444391051000'
15/10/09 06:44:11 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391046000
15/10/09 06:44:11 INFO CheckpointWriter: Checkpoint for time 1444391051000 ms saved to file 'file:/tmp/checkpoint-1444391051000', took 3615 bytes and 6 ms
15/10/09 06:44:11 INFO DStreamGraph: Clearing checkpoint data for time 1444391051000 ms
15/10/09 06:44:11 INFO DStreamGraph: Cleared checkpoint data for time 1444391051000 ms
15/10/09 06:44:11 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391049000 ms)
15/10/09 06:44:11 INFO WriteAheadLogManager : Attempting to clear 1 old log files in file:/tmp/receivedBlockMetadata older than 1444391050000: file:/tmp/receivedBlockMetadata/log-1444390989004-1444391049004
15/10/09 06:44:11 INFO ReceiverTracker: Cleanup old received batch data: 1444391050000 ms
15/10/09 06:44:11 INFO InputInfoTracker: remove old batch metadata: 1444391049000 ms
15/10/09 06:44:11 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391050000: 
15/10/09 06:44:11 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391050000
15/10/09 06:44:11 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391050000
15/10/09 06:44:12 INFO JobScheduler: Added jobs for time 1444391052000 ms
15/10/09 06:44:12 INFO JobGenerator: Checkpointing graph for time 1444391052000 ms
15/10/09 06:44:12 INFO DStreamGraph: Updating checkpoint data for time 1444391052000 ms
15/10/09 06:44:12 INFO JobScheduler: Starting job streaming job 1444391052000 ms.0 from job set of time 1444391052000 ms
15/10/09 06:44:12 INFO DStreamGraph: Updated checkpoint data for time 1444391052000 ms
15/10/09 06:44:12 INFO CheckpointWriter: Saving checkpoint for time 1444391052000 ms to file 'file:/tmp/checkpoint-1444391052000'
15/10/09 06:44:12 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:12 INFO DAGScheduler: Job 64 finished: foreachRDD at Events.scala:18, took 0.000023 s
15/10/09 06:44:12 INFO JobScheduler: Finished job streaming job 1444391052000 ms.0 from job set of time 1444391052000 ms
15/10/09 06:44:12 INFO JobScheduler: Total delay: 0.007 s for time 1444391052000 ms (execution: 0.003 s)
15/10/09 06:44:12 INFO MapPartitionsRDD: Removing RDD 126 from persistence list
15/10/09 06:44:12 INFO BlockManager: Removing RDD 126
15/10/09 06:44:12 INFO WriteAheadLogBackedBlockRDD: Removing RDD 125 from persistence list
15/10/09 06:44:12 INFO BlockManager: Removing RDD 125
15/10/09 06:44:12 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[125] at createStream at Events.scala:13 of time 1444391052000 ms
15/10/09 06:44:12 INFO JobGenerator: Checkpointing graph for time 1444391052000 ms
15/10/09 06:44:12 INFO DStreamGraph: Updating checkpoint data for time 1444391052000 ms
15/10/09 06:44:12 INFO DStreamGraph: Updated checkpoint data for time 1444391052000 ms
15/10/09 06:44:12 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391047000.bk
15/10/09 06:44:12 INFO CheckpointWriter: Checkpoint for time 1444391052000 ms saved to file 'file:/tmp/checkpoint-1444391052000', took 3619 bytes and 9 ms
15/10/09 06:44:12 INFO CheckpointWriter: Saving checkpoint for time 1444391052000 ms to file 'file:/tmp/checkpoint-1444391052000'
15/10/09 06:44:12 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391047000
15/10/09 06:44:12 INFO CheckpointWriter: Checkpoint for time 1444391052000 ms saved to file 'file:/tmp/checkpoint-1444391052000', took 3615 bytes and 6 ms
15/10/09 06:44:12 INFO DStreamGraph: Clearing checkpoint data for time 1444391052000 ms
15/10/09 06:44:12 INFO DStreamGraph: Cleared checkpoint data for time 1444391052000 ms
15/10/09 06:44:12 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391050000 ms)
15/10/09 06:44:12 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391051000: 
15/10/09 06:44:12 INFO ReceiverTracker: Cleanup old received batch data: 1444391051000 ms
15/10/09 06:44:12 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391051000
15/10/09 06:44:12 INFO InputInfoTracker: remove old batch metadata: 1444391050000 ms
15/10/09 06:44:12 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391051000: 
15/10/09 06:44:12 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391051000
15/10/09 06:44:13 INFO JobScheduler: Added jobs for time 1444391053000 ms
15/10/09 06:44:13 INFO JobGenerator: Checkpointing graph for time 1444391053000 ms
15/10/09 06:44:13 INFO DStreamGraph: Updating checkpoint data for time 1444391053000 ms
15/10/09 06:44:13 INFO JobScheduler: Starting job streaming job 1444391053000 ms.0 from job set of time 1444391053000 ms
15/10/09 06:44:13 INFO DStreamGraph: Updated checkpoint data for time 1444391053000 ms
15/10/09 06:44:13 INFO CheckpointWriter: Saving checkpoint for time 1444391053000 ms to file 'file:/tmp/checkpoint-1444391053000'
15/10/09 06:44:13 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:13 INFO DAGScheduler: Job 65 finished: foreachRDD at Events.scala:18, took 0.000020 s
15/10/09 06:44:13 INFO JobScheduler: Finished job streaming job 1444391053000 ms.0 from job set of time 1444391053000 ms
15/10/09 06:44:13 INFO JobScheduler: Total delay: 0.009 s for time 1444391053000 ms (execution: 0.003 s)
15/10/09 06:44:13 INFO MapPartitionsRDD: Removing RDD 128 from persistence list
15/10/09 06:44:13 INFO BlockManager: Removing RDD 128
15/10/09 06:44:13 INFO WriteAheadLogBackedBlockRDD: Removing RDD 127 from persistence list
15/10/09 06:44:13 INFO BlockManager: Removing RDD 127
15/10/09 06:44:13 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[127] at createStream at Events.scala:13 of time 1444391053000 ms
15/10/09 06:44:13 INFO JobGenerator: Checkpointing graph for time 1444391053000 ms
15/10/09 06:44:13 INFO DStreamGraph: Updating checkpoint data for time 1444391053000 ms
15/10/09 06:44:13 INFO DStreamGraph: Updated checkpoint data for time 1444391053000 ms
15/10/09 06:44:13 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391048000.bk
15/10/09 06:44:13 INFO CheckpointWriter: Checkpoint for time 1444391053000 ms saved to file 'file:/tmp/checkpoint-1444391053000', took 3619 bytes and 8 ms
15/10/09 06:44:13 INFO CheckpointWriter: Saving checkpoint for time 1444391053000 ms to file 'file:/tmp/checkpoint-1444391053000'
15/10/09 06:44:13 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391048000
15/10/09 06:44:13 INFO CheckpointWriter: Checkpoint for time 1444391053000 ms saved to file 'file:/tmp/checkpoint-1444391053000', took 3615 bytes and 6 ms
15/10/09 06:44:13 INFO DStreamGraph: Clearing checkpoint data for time 1444391053000 ms
15/10/09 06:44:13 INFO DStreamGraph: Cleared checkpoint data for time 1444391053000 ms
15/10/09 06:44:13 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391051000 ms)
15/10/09 06:44:13 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391052000: 
15/10/09 06:44:13 INFO ReceiverTracker: Cleanup old received batch data: 1444391052000 ms
15/10/09 06:44:13 INFO InputInfoTracker: remove old batch metadata: 1444391051000 ms
15/10/09 06:44:13 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391052000
15/10/09 06:44:13 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391052000: 
15/10/09 06:44:13 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391052000
15/10/09 06:44:14 INFO JobScheduler: Added jobs for time 1444391054000 ms
15/10/09 06:44:14 INFO JobGenerator: Checkpointing graph for time 1444391054000 ms
15/10/09 06:44:14 INFO DStreamGraph: Updating checkpoint data for time 1444391054000 ms
15/10/09 06:44:14 INFO DStreamGraph: Updated checkpoint data for time 1444391054000 ms
15/10/09 06:44:14 INFO JobScheduler: Starting job streaming job 1444391054000 ms.0 from job set of time 1444391054000 ms
15/10/09 06:44:14 INFO CheckpointWriter: Saving checkpoint for time 1444391054000 ms to file 'file:/tmp/checkpoint-1444391054000'
15/10/09 06:44:14 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:14 INFO DAGScheduler: Job 66 finished: foreachRDD at Events.scala:18, took 0.000021 s
15/10/09 06:44:14 INFO JobScheduler: Finished job streaming job 1444391054000 ms.0 from job set of time 1444391054000 ms
15/10/09 06:44:14 INFO JobScheduler: Total delay: 0.010 s for time 1444391054000 ms (execution: 0.003 s)
15/10/09 06:44:14 INFO MapPartitionsRDD: Removing RDD 130 from persistence list
15/10/09 06:44:14 INFO BlockManager: Removing RDD 130
15/10/09 06:44:14 INFO WriteAheadLogBackedBlockRDD: Removing RDD 129 from persistence list
15/10/09 06:44:14 INFO BlockManager: Removing RDD 129
15/10/09 06:44:14 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[129] at createStream at Events.scala:13 of time 1444391054000 ms
15/10/09 06:44:14 INFO JobGenerator: Checkpointing graph for time 1444391054000 ms
15/10/09 06:44:14 INFO DStreamGraph: Updating checkpoint data for time 1444391054000 ms
15/10/09 06:44:14 INFO DStreamGraph: Updated checkpoint data for time 1444391054000 ms
15/10/09 06:44:14 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391049000.bk
15/10/09 06:44:14 INFO CheckpointWriter: Checkpoint for time 1444391054000 ms saved to file 'file:/tmp/checkpoint-1444391054000', took 3619 bytes and 8 ms
15/10/09 06:44:14 INFO CheckpointWriter: Saving checkpoint for time 1444391054000 ms to file 'file:/tmp/checkpoint-1444391054000'
15/10/09 06:44:14 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391049000
15/10/09 06:44:14 INFO CheckpointWriter: Checkpoint for time 1444391054000 ms saved to file 'file:/tmp/checkpoint-1444391054000', took 3615 bytes and 6 ms
15/10/09 06:44:14 INFO DStreamGraph: Clearing checkpoint data for time 1444391054000 ms
15/10/09 06:44:14 INFO DStreamGraph: Cleared checkpoint data for time 1444391054000 ms
15/10/09 06:44:14 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391052000 ms)
15/10/09 06:44:14 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391053000: 
15/10/09 06:44:14 INFO ReceiverTracker: Cleanup old received batch data: 1444391053000 ms
15/10/09 06:44:14 INFO InputInfoTracker: remove old batch metadata: 1444391052000 ms
15/10/09 06:44:14 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391053000
15/10/09 06:44:14 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391053000: 
15/10/09 06:44:14 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391053000
15/10/09 06:44:15 INFO JobScheduler: Added jobs for time 1444391055000 ms
15/10/09 06:44:15 INFO JobGenerator: Checkpointing graph for time 1444391055000 ms
15/10/09 06:44:15 INFO DStreamGraph: Updating checkpoint data for time 1444391055000 ms
15/10/09 06:44:15 INFO DStreamGraph: Updated checkpoint data for time 1444391055000 ms
15/10/09 06:44:15 INFO JobScheduler: Starting job streaming job 1444391055000 ms.0 from job set of time 1444391055000 ms
15/10/09 06:44:15 INFO CheckpointWriter: Saving checkpoint for time 1444391055000 ms to file 'file:/tmp/checkpoint-1444391055000'
15/10/09 06:44:15 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:15 INFO DAGScheduler: Job 67 finished: foreachRDD at Events.scala:18, took 0.000021 s
15/10/09 06:44:15 INFO JobScheduler: Finished job streaming job 1444391055000 ms.0 from job set of time 1444391055000 ms
15/10/09 06:44:15 INFO JobScheduler: Total delay: 0.008 s for time 1444391055000 ms (execution: 0.002 s)
15/10/09 06:44:15 INFO MapPartitionsRDD: Removing RDD 132 from persistence list
15/10/09 06:44:15 INFO BlockManager: Removing RDD 132
15/10/09 06:44:15 INFO WriteAheadLogBackedBlockRDD: Removing RDD 131 from persistence list
15/10/09 06:44:15 INFO BlockManager: Removing RDD 131
15/10/09 06:44:15 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[131] at createStream at Events.scala:13 of time 1444391055000 ms
15/10/09 06:44:15 INFO JobGenerator: Checkpointing graph for time 1444391055000 ms
15/10/09 06:44:15 INFO DStreamGraph: Updating checkpoint data for time 1444391055000 ms
15/10/09 06:44:15 INFO DStreamGraph: Updated checkpoint data for time 1444391055000 ms
15/10/09 06:44:15 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391050000.bk
15/10/09 06:44:15 INFO CheckpointWriter: Checkpoint for time 1444391055000 ms saved to file 'file:/tmp/checkpoint-1444391055000', took 3619 bytes and 9 ms
15/10/09 06:44:15 INFO CheckpointWriter: Saving checkpoint for time 1444391055000 ms to file 'file:/tmp/checkpoint-1444391055000'
15/10/09 06:44:15 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391050000
15/10/09 06:44:15 INFO CheckpointWriter: Checkpoint for time 1444391055000 ms saved to file 'file:/tmp/checkpoint-1444391055000', took 3615 bytes and 6 ms
15/10/09 06:44:15 INFO DStreamGraph: Clearing checkpoint data for time 1444391055000 ms
15/10/09 06:44:15 INFO DStreamGraph: Cleared checkpoint data for time 1444391055000 ms
15/10/09 06:44:15 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391053000 ms)
15/10/09 06:44:15 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391054000: 
15/10/09 06:44:15 INFO ReceiverTracker: Cleanup old received batch data: 1444391054000 ms
15/10/09 06:44:15 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391054000
15/10/09 06:44:15 INFO InputInfoTracker: remove old batch metadata: 1444391053000 ms
15/10/09 06:44:15 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391054000: 
15/10/09 06:44:15 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391054000
15/10/09 06:44:16 INFO JobScheduler: Added jobs for time 1444391056000 ms
15/10/09 06:44:16 INFO JobGenerator: Checkpointing graph for time 1444391056000 ms
15/10/09 06:44:16 INFO DStreamGraph: Updating checkpoint data for time 1444391056000 ms
15/10/09 06:44:16 INFO JobScheduler: Starting job streaming job 1444391056000 ms.0 from job set of time 1444391056000 ms
15/10/09 06:44:16 INFO DStreamGraph: Updated checkpoint data for time 1444391056000 ms
15/10/09 06:44:16 INFO CheckpointWriter: Saving checkpoint for time 1444391056000 ms to file 'file:/tmp/checkpoint-1444391056000'
15/10/09 06:44:16 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:16 INFO DAGScheduler: Job 68 finished: foreachRDD at Events.scala:18, took 0.000019 s
15/10/09 06:44:16 INFO JobScheduler: Finished job streaming job 1444391056000 ms.0 from job set of time 1444391056000 ms
15/10/09 06:44:16 INFO JobScheduler: Total delay: 0.004 s for time 1444391056000 ms (execution: 0.002 s)
15/10/09 06:44:16 INFO MapPartitionsRDD: Removing RDD 134 from persistence list
15/10/09 06:44:16 INFO BlockManager: Removing RDD 134
15/10/09 06:44:16 INFO WriteAheadLogBackedBlockRDD: Removing RDD 133 from persistence list
15/10/09 06:44:16 INFO BlockManager: Removing RDD 133
15/10/09 06:44:16 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[133] at createStream at Events.scala:13 of time 1444391056000 ms
15/10/09 06:44:16 INFO JobGenerator: Checkpointing graph for time 1444391056000 ms
15/10/09 06:44:16 INFO DStreamGraph: Updating checkpoint data for time 1444391056000 ms
15/10/09 06:44:16 INFO DStreamGraph: Updated checkpoint data for time 1444391056000 ms
15/10/09 06:44:16 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391051000.bk
15/10/09 06:44:16 INFO CheckpointWriter: Checkpoint for time 1444391056000 ms saved to file 'file:/tmp/checkpoint-1444391056000', took 3619 bytes and 9 ms
15/10/09 06:44:16 INFO CheckpointWriter: Saving checkpoint for time 1444391056000 ms to file 'file:/tmp/checkpoint-1444391056000'
15/10/09 06:44:16 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391051000
15/10/09 06:44:16 INFO CheckpointWriter: Checkpoint for time 1444391056000 ms saved to file 'file:/tmp/checkpoint-1444391056000', took 3615 bytes and 5 ms
15/10/09 06:44:16 INFO DStreamGraph: Clearing checkpoint data for time 1444391056000 ms
15/10/09 06:44:16 INFO DStreamGraph: Cleared checkpoint data for time 1444391056000 ms
15/10/09 06:44:16 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391054000 ms)
15/10/09 06:44:16 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391055000: 
15/10/09 06:44:16 INFO ReceiverTracker: Cleanup old received batch data: 1444391055000 ms
15/10/09 06:44:16 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391055000
15/10/09 06:44:16 INFO InputInfoTracker: remove old batch metadata: 1444391054000 ms
15/10/09 06:44:16 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391055000: 
15/10/09 06:44:16 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391055000
15/10/09 06:44:17 INFO JobScheduler: Added jobs for time 1444391057000 ms
15/10/09 06:44:17 INFO JobGenerator: Checkpointing graph for time 1444391057000 ms
15/10/09 06:44:17 INFO DStreamGraph: Updating checkpoint data for time 1444391057000 ms
15/10/09 06:44:17 INFO DStreamGraph: Updated checkpoint data for time 1444391057000 ms
15/10/09 06:44:17 INFO JobScheduler: Starting job streaming job 1444391057000 ms.0 from job set of time 1444391057000 ms
15/10/09 06:44:17 INFO CheckpointWriter: Saving checkpoint for time 1444391057000 ms to file 'file:/tmp/checkpoint-1444391057000'
15/10/09 06:44:17 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:17 INFO DAGScheduler: Job 69 finished: foreachRDD at Events.scala:18, took 0.000022 s
15/10/09 06:44:17 INFO JobScheduler: Finished job streaming job 1444391057000 ms.0 from job set of time 1444391057000 ms
15/10/09 06:44:17 INFO JobScheduler: Total delay: 0.007 s for time 1444391057000 ms (execution: 0.003 s)
15/10/09 06:44:17 INFO MapPartitionsRDD: Removing RDD 136 from persistence list
15/10/09 06:44:17 INFO WriteAheadLogBackedBlockRDD: Removing RDD 135 from persistence list
15/10/09 06:44:17 INFO BlockManager: Removing RDD 136
15/10/09 06:44:17 INFO BlockManager: Removing RDD 135
15/10/09 06:44:17 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[135] at createStream at Events.scala:13 of time 1444391057000 ms
15/10/09 06:44:17 INFO JobGenerator: Checkpointing graph for time 1444391057000 ms
15/10/09 06:44:17 INFO DStreamGraph: Updating checkpoint data for time 1444391057000 ms
15/10/09 06:44:17 INFO DStreamGraph: Updated checkpoint data for time 1444391057000 ms
15/10/09 06:44:17 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391052000.bk
15/10/09 06:44:17 INFO CheckpointWriter: Checkpoint for time 1444391057000 ms saved to file 'file:/tmp/checkpoint-1444391057000', took 3619 bytes and 8 ms
15/10/09 06:44:17 INFO CheckpointWriter: Saving checkpoint for time 1444391057000 ms to file 'file:/tmp/checkpoint-1444391057000'
15/10/09 06:44:17 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391052000
15/10/09 06:44:17 INFO CheckpointWriter: Checkpoint for time 1444391057000 ms saved to file 'file:/tmp/checkpoint-1444391057000', took 3615 bytes and 6 ms
15/10/09 06:44:17 INFO DStreamGraph: Clearing checkpoint data for time 1444391057000 ms
15/10/09 06:44:17 INFO DStreamGraph: Cleared checkpoint data for time 1444391057000 ms
15/10/09 06:44:17 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391055000 ms)
15/10/09 06:44:17 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391056000: 
15/10/09 06:44:17 INFO ReceiverTracker: Cleanup old received batch data: 1444391056000 ms
15/10/09 06:44:17 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391056000
15/10/09 06:44:17 INFO InputInfoTracker: remove old batch metadata: 1444391055000 ms
15/10/09 06:44:17 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391056000: 
15/10/09 06:44:17 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391056000
15/10/09 06:44:18 INFO JobScheduler: Added jobs for time 1444391058000 ms
15/10/09 06:44:18 INFO JobGenerator: Checkpointing graph for time 1444391058000 ms
15/10/09 06:44:18 INFO DStreamGraph: Updating checkpoint data for time 1444391058000 ms
15/10/09 06:44:18 INFO DStreamGraph: Updated checkpoint data for time 1444391058000 ms
15/10/09 06:44:18 INFO JobScheduler: Starting job streaming job 1444391058000 ms.0 from job set of time 1444391058000 ms
15/10/09 06:44:18 INFO CheckpointWriter: Saving checkpoint for time 1444391058000 ms to file 'file:/tmp/checkpoint-1444391058000'
15/10/09 06:44:18 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:18 INFO DAGScheduler: Job 70 finished: foreachRDD at Events.scala:18, took 0.000021 s
15/10/09 06:44:18 INFO JobScheduler: Finished job streaming job 1444391058000 ms.0 from job set of time 1444391058000 ms
15/10/09 06:44:18 INFO JobScheduler: Total delay: 0.010 s for time 1444391058000 ms (execution: 0.003 s)
15/10/09 06:44:18 INFO MapPartitionsRDD: Removing RDD 138 from persistence list
15/10/09 06:44:18 INFO BlockManager: Removing RDD 138
15/10/09 06:44:18 INFO WriteAheadLogBackedBlockRDD: Removing RDD 137 from persistence list
15/10/09 06:44:18 INFO BlockManager: Removing RDD 137
15/10/09 06:44:18 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[137] at createStream at Events.scala:13 of time 1444391058000 ms
15/10/09 06:44:18 INFO JobGenerator: Checkpointing graph for time 1444391058000 ms
15/10/09 06:44:18 INFO DStreamGraph: Updating checkpoint data for time 1444391058000 ms
15/10/09 06:44:18 INFO DStreamGraph: Updated checkpoint data for time 1444391058000 ms
15/10/09 06:44:18 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391053000.bk
15/10/09 06:44:18 INFO CheckpointWriter: Checkpoint for time 1444391058000 ms saved to file 'file:/tmp/checkpoint-1444391058000', took 3619 bytes and 8 ms
15/10/09 06:44:18 INFO CheckpointWriter: Saving checkpoint for time 1444391058000 ms to file 'file:/tmp/checkpoint-1444391058000'
15/10/09 06:44:18 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391053000
15/10/09 06:44:18 INFO CheckpointWriter: Checkpoint for time 1444391058000 ms saved to file 'file:/tmp/checkpoint-1444391058000', took 3615 bytes and 6 ms
15/10/09 06:44:18 INFO DStreamGraph: Clearing checkpoint data for time 1444391058000 ms
15/10/09 06:44:18 INFO DStreamGraph: Cleared checkpoint data for time 1444391058000 ms
15/10/09 06:44:18 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391056000 ms)
15/10/09 06:44:18 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391057000: 
15/10/09 06:44:18 INFO ReceiverTracker: Cleanup old received batch data: 1444391057000 ms
15/10/09 06:44:18 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391057000
15/10/09 06:44:18 INFO InputInfoTracker: remove old batch metadata: 1444391056000 ms
15/10/09 06:44:18 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391057000: 
15/10/09 06:44:18 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391057000
15/10/09 06:44:19 INFO JobScheduler: Added jobs for time 1444391059000 ms
15/10/09 06:44:19 INFO JobGenerator: Checkpointing graph for time 1444391059000 ms
15/10/09 06:44:19 INFO DStreamGraph: Updating checkpoint data for time 1444391059000 ms
15/10/09 06:44:19 INFO DStreamGraph: Updated checkpoint data for time 1444391059000 ms
15/10/09 06:44:19 INFO JobScheduler: Starting job streaming job 1444391059000 ms.0 from job set of time 1444391059000 ms
15/10/09 06:44:19 INFO CheckpointWriter: Saving checkpoint for time 1444391059000 ms to file 'file:/tmp/checkpoint-1444391059000'
15/10/09 06:44:19 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:19 INFO DAGScheduler: Job 71 finished: foreachRDD at Events.scala:18, took 0.000025 s
15/10/09 06:44:19 INFO JobScheduler: Finished job streaming job 1444391059000 ms.0 from job set of time 1444391059000 ms
15/10/09 06:44:19 INFO JobScheduler: Total delay: 0.007 s for time 1444391059000 ms (execution: 0.003 s)
15/10/09 06:44:19 INFO MapPartitionsRDD: Removing RDD 140 from persistence list
15/10/09 06:44:19 INFO BlockManager: Removing RDD 140
15/10/09 06:44:19 INFO WriteAheadLogBackedBlockRDD: Removing RDD 139 from persistence list
15/10/09 06:44:19 INFO BlockManager: Removing RDD 139
15/10/09 06:44:19 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[139] at createStream at Events.scala:13 of time 1444391059000 ms
15/10/09 06:44:19 INFO JobGenerator: Checkpointing graph for time 1444391059000 ms
15/10/09 06:44:19 INFO DStreamGraph: Updating checkpoint data for time 1444391059000 ms
15/10/09 06:44:19 INFO DStreamGraph: Updated checkpoint data for time 1444391059000 ms
15/10/09 06:44:19 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391054000.bk
15/10/09 06:44:19 INFO CheckpointWriter: Checkpoint for time 1444391059000 ms saved to file 'file:/tmp/checkpoint-1444391059000', took 3619 bytes and 8 ms
15/10/09 06:44:19 INFO CheckpointWriter: Saving checkpoint for time 1444391059000 ms to file 'file:/tmp/checkpoint-1444391059000'
15/10/09 06:44:19 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391054000
15/10/09 06:44:19 INFO CheckpointWriter: Checkpoint for time 1444391059000 ms saved to file 'file:/tmp/checkpoint-1444391059000', took 3615 bytes and 6 ms
15/10/09 06:44:19 INFO DStreamGraph: Clearing checkpoint data for time 1444391059000 ms
15/10/09 06:44:19 INFO DStreamGraph: Cleared checkpoint data for time 1444391059000 ms
15/10/09 06:44:19 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391057000 ms)
15/10/09 06:44:19 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391058000: 
15/10/09 06:44:19 INFO ReceiverTracker: Cleanup old received batch data: 1444391058000 ms
15/10/09 06:44:19 INFO InputInfoTracker: remove old batch metadata: 1444391057000 ms
15/10/09 06:44:19 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391058000
15/10/09 06:44:19 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391058000: 
15/10/09 06:44:19 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391058000
15/10/09 06:44:20 INFO JobScheduler: Added jobs for time 1444391060000 ms
15/10/09 06:44:20 INFO JobGenerator: Checkpointing graph for time 1444391060000 ms
15/10/09 06:44:20 INFO DStreamGraph: Updating checkpoint data for time 1444391060000 ms
15/10/09 06:44:20 INFO DStreamGraph: Updated checkpoint data for time 1444391060000 ms
15/10/09 06:44:20 INFO JobScheduler: Starting job streaming job 1444391060000 ms.0 from job set of time 1444391060000 ms
15/10/09 06:44:20 INFO CheckpointWriter: Saving checkpoint for time 1444391060000 ms to file 'file:/tmp/checkpoint-1444391060000'
15/10/09 06:44:20 INFO SparkContext: Starting job: foreachRDD at Events.scala:18
15/10/09 06:44:20 INFO DAGScheduler: Job 72 finished: foreachRDD at Events.scala:18, took 0.000022 s
15/10/09 06:44:20 INFO JobScheduler: Finished job streaming job 1444391060000 ms.0 from job set of time 1444391060000 ms
15/10/09 06:44:20 INFO JobScheduler: Total delay: 0.006 s for time 1444391060000 ms (execution: 0.003 s)
15/10/09 06:44:20 INFO MapPartitionsRDD: Removing RDD 142 from persistence list
15/10/09 06:44:20 INFO BlockManager: Removing RDD 142
15/10/09 06:44:20 INFO WriteAheadLogBackedBlockRDD: Removing RDD 141 from persistence list
15/10/09 06:44:20 INFO BlockManager: Removing RDD 141
15/10/09 06:44:20 INFO KafkaInputDStream: Removing blocks of RDD WriteAheadLogBackedBlockRDD[141] at createStream at Events.scala:13 of time 1444391060000 ms
15/10/09 06:44:20 INFO JobGenerator: Checkpointing graph for time 1444391060000 ms
15/10/09 06:44:20 INFO DStreamGraph: Updating checkpoint data for time 1444391060000 ms
15/10/09 06:44:20 INFO DStreamGraph: Updated checkpoint data for time 1444391060000 ms
15/10/09 06:44:20 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391055000.bk
15/10/09 06:44:20 INFO CheckpointWriter: Checkpoint for time 1444391060000 ms saved to file 'file:/tmp/checkpoint-1444391060000', took 3619 bytes and 8 ms
15/10/09 06:44:20 INFO CheckpointWriter: Saving checkpoint for time 1444391060000 ms to file 'file:/tmp/checkpoint-1444391060000'
15/10/09 06:44:20 INFO CheckpointWriter: Deleting file:/tmp/checkpoint-1444391055000
15/10/09 06:44:20 INFO CheckpointWriter: Checkpoint for time 1444391060000 ms saved to file 'file:/tmp/checkpoint-1444391060000', took 3615 bytes and 6 ms
15/10/09 06:44:20 INFO DStreamGraph: Clearing checkpoint data for time 1444391060000 ms
15/10/09 06:44:20 INFO DStreamGraph: Cleared checkpoint data for time 1444391060000 ms
15/10/09 06:44:20 INFO ReceivedBlockTracker: Deleting batches ArrayBuffer(1444391058000 ms)
15/10/09 06:44:20 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedBlockMetadata older than 1444391059000: 
15/10/09 06:44:20 INFO ReceiverTracker: Cleanup old received batch data: 1444391059000 ms
15/10/09 06:44:20 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedBlockMetadata older than 1444391059000
15/10/09 06:44:20 INFO InputInfoTracker: remove old batch metadata: 1444391058000 ms
15/10/09 06:44:20 INFO WriteAheadLogManager : Attempting to clear 0 old log files in file:/tmp/receivedData/0 older than 1444391059000: 
15/10/09 06:44:20 INFO WriteAheadLogManager : Cleared log files in file:/tmp/receivedData/0 older than 1444391059000
15/10/09 06:44:20 INFO StreamingContext: Invoking stop(stopGracefully=false) from shutdown hook
15/10/09 06:44:20 INFO ReceiverTracker: Sent stop signal to all 1 receivers
15/10/09 06:44:20 INFO ReceiverSupervisorImpl: Received stop signal
15/10/09 06:44:20 INFO ReceiverSupervisorImpl: Stopping receiver with message: Stopped by driver: 
15/10/09 06:44:20 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], ZKConsumerConnector shutting down
15/10/09 06:44:20 INFO ConsumerFetcherManager: [ConsumerFetcherManager-1444390989404] Stopping leader finder thread
15/10/09 06:44:20 INFO ConsumerFetcherManager$LeaderFinderThread: [Events_Andrew-C-MBP.local-1444390989356-f9caca59-leader-finder-thread], Shutting down
15/10/09 06:44:20 INFO ConsumerFetcherManager$LeaderFinderThread: [Events_Andrew-C-MBP.local-1444390989356-f9caca59-leader-finder-thread], Stopped 
15/10/09 06:44:20 INFO ConsumerFetcherManager$LeaderFinderThread: [Events_Andrew-C-MBP.local-1444390989356-f9caca59-leader-finder-thread], Shutdown completed
15/10/09 06:44:20 INFO ConsumerFetcherManager: [ConsumerFetcherManager-1444390989404] Stopping all fetchers
15/10/09 06:44:20 INFO ConsumerFetcherThread: [ConsumerFetcherThread-Events_Andrew-C-MBP.local-1444390989356-f9caca59-0-0], Shutting down
15/10/09 06:44:20 INFO SimpleConsumer: Reconnect due to socket error: java.nio.channels.ClosedByInterruptException
15/10/09 06:44:20 INFO ConsumerFetcherThread: [ConsumerFetcherThread-Events_Andrew-C-MBP.local-1444390989356-f9caca59-0-0], Stopped 
15/10/09 06:44:20 INFO ConsumerFetcherThread: [ConsumerFetcherThread-Events_Andrew-C-MBP.local-1444390989356-f9caca59-0-0], Shutdown completed
15/10/09 06:44:20 INFO ConsumerFetcherManager: [ConsumerFetcherManager-1444390989404] All connections stopped
15/10/09 06:44:20 INFO ZkEventThread: Terminate ZkClient event thread.
15/10/09 06:44:20 INFO ZooKeeper: Session: 0x15048f8a9500015 closed
15/10/09 06:44:20 INFO ClientCnxn: EventThread shut down
15/10/09 06:44:20 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], ZKConsumerConnector shutdown completed in 9 ms
15/10/09 06:44:20 INFO ZkEventThread: Terminate ZkClient event thread.
15/10/09 06:44:20 INFO ZooKeeper: Session: 0x15048f8a9500016 closed
15/10/09 06:44:20 INFO ClientCnxn: EventThread shut down
15/10/09 06:44:20 INFO BlockGenerator: Stopping BlockGenerator
15/10/09 06:44:20 INFO RecurringTimer: Stopped timer for BlockGenerator after time 1444391060400
15/10/09 06:44:20 INFO BlockGenerator: Waiting for block pushing thread to terminate
15/10/09 06:44:20 INFO BlockGenerator: Pushing out the last 0 blocks
15/10/09 06:44:20 INFO BlockGenerator: Stopped block pushing thread
15/10/09 06:44:20 INFO BlockGenerator: Stopped BlockGenerator
15/10/09 06:44:20 INFO ReceiverSupervisorImpl: Called receiver onStop
15/10/09 06:44:20 INFO ReceiverSupervisorImpl: Deregistering receiver 0
15/10/09 06:44:20 ERROR ReceiverTracker: Deregistered receiver for stream 0: Stopped by driver
15/10/09 06:44:20 INFO ReceiverSupervisorImpl: Stopped receiver 0
15/10/09 06:44:20 INFO BlockGenerator: Stopping BlockGenerator
15/10/09 06:44:20 INFO ZookeeperConsumerConnector: [Events_Andrew-C-MBP.local-1444390989356-f9caca59], stopping watcher executor thread for consumer Events_Andrew-C-MBP.local-1444390989356-f9caca59
15/10/09 06:44:20 INFO RecurringTimer: Stopped timer for BlockGenerator after time 1444391060800
15/10/09 06:44:20 INFO BlockGenerator: Waiting for block pushing thread to terminate
15/10/09 06:44:20 INFO BlockGenerator: Pushing out the last 0 blocks
15/10/09 06:44:20 INFO BlockGenerator: Stopped block pushing thread
15/10/09 06:44:20 INFO BlockGenerator: Stopped BlockGenerator
15/10/09 06:44:20 WARN BlockGenerator: Cannot stop BlockGenerator as its not in the Active state [state = StoppedAll]
15/10/09 06:44:20 INFO ReceiverSupervisorImpl: Stopped receiver without error
15/10/09 06:44:20 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 915 bytes result sent to driver
15/10/09 06:44:20 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 71772 ms on localhost (1/1)
15/10/09 06:44:20 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
15/10/09 06:44:20 INFO DAGScheduler: ResultStage 0 (start at Events.scala:35) finished in 71.784 s
15/10/09 06:44:20 INFO ReceiverTracker: All of the receivers have deregistered successfully
15/10/09 06:44:20 INFO WriteAheadLogManager : Stopped write ahead log manager
15/10/09 06:44:20 INFO ReceiverTracker: ReceiverTracker stopped
15/10/09 06:44:20 INFO JobGenerator: Stopping JobGenerator immediately
15/10/09 06:44:20 INFO RecurringTimer: Stopped timer for JobGenerator after time 1444391060000
15/10/09 06:44:20 INFO CheckpointWriter: CheckpointWriter executor terminated ? true, waited for 0 ms.
15/10/09 06:44:20 INFO JobGenerator: Stopped JobGenerator
15/10/09 06:44:20 INFO JobScheduler: Stopped JobScheduler
15/10/09 06:44:20 INFO StreamingContext: StreamingContext stopped successfully
15/10/09 06:44:20 INFO SparkContext: Invoking stop() from shutdown hook
15/10/09 06:44:20 INFO SparkUI: Stopped Spark web UI at http://10.101.21.127:4040
15/10/09 06:44:20 INFO DAGScheduler: Stopping DAGScheduler
15/10/09 06:44:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/10/09 06:44:20 INFO MemoryStore: MemoryStore cleared
15/10/09 06:44:20 INFO BlockManager: BlockManager stopped
15/10/09 06:44:20 INFO BlockManagerMaster: BlockManagerMaster stopped
15/10/09 06:44:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/10/09 06:44:20 INFO SparkContext: Successfully stopped SparkContext
15/10/09 06:44:20 INFO ShutdownHookManager: Shutdown hook called
15/10/09 06:44:20 INFO ShutdownHookManager: Deleting directory /private/var/folders/q_/19cvvfns3qj0zgz98w1q0yqw0000gn/T/spark-e8c0e34a-82c4-4275-a5e7-1a397218521b
